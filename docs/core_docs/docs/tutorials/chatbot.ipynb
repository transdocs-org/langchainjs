{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "sidebar_position: 1\n",
        "keywords: [conversationchain]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 构建一个聊天机器人\n",
        "\n",
        "\n",
        ":::info 预备知识\n",
        "\n",
        "本指南假定您熟悉以下概念：\n",
        "\n",
        "- [聊天模型](/docs/concepts/chat_models)\n",
        "- [提示模板](/docs/concepts/prompt_templates)\n",
        "- [聊天历史](/docs/concepts/chat_history)\n",
        "\n",
        "本指南需要 `langgraph >= 0.2.28`。\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "```{=mdx}\n",
        "\n",
        ":::note\n",
        "\n",
        "本教程之前使用 [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html) 构建了一个聊天机器人。您可以在 [v0.2 文档](https://js.langchain.com/v0.2/docs/tutorials/chatbot/) 中访问本教程的这个版本。\n",
        "\n",
        "LangGraph 实现相比 `RunnableWithMessageHistory` 提供了许多优势，包括能够持久化应用程序状态的任意组件（而不仅仅是消息）。\n",
        "\n",
        ":::\n",
        "\n",
        "```\n",
        "\n",
        "## 概述\n",
        "\n",
        "我们将介绍如何设计和实现一个由大型语言模型（LLM）驱动的聊天机器人。\n",
        "这个聊天机器人将能够进行对话并记住之前的交互。\n",
        "\n",
        "\n",
        "请注意，我们构建的这个聊天机器人将仅使用语言模型进行对话。\n",
        "您可能还会对以下几个相关概念感兴趣：\n",
        "\n",
        "- [对话式检索增强生成（Conversational RAG）](/docs/tutorials/qa_chat_history)：在外部数据源上启用聊天机器人体验\n",
        "- [代理（Agents）](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/agent_supervisor/)：构建能够执行操作的聊天机器人\n",
        "\n",
        "本教程将涵盖基础知识，这些知识将对这两个更高级的主题有所帮助，但如果您愿意，也可以直接跳转到那里。\n",
        "\n",
        "## 准备工作\n",
        "\n",
        "### Jupyter Notebook\n",
        "\n",
        "本指南（以及文档中的大多数其他指南）使用 [Jupyter notebooks](https://jupyter.org/) 并假定读者也使用它。Jupyter notebooks 非常适合学习如何与大型语言模型系统协作，因为很多时候事情可能会出错（输出意外、API 不可用等），在交互式环境中学习指南是更好地理解它们的好方法。\n",
        "\n",
        "这本教程和其他教程可能最适合在 Jupyter notebook 中运行。有关安装说明，请参见[此处](https://jupyter.org/install)。\n",
        "\n",
        "### 安装\n",
        "\n",
        "在本教程中，我们需要安装 `@langchain/core` 和 `langgraph`：\n",
        "\n",
        "```{=mdx}\n",
        "import Npm2Yarn from \"@theme/Npm2Yarn\"\n",
        "\n",
        "<Npm2Yarn>\n",
        "  @langchain/core @langchain/langgraph uuid\n",
        "</Npm2Yarn>\n",
        "```\n",
        "\n",
        "更多详细信息，请参见我们的[安装指南](/docs/how_to/installation)。\n",
        "\n",
        "### LangSmith\n",
        "\n",
        "您使用 LangChain 构建的许多应用程序将包含多个步骤和多次调用 LLM。\n",
        "随着这些应用程序变得越来越复杂，能够检查您的链或代理内部发生了什么变得至关重要。\n",
        "最佳方式是使用 [LangSmith](https://smith.langchain.com)。\n",
        "\n",
        "在上面的链接注册后，请确保设置您的环境变量以开始记录追踪信息：\n",
        "\n",
        "```typescript\n",
        "process.env.LANGSMITH_TRACING = \"true\"\n",
        "process.env.LANGSMITH_API_KEY = \"...\"\n",
        "```\n",
        "\n",
        "## 快速入门\n",
        "\n",
        "首先，让我们学习如何单独使用一个语言模型。LangChain 支持许多不同的语言模型，您可以根据需要选择使用以下任意一个！\n",
        "\n",
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs customVarName=\"llm\" />\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llm = new ChatOpenAI({ model: \"gpt-4o-mini\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们首先直接使用该模型。`ChatModel` 是 LangChain \"Runnables\" 的实例，这意味着它们提供了一个标准接口用于与它们进行交互。要简单地调用模型，我们可以将消息列表传递给 `.invoke` 方法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekDrrCyaBauLYHuVv3dkacxW2G1J\",\n",
            "  \"content\": \"Hi Bob! How can I help you today?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 10,\n",
            "      \"completionTokens\": 10,\n",
            "      \"totalTokens\": 20\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 10,\n",
            "      \"completion_tokens\": 10,\n",
            "      \"total_tokens\": 20,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 10,\n",
            "    \"input_tokens\": 10,\n",
            "    \"total_tokens\": 20,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "await llm.invoke([{ role: \"user\", content: \"Hi im bob\" }])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "模型本身没有任何状态的概念。例如，如果你提出一个后续问题："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekDuOk1LjOdBVLtuCvuHjAs5aoad\",\n",
            "  \"content\": \"I'm sorry, but I don't have access to personal information about users unless you've shared it with me in this conversation. How can I assist you today?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 10,\n",
            "      \"completionTokens\": 30,\n",
            "      \"totalTokens\": 40\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 10,\n",
            "      \"completion_tokens\": 30,\n",
            "      \"total_tokens\": 40,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 30,\n",
            "    \"input_tokens\": 10,\n",
            "    \"total_tokens\": 40,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "await llm.invoke([{ role: \"user\", content: \"Whats my name\" }])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "让我们看一下这个示例 [LangSmith 追踪](https://smith.langchain.com/public/3b768e44-a319-453a-bd6e-30f9df75f16a/r)\n",
        "\n",
        "我们可以看到，它没有将之前的对话轮次纳入上下文，因此无法回答问题。\n",
        "这会导致非常糟糕的聊天机器人体验！\n",
        "\n",
        "为了解决这个问题，我们需要将整个对话历史传递给模型。让我们看看这样做的时候会发生什么："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekDyJdj6y9IREyNIf3tkKGRKhN1Z\",\n",
            "  \"content\": \"Your name is Bob! How can I help you today, Bob?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 33,\n",
            "      \"completionTokens\": 14,\n",
            "      \"totalTokens\": 47\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 33,\n",
            "      \"completion_tokens\": 14,\n",
            "      \"total_tokens\": 47,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 14,\n",
            "    \"input_tokens\": 33,\n",
            "    \"total_tokens\": 47,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "await llm.invoke([\n",
        "  { role: \"user\", content: \"Hi! I'm Bob\" },\n",
        "  { role: \"assistant\", content: \"Hello Bob! How can I assist you today?\" },\n",
        "  { role: \"user\", content: \"What's my name?\" }\n",
        "]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在我们可以看到我们得到了一个很好的回复！\n",
        "\n",
        "这就是聊天机器人能够进行对话交互的基本原理。\n",
        "那么我们该如何最好地实现这一点呢？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 消息持久化\n",
        "\n",
        "[LangGraph](https://langchain-ai.github.io/langgraphjs/) 实现了一个内置的持久化层，使其非常适合支持多轮对话的聊天应用。\n",
        "\n",
        "将我们的聊天模型封装在一个最小的 LangGraph 应用中，可以自动持久化消息历史记录，从而简化多轮应用的开发。\n",
        "\n",
        "LangGraph 配备了一个简单的内存检查点工具，我们在下面使用它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\n",
        "\n",
        "// Define the function that calls the model\n",
        "const callModel = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const response = await llm.invoke(state.messages);\n",
        "  return { messages: response };\n",
        "};\n",
        "\n",
        "// Define a new graph\n",
        "const workflow = new StateGraph(MessagesAnnotation)\n",
        "  // Define the node and edge\n",
        "  .addNode(\"model\", callModel)\n",
        "  .addEdge(START, \"model\")\n",
        "  .addEdge(\"model\", END);\n",
        "\n",
        "// Add memory\n",
        "const memory = new MemorySaver();\n",
        "const app = workflow.compile({ checkpointer: memory });"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们现在需要创建一个 `config`，每次都要将其传递给可运行对象。此 config 包含不直接属于输入但仍然有用的信息。在这种情况下，我们想要包含一个 `thread_id`。它应该如下所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { v4 as uuidv4 } from \"uuid\";\n",
        "\n",
        "const config = { configurable: { thread_id: uuidv4() } };"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这使我们能够使用单个应用程序支持多个对话线程，当您的应用程序具有多个用户时，这是一个常见需求。\n",
        "\n",
        "然后我们可以调用该应用程序："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekEFPclmrO7YfAe7J0zUAanS4ifx\",\n",
            "  \"content\": \"Hi Bob! How can I assist you today?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 12,\n",
            "      \"completionTokens\": 10,\n",
            "      \"totalTokens\": 22\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 12,\n",
            "      \"completion_tokens\": 10,\n",
            "      \"total_tokens\": 22,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 10,\n",
            "    \"input_tokens\": 12,\n",
            "    \"total_tokens\": 22,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const input = [\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: \"Hi! I'm Bob.\",\n",
        "  }\n",
        "]\n",
        "const output = await app.invoke({ messages: input }, config)\n",
        "// The output contains all messages in the state.\n",
        "// This will log the last message in the conversation.\n",
        "console.log(output.messages[output.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekEJgCfLodGCcuLgLQdJevH7CpCJ\",\n",
            "  \"content\": \"Your name is Bob! How can I help you today, Bob?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 34,\n",
            "      \"completionTokens\": 14,\n",
            "      \"totalTokens\": 48\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 34,\n",
            "      \"completion_tokens\": 14,\n",
            "      \"total_tokens\": 48,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 14,\n",
            "    \"input_tokens\": 34,\n",
            "    \"total_tokens\": 48,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const input2 = [\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: \"What's my name?\",\n",
        "  }\n",
        "]\n",
        "const output2 = await app.invoke({ messages: input2 }, config)\n",
        "console.log(output2.messages[output2.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "太好了！我们的聊天机器人现在可以记住关于我们的信息。如果我们更改配置以引用不同的 `thread_id`，我们会看到它重新开始对话。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekELvPXLtjOKgLN63mQzZwvyo12J\",\n",
            "  \"content\": \"I'm sorry, but I don't have access to personal information about individuals unless you share it with me. How can I assist you today?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 11,\n",
            "      \"completionTokens\": 27,\n",
            "      \"totalTokens\": 38\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 11,\n",
            "      \"completion_tokens\": 27,\n",
            "      \"total_tokens\": 38,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_39a40c96a0\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 27,\n",
            "    \"input_tokens\": 11,\n",
            "    \"total_tokens\": 38,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const config2 = { configurable: { thread_id: uuidv4() } }\n",
        "const input3 = [\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: \"What's my name?\",\n",
        "  }\n",
        "]\n",
        "const output3 = await app.invoke({ messages: input3 }, config2)\n",
        "console.log(output3.messages[output3.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "但是，我们始终可以回溯到原始对话（因为我们将其持久化存储在数据库中）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekEQ8Z5JmYquSfzPsCWv1BDTKZSh\",\n",
            "  \"content\": \"Your name is Bob. Is there something specific you would like to talk about?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 60,\n",
            "      \"completionTokens\": 16,\n",
            "      \"totalTokens\": 76\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 60,\n",
            "      \"completion_tokens\": 16,\n",
            "      \"total_tokens\": 76,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_39a40c96a0\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 16,\n",
            "    \"input_tokens\": 60,\n",
            "    \"total_tokens\": 76,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const output4 = await app.invoke({ messages: input2 }, config)\n",
        "console.log(output4.messages[output4.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这就是我们如何支持聊天机器人与多个用户进行对话！\n",
        "\n",
        "目前，我们所做的只是在模型周围添加了一个简单的持久化层。我们可以通过添加提示模板（prompt template）来使其变得更加复杂和个性化。\n",
        "\n",
        "## 提示模板\n",
        "\n",
        "提示模板有助于将原始用户信息转换为LLM可以处理的格式。在这种情况下，原始用户输入只是一个消息，我们将其传递给LLM。现在我们让这个过程稍微复杂一些。首先，让我们添加一条带有自定义指令的系统消息（system message），但仍然接收消息作为输入。接下来，我们将添加除消息之外的更多输入。\n",
        "\n",
        "要添加系统消息，我们将创建一个`ChatPromptTemplate`。我们将使用`MessagesPlaceholder`将所有消息传递进去。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "const promptTemplate = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", \"You talk like a pirate. Answer all questions to the best of your ability.\"],\n",
        "  [\"placeholder\", \"{messages}\"],\n",
        "]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在我们可以更新我们的应用程序以包含此模板："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\n",
        "\n",
        "// Define the function that calls the model\n",
        "const callModel2 = async (state: typeof MessagesAnnotation.State) => {\n",
        "  // highlight-start\n",
        "  const prompt = await promptTemplate.invoke(state)\n",
        "  const response = await llm.invoke(prompt);\n",
        "  // highlight-end\n",
        "  // Update message history with response:\n",
        "  return { messages: [response] };\n",
        "};\n",
        "\n",
        "// Define a new graph\n",
        "const workflow2 = new StateGraph(MessagesAnnotation)\n",
        "  // Define the (single) node in the graph\n",
        "  .addNode(\"model\", callModel2)\n",
        "  .addEdge(START, \"model\")\n",
        "  .addEdge(\"model\", END);\n",
        "\n",
        "// Add memory\n",
        "const app2 = workflow2.compile({ checkpointer: new MemorySaver() });"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们以相同的方式调用应用程序："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekEYAQVqh9OFZRGdzGiPz33WPf1v\",\n",
            "  \"content\": \"Ahoy, Jim! A pleasure to meet ye, matey! What be on yer mind this fine day?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 32,\n",
            "      \"completionTokens\": 23,\n",
            "      \"totalTokens\": 55\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 32,\n",
            "      \"completion_tokens\": 23,\n",
            "      \"total_tokens\": 55,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_39a40c96a0\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 23,\n",
            "    \"input_tokens\": 32,\n",
            "    \"total_tokens\": 55,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const config3 = { configurable: { thread_id: uuidv4() } }\n",
        "const input4 = [\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: \"Hi! I'm Jim.\",\n",
        "  }\n",
        "]\n",
        "const output5 = await app2.invoke({ messages: input4 }, config3)\n",
        "console.log(output5.messages[output5.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekEbrpFI3K8BxemHZ5fG4xF2tT8x\",\n",
            "  \"content\": \"Ye be callin' yerself Jim, if I heard ye right, savvy? What else can I do fer ye, me hearty?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 68,\n",
            "      \"completionTokens\": 29,\n",
            "      \"totalTokens\": 97\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 68,\n",
            "      \"completion_tokens\": 29,\n",
            "      \"total_tokens\": 97,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 29,\n",
            "    \"input_tokens\": 68,\n",
            "    \"total_tokens\": 97,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const input5 = [\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content:  \"What is my name?\"\n",
        "  }\n",
        "]\n",
        "const output6 = await app2.invoke({ messages: input5 }, config3)\n",
        "console.log(output6.messages[output6.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "太棒了！现在让我们让提示词稍微复杂一点。假设提示模板现在看起来像这样："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "const promptTemplate2 = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\"],\n",
        "  [\"placeholder\", \"{messages}\"],\n",
        "]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "请注意，我们已在提示中添加了新的 `language` 输入。我们的应用程序现在有两个参数——输入的 `messages` 和 `language`。我们需要更新应用程序的状态以反映此更改："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { START, END, StateGraph, MemorySaver, MessagesAnnotation, Annotation } from \"@langchain/langgraph\";\n",
        "\n",
        "// Define the State\n",
        "const GraphAnnotation = Annotation.Root({\n",
        "  ...MessagesAnnotation.spec,\n",
        "  language: Annotation<string>(),\n",
        "});\n",
        "\n",
        "// Define the function that calls the model\n",
        "const callModel3 = async (state: typeof GraphAnnotation.State) => {\n",
        "  const prompt = await promptTemplate2.invoke(state);\n",
        "  const response = await llm.invoke(prompt);\n",
        "  return { messages: [response] };\n",
        "};\n",
        "\n",
        "const workflow3 = new StateGraph(GraphAnnotation)\n",
        "  .addNode(\"model\", callModel3)\n",
        "  .addEdge(START, \"model\")\n",
        "  .addEdge(\"model\", END);\n",
        "\n",
        "const app3 = workflow3.compile({ checkpointer: new MemorySaver() });"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekF4R7ioefFo6PmOYo3YuCbGpROq\",\n",
            "  \"content\": \"¡Hola, Bob! ¿Cómo puedo ayudarte hoy?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 32,\n",
            "      \"completionTokens\": 11,\n",
            "      \"totalTokens\": 43\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 32,\n",
            "      \"completion_tokens\": 11,\n",
            "      \"total_tokens\": 43,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_39a40c96a0\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 11,\n",
            "    \"input_tokens\": 32,\n",
            "    \"total_tokens\": 43,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const config4 = { configurable: { thread_id: uuidv4() } }\n",
        "const input6 = {\n",
        "  messages: [\n",
        "    {\n",
        "      role: \"user\",\n",
        "      content:  \"Hi im bob\"\n",
        "    }\n",
        "  ],\n",
        "  language: \"Spanish\"\n",
        "}\n",
        "const output7 = await app3.invoke(input6, config4)\n",
        "console.log(output7.messages[output7.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "请注意，整个状态都会被持久化，因此如果没有更改需求，我们可以省略诸如 `language` 之类的参数："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekF8yN7H81ITccWlBzSahmduP69T\",\n",
            "  \"content\": \"Tu nombre es Bob. ¿En qué puedo ayudarte, Bob?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 56,\n",
            "      \"completionTokens\": 13,\n",
            "      \"totalTokens\": 69\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 56,\n",
            "      \"completion_tokens\": 13,\n",
            "      \"total_tokens\": 69,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 13,\n",
            "    \"input_tokens\": 56,\n",
            "    \"total_tokens\": 69,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const input7 = {\n",
        "  messages: [\n",
        "    {\n",
        "      role: \"user\",\n",
        "      content:  \"What is my name?\"\n",
        "    }\n",
        "  ],\n",
        "}\n",
        "const output8 = await app3.invoke(input7, config4)\n",
        "console.log(output8.messages[output8.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "为了帮助您了解内部发生的情况，请查看 [此 LangSmith 跟踪](https://smith.langchain.com/public/d61630b7-6a52-4dc9-974c-8452008c498a/r)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 管理对话历史\n",
        "\n",
        "构建聊天机器人时需要理解的一个重要概念是如何管理对话历史。如果任其发展，消息列表将无限增长，并可能导致超出LLM的上下文窗口限制。因此，重要的是增加一个步骤来限制传入消息的大小。\n",
        "\n",
        "**需要注意的是，此步骤应在应用提示模板之前，但在从消息历史中加载先前消息之后进行。**\n",
        "\n",
        "我们可以通过在提示模板前添加一个简单的步骤来实现这一点，该步骤适当地修改`messages`键，然后将这个新的链封装到消息历史类中。\n",
        "\n",
        "LangChain提供了一些内建的帮助函数来[管理消息列表](/docs/how_to/#messages)。在本例中，我们将使用[trimMessages](/docs/how_to/trim_messages/)工具来减少发送给模型的消息数量。该修剪器允许我们指定需要保留的token数量，以及其他参数，例如是否始终保留系统消息以及是否允许部分消息："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  SystemMessage {\n",
            "    \"content\": \"you're a good assistant\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {}\n",
            "  },\n",
            "  HumanMessage {\n",
            "    \"content\": \"I like vanilla ice cream\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {}\n",
            "  },\n",
            "  AIMessage {\n",
            "    \"content\": \"nice\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {},\n",
            "    \"tool_calls\": [],\n",
            "    \"invalid_tool_calls\": []\n",
            "  },\n",
            "  HumanMessage {\n",
            "    \"content\": \"whats 2 + 2\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {}\n",
            "  },\n",
            "  AIMessage {\n",
            "    \"content\": \"4\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {},\n",
            "    \"tool_calls\": [],\n",
            "    \"invalid_tool_calls\": []\n",
            "  },\n",
            "  HumanMessage {\n",
            "    \"content\": \"thanks\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {}\n",
            "  },\n",
            "  AIMessage {\n",
            "    \"content\": \"no problem!\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {},\n",
            "    \"tool_calls\": [],\n",
            "    \"invalid_tool_calls\": []\n",
            "  },\n",
            "  HumanMessage {\n",
            "    \"content\": \"having fun?\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {}\n",
            "  },\n",
            "  AIMessage {\n",
            "    \"content\": \"yes!\",\n",
            "    \"additional_kwargs\": {},\n",
            "    \"response_metadata\": {},\n",
            "    \"tool_calls\": [],\n",
            "    \"invalid_tool_calls\": []\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import { SystemMessage, HumanMessage, AIMessage, trimMessages } from \"@langchain/core/messages\"\n",
        "\n",
        "const trimmer = trimMessages({\n",
        "  maxTokens: 10,\n",
        "  strategy: \"last\",\n",
        "  tokenCounter: (msgs) => msgs.length,\n",
        "  includeSystem: true,\n",
        "  allowPartial: false,\n",
        "  startOn: \"human\",\n",
        "})\n",
        "\n",
        "const messages = [\n",
        "    new SystemMessage(\"you're a good assistant\"),\n",
        "    new HumanMessage(\"hi! I'm bob\"),\n",
        "    new AIMessage(\"hi!\"),\n",
        "    new HumanMessage(\"I like vanilla ice cream\"),\n",
        "    new AIMessage(\"nice\"),\n",
        "    new HumanMessage(\"whats 2 + 2\"),\n",
        "    new AIMessage(\"4\"),\n",
        "    new HumanMessage(\"thanks\"),\n",
        "    new AIMessage(\"no problem!\"),\n",
        "    new HumanMessage(\"having fun?\"),\n",
        "    new AIMessage(\"yes!\"),\n",
        "]\n",
        "\n",
        "await trimmer.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "在我们的链中使用它时，只需在将 `messages` 输入传递给提示之前运行修剪器即可。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "const callModel4 = async (state: typeof GraphAnnotation.State) => {\n",
        "  // highlight-start\n",
        "  const trimmedMessage = await trimmer.invoke(state.messages);\n",
        "  const prompt = await promptTemplate2.invoke({ messages: trimmedMessage, language: state.language });\n",
        "  const response = await llm.invoke(prompt);\n",
        "  // highlight-end\n",
        "  return { messages: [response] };\n",
        "};\n",
        "\n",
        "\n",
        "const workflow4 = new StateGraph(GraphAnnotation)\n",
        "  .addNode(\"model\", callModel4)\n",
        "  .addEdge(START, \"model\")\n",
        "  .addEdge(\"model\", END);\n",
        "\n",
        "const app4 = workflow4.compile({ checkpointer: new MemorySaver() });"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在如果我们尝试问模型我们的名字，它不会知道，因为我们已经删掉了聊天记录中的那部分内容："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekHyVN7f0Pnuyc2RHVL8CxKmFfMQ\",\n",
            "  \"content\": \"I don't know your name. You haven't shared it yet!\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 97,\n",
            "      \"completionTokens\": 12,\n",
            "      \"totalTokens\": 109\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 97,\n",
            "      \"completion_tokens\": 12,\n",
            "      \"total_tokens\": 109,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 12,\n",
            "    \"input_tokens\": 97,\n",
            "    \"total_tokens\": 109,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const config5 = { configurable: { thread_id: uuidv4() }}\n",
        "const input8 = {\n",
        "  // highlight-next-line\n",
        "  messages: [...messages, new HumanMessage(\"What is my name?\")],\n",
        "  language: \"English\"\n",
        "}\n",
        "\n",
        "const output9 = await app4.invoke(\n",
        "  input8,\n",
        "  config5,\n",
        ")\n",
        "console.log(output9.messages[output9.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "但如果我们在最近的几条消息中询问信息，它是能记住的："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AekI1jwlErzHuZ3BhAxr97Ct818Pp\",\n",
            "  \"content\": \"You asked what 2 + 2 equals.\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 99,\n",
            "      \"completionTokens\": 10,\n",
            "      \"totalTokens\": 109\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 99,\n",
            "      \"completion_tokens\": 10,\n",
            "      \"total_tokens\": 109,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0,\n",
            "        \"audio_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 10,\n",
            "    \"input_tokens\": 99,\n",
            "    \"total_tokens\": 109,\n",
            "    \"input_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"audio\": 0,\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const config6 = { configurable: { thread_id: uuidv4() }}\n",
        "const input9 = {\n",
        "  // highlight-next-line\n",
        "  messages: [...messages, new HumanMessage(\"What math problem did I ask?\")],\n",
        "  language: \"English\"\n",
        "}\n",
        "\n",
        "const output10 = await app4.invoke(\n",
        "  input9,\n",
        "  config6,\n",
        ")\n",
        "console.log(output10.messages[output10.messages.length - 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如果你查看 LangSmith，你可以确切地看到 [LangSmith 追踪](https://smith.langchain.com/public/ac63745d-8429-4ae5-8c11-9ec79d9632f2/r) 中底层发生了什么。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 下一步\n",
        "\n",
        "既然你已经了解了如何在 LangChain 中创建聊天机器人的基础知识，你可能还会对以下更高级的教程感兴趣：\n",
        "\n",
        "- [对话式 RAG](/docs/tutorials/qa_chat_history)：在外部数据源上实现聊天机器人体验\n",
        "- [代理](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/agent_supervisor/)：构建能够执行操作的聊天机器人\n",
        "\n",
        "如果你想深入了解具体细节，以下内容也值得查阅：\n",
        "\n",
        "- [流式传输](/docs/how_to/streaming)：流式传输对于聊天应用至关重要\n",
        "- [如何添加消息历史记录](/docs/how_to/message_history)：深入了解与消息历史相关的所有内容\n",
        "- [如何管理大量消息历史记录](/docs/how_to/trim_messages/)：管理大量聊天历史的更多技巧\n",
        "- [LangGraph 主文档](https://langchain-ai.github.io/langgraphjs/)：深入了解使用 LangGraph 构建应用的更多细节"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}