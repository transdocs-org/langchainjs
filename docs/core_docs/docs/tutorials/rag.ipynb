{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5630b0ca",
      "metadata": {},
      "source": [
        "# 构建一个检索增强生成（RAG）应用：第一部分\n",
        "\n",
        "由大语言模型（LLM）赋能的最强大应用之一是复杂的问答（Q&A）聊天机器人。这类应用能够回答关于特定信息源的问题。这些应用使用一种称为检索增强生成（Retrieval Augmented Generation，简称 RAG）的技术，或称为 [RAG](/docs/concepts/rag/)。\n",
        "\n",
        "这是一个多部分教程：\n",
        "\n",
        "- [第一部分](/docs/tutorials/rag)（本指南）介绍 RAG 并演示一个最小实现。\n",
        "- [第二部分](/docs/tutorials/qa_chat_history) 将扩展实现，以支持对话式交互和多步骤检索过程。\n",
        "\n",
        "本教程将展示如何构建一个简单的问答应用\n",
        "针对文本数据源。在此过程中，我们将介绍一个典型的问答\n",
        "架构，并介绍更多高级问答技术的附加资源。我们还将看到\n",
        "LangSmith 如何帮助我们追踪和理解我们的应用。\n",
        "随着我们的应用复杂度增加，LangSmith 将变得越来越有用。\n",
        "\n",
        "如果您已经熟悉基本的检索，您可能也会对\n",
        "这个 [不同检索技术的高层次概述](/docs/concepts/retrieval) 感兴趣。\n",
        "\n",
        "**注意**：这里我们专注于非结构化数据的问答。如果您对结构化数据上的 RAG 感兴趣，请查看我们的教程：[针对 SQL 数据的问答](/docs/tutorials/sql_qa)。\n",
        "\n",
        "## 概览\n",
        "典型的 RAG 应用有两个主要组成部分：\n",
        "\n",
        "**索引**：一个用于从信息源中提取数据并建立索引的流水线。*这通常在离线状态下进行。*\n",
        "\n",
        "**检索与生成**：实际的 RAG 链条，它在运行时接收用户查询，从索引中检索相关数据，然后将这些数据传递给模型。\n",
        "\n",
        "注意：本教程的索引部分将主要遵循 [语义搜索教程](/docs/tutorials/retrievers)。\n",
        "\n",
        "从原始数据到答案的最常见完整流程如下所示：\n",
        "\n",
        "### 索引\n",
        "1. **加载**：首先我们需要加载数据。这通过 [文档加载器](/docs/concepts/document_loaders) 来完成。\n",
        "2. **拆分**：[文本拆分器](/docs/concepts/text_splitters) 将大型 `文档` 拆分为更小的块。这对于索引数据和传递给模型都很有用，因为较大的块难以搜索，而且无法适应模型有限的上下文窗口。\n",
        "3. **存储**：我们需要一个地方来存储和索引我们的拆分块，以便以后可以进行搜索。这通常使用 [向量存储](/docs/concepts/vectorstores) 和 [嵌入模型](/docs/concepts/embedding_models) 来完成。\n",
        "\n",
        "![index_diagram](../../static/img/rag_indexing.png)\n",
        "\n",
        "### 检索与生成\n",
        "4. **检索**：给定一个用户输入，使用 [检索器](/docs/concepts/retrievers) 从存储中检索相关的拆分块。\n",
        "5. **生成**：使用包含问题和检索到的数据的提示，由 [聊天模型](/docs/concepts/chat_models) / [大语言模型（LLM）](/docs/concepts/text_llms) 生成答案。\n",
        "\n",
        "![retrieval_diagram](../../static/img/rag_retrieval_generation.png)\n",
        "\n",
        "一旦我们完成了数据的索引，我们将使用 [LangGraph](https://langchain-ai.github.io/langgraphjs/) 作为我们的编排框架来实现检索和生成步骤。\n",
        "\n",
        "## 准备工作\n",
        "\n",
        "### Jupyter Notebook\n",
        "\n",
        "本教程和其他教程或许最方便在 [Jupyter Notebook](https://jupyter.org/) 中运行。在交互式环境中学习指南是更好地理解它们的好方法。有关安装说明，请参阅 [此处](https://jupyter.org/install)。\n",
        "\n",
        "### 安装\n",
        "\n",
        "本指南需要以下依赖项：\n",
        "\n",
        "```{=mdx}\n",
        "import Npm2Yarn from '@theme/Npm2Yarn';\n",
        "\n",
        "<Npm2Yarn>\n",
        "  langchain @langchain/core @langchain/langgraph\n",
        "</Npm2Yarn>\n",
        "```\n",
        "\n",
        "如需更多详细信息，请参阅我们的 [安装指南](/docs/how_to/installation)。\n",
        "\n",
        "### LangSmith\n",
        "\n",
        "你使用 LangChain 构建的许多应用将包含多个步骤，并涉及多次调用 LLM。\n",
        "随着这些应用变得越来越复杂，能够检查链或代理内部发生的事情变得至关重要。\n",
        "最佳方式是使用 [LangSmith](https://smith.langchain.com)。\n",
        "\n",
        "在上方链接注册后，请确保设置您的环境变量以开始记录追踪信息：\n",
        "\n",
        "```shell\n",
        "export LANGSMITH_TRACING=\"true\"\n",
        "export LANGSMITH_API_KEY=\"...\"\n",
        "\n",
        "# 如果您不在无服务器环境中，可减少追踪延迟\n",
        "# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff1b425",
      "metadata": {},
      "source": [
        "## 组件\n",
        "\n",
        "我们需要从LangChain的集成套件中选择三个组件。\n",
        "\n",
        "一个 [聊天模型](/docs/integrations/chat/)：\n",
        "\n",
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs customVarName=\"llm\" />\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "26ef9d35",
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "import { ChatOpenAI } from '@langchain/openai';\n",
        "\n",
        "const llm = new ChatOpenAI({\n",
        "  model: \"gpt-4o-mini\",\n",
        "  temperature: 0,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b78672-f21e-4827-843e-59514d18ca20",
      "metadata": {},
      "source": [
        "一个[嵌入模型](/docs/integrations/text_embedding/)：\n",
        "\n",
        "```{=mdx}\n",
        "import EmbeddingTabs from \"@theme/EmbeddingTabs\";\n",
        "\n",
        "<EmbeddingTabs/>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a199c764-5dfd-45cf-a4d4-731f2c3d474f",
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
        "\n",
        "const embeddings = new OpenAIEmbeddings({model: \"text-embedding-3-large\"});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "859ffca8-055e-4f5a-95fe-55906ed1d63f",
      "metadata": {},
      "source": [
        "以及一个[向量存储](/docs/integrations/vectorstores/)：\n",
        "\n",
        "```{=mdx}\n",
        "import VectorStoreTabs from \"@theme/VectorStoreTabs\";\n",
        "\n",
        "<VectorStoreTabs/>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f4db6b46-ea3f-4994-9d54-d7c84beb50cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
        "\n",
        "const vectorStore = new MemoryVectorStore(embeddings);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b2d316-922c-4318-b72d-486fd6813b94",
      "metadata": {},
      "source": [
        "## 预览\n",
        "\n",
        "在本指南中，我们将构建一个可以回答有关网站内容问题的应用程序。我们使用的特定网站是Lilian Weng撰写的[LLM驱动的自主代理](https://lilianweng.github.io/posts/2023-06-23-agent/)博客文章，\n",
        "该文章允许我们针对帖子的内容提出问题。\n",
        "\n",
        "我们可以创建一个简单的索引管道和RAG链，用大约50行代码实现此功能。\n",
        "\n",
        "```javascript\n",
        "import \"cheerio\";\n",
        "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
        "import { Document } from \"@langchain/core/documents\";\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "import { pull } from \"langchain/hub\";\n",
        "import { Annotation, StateGraph } from \"@langchain/langgraph\";\n",
        "import { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n",
        "\n",
        "\n",
        "// 加载并分块博客内容\n",
        "const pTagSelector = \"p\";\n",
        "const cheerioLoader = new CheerioWebBaseLoader(\n",
        "  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "  {\n",
        "    selector: pTagSelector\n",
        "  }\n",
        ");\n",
        "\n",
        "const docs = await cheerioLoader.load();\n",
        "\n",
        "const splitter = new RecursiveCharacterTextSplitter({\n",
        "  chunkSize: 1000, chunkOverlap: 200\n",
        "});\n",
        "const allSplits = await splitter.splitDocuments(docs);\n",
        "\n",
        "\n",
        "// 索引分块\n",
        "await vectorStore.addDocuments(allSplits)\n",
        "\n",
        "// 定义问答的提示词模板\n",
        "const promptTemplate = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\n",
        "\n",
        "// 定义应用程序的状态\n",
        "const InputStateAnnotation = Annotation.Root({\n",
        "  question: Annotation<string>,\n",
        "});\n",
        "\n",
        "const StateAnnotation = Annotation.Root({\n",
        "  question: Annotation<string>,\n",
        "  context: Annotation<Document[]>,\n",
        "  answer: Annotation<string>,\n",
        "});\n",
        "\n",
        "// 定义应用程序步骤\n",
        "const retrieve = async (state: typeof InputStateAnnotation.State) => {\n",
        "  const retrievedDocs = await vectorStore.similaritySearch(state.question)\n",
        "  return { context: retrievedDocs };\n",
        "};\n",
        "\n",
        "\n",
        "const generate = async (state: typeof StateAnnotation.State) => {\n",
        "  const docsContent = state.context.map(doc => doc.pageContent).join(\"\\n\");\n",
        "  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });\n",
        "  const response = await llm.invoke(messages);\n",
        "  return { answer: response.content };\n",
        "};\n",
        "\n",
        "\n",
        "// 编译应用程序并进行测试\n",
        "const graph = new StateGraph(StateAnnotation)\n",
        "  .addNode(\"retrieve\", retrieve)\n",
        "  .addNode(\"generate\", generate)\n",
        "  .addEdge(\"__start__\", \"retrieve\")\n",
        "  .addEdge(\"retrieve\", \"generate\")\n",
        "  .addEdge(\"generate\", \"__end__\")\n",
        "  .compile();\n",
        "```\n",
        "\n",
        "```javascript\n",
        "let inputs = { question: \"什么是任务分解？\" };\n",
        "\n",
        "const result = await graph.invoke(inputs);\n",
        "console.log(result.answer)\n",
        "```\n",
        "\n",
        "```\n",
        "任务分解是将复杂任务分解为更小、更易管理步骤的过程。这可以通过多种方法实现，包括提示大型语言模型（LLM）或使用任务特定指令。诸如思维链（CoT）和思维树（Tree of Thoughts）等技术通过结构化推理并在每一步探索多种可能性，进一步增强了这一过程。\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff8204b-dabc-4790-80ea-50d4cf4fceb0",
      "metadata": {},
      "source": [
        "查看 [LangSmith\n",
        "追踪](https://smith.langchain.com/public/84a36239-b466-41bd-ac84-befc33ab50df/r)。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa9ea6a-f914-4f50-8e35-52e6c34b8001",
      "metadata": {},
      "source": [
        "## 详细步骤说明\n",
        "\n",
        "让我们逐步浏览上面的代码，以真正理解\n",
        "其中发生的事情。\n",
        "\n",
        "## 1. 索引 {#indexing}\n",
        "\n",
        "```{=mdx}\n",
        ":::note\n",
        "\n",
        "本节是[语义搜索教程](/docs/tutorials/retrievers)内容的简要版本。\n",
        "如果您已熟悉[文档加载器](/docs/concepts/document_loaders)、[嵌入模型](/docs/concepts/embedding_models)和[向量存储](/docs/concepts/vectorstores)，\n",
        "可以跳至下一节[检索与生成](/docs/tutorials/rag/#orchestration)。\n",
        "\n",
        ":::\n",
        "```\n",
        "\n",
        "### 加载文档\n",
        "\n",
        "我们首先需要加载博客文章内容。我们可以使用[文档加载器](/docs/concepts/document_loaders)，它可以从数据源加载数据并返回一个[文档](https://api.js.langchain.com/classes/langchain_core.documents.Document.html)列表。文档是一个包含某些页面内容（`string`类型）和元数据（`Record<string, any>`类型）的对象。\n",
        "\n",
        "在本例中，我们将使用[CheerioWebBaseLoader](https://api.js.langchain.com/classes/langchain.document_loaders_web_cheerio.CheerioWebBaseLoader.html)，它使用cheerio从网页URL加载HTML并将其解析为文本。我们可以在构造函数中传递自定义选择器，以便仅解析特定元素："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "38d1a436-c60e-4709-877d-f5767d8e4eed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total characters: 22360\n"
          ]
        }
      ],
      "source": [
        "import \"cheerio\";\n",
        "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
        "\n",
        "const pTagSelector = \"p\";\n",
        "const cheerioLoader = new CheerioWebBaseLoader(\n",
        "  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "  {\n",
        "    selector: pTagSelector\n",
        "  }\n",
        ");\n",
        "\n",
        "const docs = await cheerioLoader.load();\n",
        "\n",
        "console.assert(docs.length === 1);\n",
        "console.log(`Total characters: ${docs[0].pageContent.length}`);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3305194a-5fde-45af-ab30-ff8c9a70b123",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:A complicated task usually involv\n"
          ]
        }
      ],
      "source": [
        "console.log(docs[0].pageContent.slice(0, 500));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f11795-e19f-4697-bc6e-6d477355a1cd",
      "metadata": {},
      "source": [
        "#### 深入了解\n",
        "\n",
        "`DocumentLoader`: 一个将数据从数据源加载为文档列表的类。\n",
        "\n",
        "- [文档](/docs/concepts/document_loaders)：有关如何使用的详细说明。\n",
        "- [集成](/docs/integrations/document_loaders/)\n",
        "- [接口](https://api.js.langchain.com/classes/langchain.document_loaders_base.BaseDocumentLoader.html)：基础接口的 API 参考。\n",
        "\n",
        "### 拆分文档\n",
        "\n",
        "我们加载的文档超过 42,000 个字符，这对于许多模型的上下文窗口来说太长了。即使是那些可以在其上下文窗口中容纳完整文章的模型，也可能在非常长的输入中查找信息时遇到困难。\n",
        "\n",
        "为了处理这个问题，我们会将 `Document` 拆分成块进行嵌入和向量存储。这将帮助我们在运行时仅检索博客文章中最相关的部分。\n",
        "\n",
        "如[语义搜索教程](/docs/tutorials/retrievers)中所述，我们使用[RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter)递归地使用常见的分隔符（如换行符）拆分文档，直到每个块的大小合适为止。这是推荐用于通用文本用例的文本拆分器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4870cbe1-9cf4-4ba2-a759-b2f3fcb4c677",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split blog post into 29 sub-documents.\n"
          ]
        }
      ],
      "source": [
        "import { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n",
        "\n",
        "const splitter = new RecursiveCharacterTextSplitter({\n",
        "  chunkSize: 1000, chunkOverlap: 200\n",
        "});\n",
        "const allSplits = await splitter.splitDocuments(docs);\n",
        "console.log(`Split blog post into ${allSplits.length} sub-documents.`);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5193e01-6cf1-45b9-9ba5-38caf75162a6",
      "metadata": {},
      "source": [
        "#### 深入了解\n",
        "\n",
        "`TextSplitter`：将`Document`列表拆分为更小块的对象，是`DocumentTransformers`的子类。\n",
        "\n",
        "- 探索 `上下文感知拆分器（Context-aware splitters）`，它会保留每个拆分在原始`Document`中的位置（“上下文”）：\n",
        "\n",
        "- [Markdown文件](/docs/how_to/code_splitter/#markdown)\n",
        "\n",
        "- [代码](/docs/how_to/code_splitter/)（支持15+种语言）\n",
        "\n",
        "- [接口](https://api.js.langchain.com/classes/langchain_textsplitters.TextSplitter.html)：基础接口的API参考。\n",
        "\n",
        "`DocumentTransformer`：对`Document`列表执行转换的对象。\n",
        "\n",
        "- 文档：关于如何使用`DocumentTransformer`的详细文档\n",
        "\n",
        "- [集成](/docs/integrations/document_transformers)\n",
        "\n",
        "- [接口](https://api.js.langchain.com/classes/langchain_core.documents.BaseDocumentTransformer.html)：基础接口的API参考。\n",
        "\n",
        "### 存储文档\n",
        "\n",
        "现在我们需要对我们拆分的66个文本块进行索引，以便在运行时进行搜索。\n",
        "根据[语义搜索教程](/docs/tutorials/retrievers)，我们的方法是对每个文档拆分的内容进行[嵌入（embed）]，并将这些嵌入向量插入到[向量存储（vector store）]中。\n",
        "对于给定的输入查询，我们可以使用向量搜索来检索相关文档。\n",
        "\n",
        "我们可以使用在[本教程开始时](/docs/tutorials/rag/#components)选择的向量存储和嵌入模型，通过一个命令来嵌入并存储我们所有的文档拆分内容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c3ded19-9bcb-46fe-936c-a65a17499b31",
      "metadata": {},
      "outputs": [],
      "source": [
        "await vectorStore.addDocuments(allSplits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57666234-a5b3-4abc-b079-755241bb2b98",
      "metadata": {},
      "source": [
        "#### 深入了解\n",
        "\n",
        "`Embeddings`: 文本嵌入模型的封装，用于将文本转换为嵌入向量。- [文档](/docs/concepts/embedding_models)：有关如何使用嵌入的详细文档。- [集成](/docs/integrations/text_embedding)：可选择的30多个集成。- [接口](https://api.js.langchain.com/classes/langchain_core.embeddings.Embeddings.html)：基础接口的API参考。\n",
        "\n",
        "`VectorStore`: 向量数据库的封装，用于存储和查询嵌入向量。- [文档](/docs/concepts/vectorstores)：有关如何使用向量存储的详细文档。- [集成](/docs/integrations/vectorstores)：可选择的40多个集成。- [接口](https://api.js.langchain.com/classes/langchain_core.vectorstores.VectorStore.html)：基础接口的API参考。\n",
        "\n",
        "这完成了管道中的**索引**部分。此时，我们已经有了一个可查询的向量存储，其中包含我们博客文章的分块内容。对于用户的问题，我们理想上应该能够返回回答该问题的博客文章片段。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72a8a4be-5214-412b-b807-efde1e201a8f",
      "metadata": {},
      "source": [
        "## 2. 检索与生成 {#orchestration}\n",
        "\n",
        "现在让我们编写实际的应用程序逻辑。我们想要创建一个简单的\n",
        "应用程序，该程序接收用户的提问，搜索与该问题相关的\n",
        "文档，将检索到的文档和初始问题传递给\n",
        "模型，并返回答案。\n",
        "\n",
        "在生成方面，我们将使用在本教程[开始时](/docs/tutorials/rag/#components)选择的聊天模型。\n",
        "\n",
        "我们将为RAG使用一个已提交到LangChain提示中心的提示词\n",
        "([链接](https://smith.langchain.com/hub/rlm/rag-prompt))。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6d833d29-4b51-4320-ad3b-ac6b5d5348de",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: (question goes here) \n",
            "Context: (context goes here) \n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "import { pull } from \"langchain/hub\";\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "const promptTemplate = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\n",
        "\n",
        "// Example:\n",
        "const example_prompt = await promptTemplate.invoke(\n",
        "    { context: \"(context goes here)\", question: \"(question goes here)\" }\n",
        ")\n",
        "const example_messages = example_prompt.messages\n",
        "\n",
        "console.assert(example_messages.length === 1);\n",
        "example_messages[0].content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77dfe84d-cc19-4227-bee4-56b69508ab11",
      "metadata": {},
      "source": [
        "我们将使用 [LangGraph](https://langchain-ai.github.io/langgraphjs/) 将检索和生成步骤整合为一个应用程序。这将带来以下好处：\n",
        "\n",
        "- 我们可以一次性定义应用程序逻辑，并自动支持多种调用模式，包括流式传输、异步和批量调用。\n",
        "- 通过 [LangGraph 平台](https://langchain-ai.github.io/langgraphjs/concepts/langgraph_platform/) 实现简化的部署。\n",
        "- LangSmith 将自动对应用程序的各个步骤进行追踪。\n",
        "- 我们可以轻松地为应用程序添加关键功能，包括 [状态持久化](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) 和 [人工回路审批](https://langchain-ai.github.io/langgraphjs/concepts/human_in_the_loop/)，同时只需进行极少的代码更改。\n",
        "\n",
        "要使用 LangGraph，我们需要定义三个要素：\n",
        "\n",
        "1. 应用程序的状态；\n",
        "2. 应用程序的节点（即应用程序步骤）；\n",
        "3. 应用程序的“控制流程”（例如步骤的执行顺序）。\n",
        "\n",
        "#### 状态：\n",
        "\n",
        "[状态](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#state) 控制着应用程序的输入数据、各步骤间传递的数据以及应用程序的输出数据。\n",
        "\n",
        "对于一个简单的 RAG 应用程序，我们只需跟踪输入问题、检索到的上下文以及生成的答案。\n",
        "\n",
        "关于如何定义图状态的更多信息，请参阅 [此处](https://langchain-ai.github.io/langgraphjs/how-tos/define-state/)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6b765f77-f820-485c-a8a2-af744dc7426b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { Document } from \"@langchain/core/documents\";\n",
        "import { Annotation } from \"@langchain/langgraph\";\n",
        "\n",
        "\n",
        "const InputStateAnnotation = Annotation.Root({\n",
        "  question: Annotation<string>,\n",
        "});\n",
        "\n",
        "\n",
        "const StateAnnotation = Annotation.Root({\n",
        "  question: Annotation<string>,\n",
        "  context: Annotation<Document[]>,\n",
        "  answer: Annotation<string>,\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77868d9a-892f-4b2c-b706-850f96b4464f",
      "metadata": {},
      "source": [
        "#### 节点（应用步骤）\n",
        "\n",
        "让我们从两个步骤的简单序列开始：检索和生成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7cc48297-f9db-421b-b4b7-6d1bb9dc7931",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { concat } from \"@langchain/core/utils/stream\";\n",
        "\n",
        "const retrieve = async (state: typeof InputStateAnnotation.State) => {\n",
        "  const retrievedDocs = await vectorStore.similaritySearch(state.question)\n",
        "  return { context: retrievedDocs };\n",
        "};\n",
        "\n",
        "\n",
        "const generate = async (state: typeof StateAnnotation.State) => {\n",
        "  const docsContent = state.context.map(doc => doc.pageContent).join(\"\\n\");\n",
        "  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });\n",
        "  const response = await llm.invoke(messages);\n",
        "  return { answer: response.content };\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ac9dc3-d73d-48c3-be05-4b60e0b8bc17",
      "metadata": {},
      "source": [
        "我们的检索步骤只是使用输入的问题运行相似性搜索，而生成步骤则将检索到的上下文和原始问题格式化为一个提示，供聊天模型使用。\n",
        "\n",
        "#### 控制流\n",
        "\n",
        "最后，我们将应用程序编译成一个单独的 `graph` 对象。在这种情况下，我们只是将检索和生成步骤连接成一个单一的序列。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9da732c0-7d35-467e-b0d3-16ab73a47766",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph } from \"@langchain/langgraph\";\n",
        "\n",
        "const graph = new StateGraph(StateAnnotation)\n",
        "  .addNode(\"retrieve\", retrieve)\n",
        "  .addNode(\"generate\", generate)\n",
        "  .addEdge(\"__start__\", \"retrieve\")\n",
        "  .addEdge(\"retrieve\", \"generate\")\n",
        "  .addEdge(\"generate\", \"__end__\")\n",
        "  .compile();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "187ef85e-0659-4a19-a153-40876f3aa452",
      "metadata": {},
      "source": [
        "LangGraph 还提供内置工具，用于可视化应用程序的控制流："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "841cbf7f-b98a-45c8-944d-42730d6c6ba7",
      "metadata": {},
      "source": [
        "```javascript\n",
        "// 注意：tslab 只能在 Jupyter Notebook 中运行。无需担心自己运行此代码！\n",
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const image = await graph.getGraph().drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));\n",
        "```\n",
        "\n",
        "![graph_img_rag](../../static/img/graph_img_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f7dc4d-cac8-4be9-b44c-df097dc28c81",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "<details>\n",
        "<summary>我是否需要使用 LangGraph？</summary>\n",
        "```\n",
        "\n",
        "构建一个 RAG 应用程序不需要使用 LangGraph。实际上，我们可以通过调用各个组件来实现相同的应用程序逻辑：\n",
        "\n",
        "```javascript\n",
        "let question = \"...\"\n",
        "\n",
        "const retrievedDocs = await vectorStore.similaritySearch(question)\n",
        "const docsContent = retrievedDocs.map(doc => doc.pageContent).join(\"\\n\");\n",
        "const messages = await promptTemplate.invoke({ question: question, context: docsContent });\n",
        "const answer = await llm.invoke(messages);\n",
        "```\n",
        "\n",
        "LangGraph 提供的好处包括：\n",
        "\n",
        "- 支持多种调用模式：如果我们想要流式传输输出 token 或流式传输各个步骤的结果，则需要重写此逻辑；\n",
        "- 通过 [LangSmith](https://docs.smith.langchain.com/) 自动支持追踪，并通过 [LangGraph Platform](https://langchain-ai.github.io/langgraphjs/concepts/langgraph_platform/) 自动支持部署；\n",
        "- 支持持久化、人工参与循环以及其他功能。\n",
        "\n",
        "许多用例要求在会话体验中使用 RAG，以便用户可以通过有状态的对话获得基于上下文的解答。正如我们将在本教程的[第二部分](/docs/tutorials/qa_chat_history)中看到的那样，LangGraph 对状态的管理和持久化极大地简化了这些应用程序。\n",
        "\n",
        "```{=mdx}\n",
        "</details>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee9c057-5a08-46a3-8c7d-6a314d1e777d",
      "metadata": {},
      "source": [
        "#### 使用方法\n",
        "\n",
        "让我们测试一下我们的应用程序！LangGraph 支持多种调用模式，包括同步、异步和流式传输。\n",
        "\n",
        "调用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "873a174b-3058-4a76-8387-997c2bfaa743",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  Document {\n",
            "    pageContent: \u001b[32m'hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain'\u001b[39m,\n",
            "    metadata: {\n",
            "      source: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[39m,\n",
            "      loc: \u001b[36m[Object]\u001b[39m\n",
            "    },\n",
            "    id: \u001b[90mundefined\u001b[39m\n",
            "  },\n",
            "  Document {\n",
            "    pageContent: \u001b[32m'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al.'\u001b[39m,\n",
            "    metadata: {\n",
            "      source: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[39m,\n",
            "      loc: \u001b[36m[Object]\u001b[39m\n",
            "    },\n",
            "    id: \u001b[90mundefined\u001b[39m\n",
            "  }\n",
            "]\n",
            "\n",
            "Answer: Task decomposition is the process of breaking down complex tasks into smaller, more manageable steps. This can be achieved through various methods, including prompting large language models (LLMs) to outline steps or using task-specific instructions. Techniques like Chain of Thought (CoT) and Tree of Thoughts further enhance this process by structuring reasoning and exploring multiple possibilities at each step.\n"
          ]
        }
      ],
      "source": [
        "let inputs = { question: \"What is Task Decomposition?\" };\n",
        "\n",
        "const result = await graph.invoke(inputs);\n",
        "console.log(result.context.slice(0, 2));\n",
        "console.log(`\\nAnswer: ${result[\"answer\"]}`);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef88f30-40ca-476b-808d-794cb72d401f",
      "metadata": {},
      "source": [
        "流步骤："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5ba9b967-19b6-409a-9a7f-b96a36d0cef9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ question: \u001b[32m'What is Task Decomposition?'\u001b[39m }\n",
            "\n",
            "====\n",
            "\n",
            "{\n",
            "  retrieve: { context: [ \u001b[36m[Document]\u001b[39m, \u001b[36m[Document]\u001b[39m, \u001b[36m[Document]\u001b[39m, \u001b[36m[Document]\u001b[39m ] }\n",
            "}\n",
            "\n",
            "====\n",
            "\n",
            "{\n",
            "  generate: {\n",
            "    answer: \u001b[32m'Task decomposition is the process of breaking down complex tasks into smaller, more manageable steps. This can be achieved through various methods, including prompting large language models (LLMs) or using task-specific instructions. Techniques like Chain of Thought (CoT) and Tree of Thoughts further enhance this process by structuring reasoning and exploring multiple possibilities at each step.'\u001b[39m\n",
            "  }\n",
            "}\n",
            "\n",
            "====\n",
            "\n"
          ]
        }
      ],
      "source": [
        "console.log(inputs)\n",
        "console.log(\"\\n====\\n\");\n",
        "for await (\n",
        "  const chunk of await graph.stream(inputs, {\n",
        "    streamMode: \"updates\",\n",
        "  })\n",
        ") {\n",
        "  console.log(chunk);\n",
        "  console.log(\"\\n====\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f860142d-d50b-4526-a03f-a59a763117fe",
      "metadata": {},
      "source": [
        "流式传输 [tokens](/docs/concepts/tokens/)（需要 `@langchain/core` >= 0.3.24 和 `@langchain/langgraph` >= 0.2.34 并使用上述实现）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "acb80ba0-d5d6-4425-9683-aaeab7081e6c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| This| can| be| achieved| through| various| methods|,| including| prompting| large| language| models| (|LL|Ms|)| to| outline| steps| or| using| task|-specific| instructions|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| further| enhance| this| process| by| struct|uring| reasoning| and| exploring| multiple| possibilities| at| each| step|.||"
          ]
        }
      ],
      "source": [
        "const stream = await graph.stream(\n",
        "  inputs,\n",
        "  { streamMode: \"messages\" },\n",
        ");\n",
        "\n",
        "for await (const [message, _metadata] of stream) {\n",
        "  process.stdout.write(message.content + \"|\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aeb45ad-9bd5-4ee4-8356-9dca9ece76c5",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        ":::note\n",
        "\n",
        "使用当前实现通过 `.invoke` 在 `generate` 步骤中流式传输 token，需要 `@langchain/core` >= 0.3.24 和 `@langchain/langgraph` >= 0.2.34。详细信息请参见[此处](https://langchain-ai.github.io/langgraphjs/how-tos/stream-tokens/)。\n",
        "\n",
        ":::\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "406534d4-66a3-4c27-b277-2bd2f5930cf5",
      "metadata": {},
      "source": [
        "#### 返回来源\n",
        "\n",
        "请注意，通过将检索到的上下文存储在图的状态中，我们可以在状态的 `\"context\"` 字段中获取模型生成答案的来源。有关返回来源的更多详细信息，请参阅[此指南](/docs/how_to/qa_sources/)。\n",
        "\n",
        "#### 深入了解\n",
        "\n",
        "[聊天模型](/docs/concepts/chat_models) 接收一系列消息并返回一条消息。\n",
        "\n",
        "- [文档](/docs/how_to#chat-models)\n",
        "- [集成](/docs/integrations/chat/)：提供25+种集成可供选择。\n",
        "\n",
        "**自定义提示词（prompt）**\n",
        "\n",
        "如上所示，我们可以从提示词中心加载提示词（例如，[这个RAG提示词](https://smith.langchain.com/hub/rlm/rag-prompt)）。提示词也可以轻松自定义。例如："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "93d03690-c858-4891-8ecd-62d6e12a7929",
      "metadata": {},
      "outputs": [],
      "source": [
        "const template = `Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:`\n",
        "\n",
        "const promptTemplateCustom = ChatPromptTemplate.fromMessages(\n",
        "  [\n",
        "    [\"user\", template]\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217cf819-da76-4595-8f75-33f931f1f92a",
      "metadata": {},
      "source": [
        "## 查询分析\n",
        "\n",
        "到目前为止，我们一直在使用原始输入查询执行检索。但是，让模型生成用于检索的查询有一些优势。例如：\n",
        "\n",
        "- 除了语义搜索之外，我们还可以添加结构化过滤器（例如，\"查找2020年以来的文档\"）；\n",
        "- 模型可以将用户查询（可能具有多个方面或包含无关语言）重写为更有效的搜索查询。\n",
        "\n",
        "[查询分析](/docs/concepts/retrieval/#query-analysis) 利用模型将原始用户输入转换或构建为优化的搜索查询。我们可以轻松地在应用程序中加入查询分析步骤。为了便于说明，让我们向向量存储中的文档添加一些元数据。稍后我们将对这些（人为构造的）文档部分进行过滤。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ecf61ed5-6f2e-4b99-a43c-0590336ec2c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  source: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[39m,\n",
            "  loc: { lines: { from: \u001b[33m1\u001b[39m, to: \u001b[33m1\u001b[39m } },\n",
            "  section: \u001b[32m'beginning'\u001b[39m\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const totalDocuments = allSplits.length;\n",
        "const third = Math.floor(totalDocuments / 3);\n",
        "\n",
        "allSplits.forEach((document, i) => {\n",
        "    if (i < third) {\n",
        "        document.metadata[\"section\"] = \"beginning\";\n",
        "    } else if (i < 2 * third) {\n",
        "        document.metadata[\"section\"] = \"middle\";\n",
        "    } else {\n",
        "        document.metadata[\"section\"] = \"end\";\n",
        "    }\n",
        "});\n",
        "\n",
        "allSplits[0].metadata;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114878bd-a334-41ed-8013-ec4ce0a9112b",
      "metadata": {},
      "source": [
        "我们需要更新我们向量存储中的文档。为此，我们将使用一个简单的[MemoryVectorStore](https://api.js.langchain.com/classes/langchain.vectorstores_memory.MemoryVectorStore.html)，因为我们会使用它的一些特定功能（例如，元数据过滤）。有关您选择的向量存储的相关功能，请参阅向量存储[integration文档](/docs/integrations/vectorstores/)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5daf62fd-4086-49ad-8b3a-514c4fa214ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
        "\n",
        "const vectorStoreQA = new MemoryVectorStore(embeddings);\n",
        "await vectorStoreQA.addDocuments(allSplits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e561ad-1193-48ae-8487-7d29cb04e312",
      "metadata": {},
      "source": [
        "接下来，我们为搜索查询定义一个模式。为此，我们将使用[结构化输出](/docs/concepts/structured_outputs/)。在这里，我们定义查询包含一个字符串查询和一个文档部分（\"开始\"、\"中间\"或\"结束\"），但你可以根据自己的需求进行定义。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7b4864dd-172a-441f-8224-0661b156ed29",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { z } from \"zod\";\n",
        "\n",
        "\n",
        "const searchSchema = z.object({\n",
        "  query: z.string().describe(\"Search query to run.\"),\n",
        "  section: z.enum([\"beginning\", \"middle\", \"end\"]).describe(\"Section to query.\"),\n",
        "});\n",
        "\n",
        "const structuredLlm = llm.withStructuredOutput(searchSchema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c908874d-d5d2-4f81-95ad-5fa0295ff6c8",
      "metadata": {},
      "source": [
        "最后，我们在LangGraph应用程序中添加了一个步骤，用于根据用户的原始输入生成查询："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "afdbe4e7-b1e1-41ff-9c9e-0194ebd73049",
      "metadata": {},
      "outputs": [],
      "source": [
        "const StateAnnotationQA = Annotation.Root({\n",
        "  question: Annotation<string>,\n",
        "  // highlight-start\n",
        "  search: Annotation<z.infer<typeof searchSchema>>,\n",
        "  // highlight-end\n",
        "  context: Annotation<Document[]>,\n",
        "  answer: Annotation<string>,\n",
        "});\n",
        "\n",
        "\n",
        "// highlight-start\n",
        "const analyzeQuery = async (state: typeof InputStateAnnotation.State) => {\n",
        "  const result = await structuredLlm.invoke(state.question)\n",
        "  return { search: result }\n",
        "};\n",
        "// highlight-end\n",
        "\n",
        "\n",
        "const retrieveQA = async (state: typeof StateAnnotationQA.State) => {\n",
        "  // highlight-start\n",
        "  const filter = (doc) => doc.metadata.section === state.search.section;\n",
        "  const retrievedDocs = await vectorStore.similaritySearch(\n",
        "    state.search.query,\n",
        "    2,\n",
        "    filter\n",
        "  )\n",
        "  // highlight-end\n",
        "  return { context: retrievedDocs };\n",
        "};\n",
        "\n",
        "\n",
        "const generateQA = async (state: typeof StateAnnotationQA.State) => {\n",
        "  const docsContent = state.context.map(doc => doc.pageContent).join(\"\\n\");\n",
        "  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });\n",
        "  const response = await llm.invoke(messages);\n",
        "  return { answer: response.content };\n",
        "};\n",
        "\n",
        "\n",
        "\n",
        "const graphQA = new StateGraph(StateAnnotationQA)\n",
        "  .addNode(\"analyzeQuery\", analyzeQuery)\n",
        "  .addNode(\"retrieveQA\", retrieveQA)\n",
        "  .addNode(\"generateQA\", generateQA)\n",
        "  .addEdge(\"__start__\", \"analyzeQuery\")\n",
        "  .addEdge(\"analyzeQuery\", \"retrieveQA\")\n",
        "  .addEdge(\"retrieveQA\", \"generateQA\")\n",
        "  .addEdge(\"generateQA\", \"__end__\")\n",
        "  .compile();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea7913a9-3c1b-4008-a573-0fd0afa9824e",
      "metadata": {},
      "source": [
        "```javascript\n",
        "// 注意：tslab 只能在 Jupyter Notebook 内部运行。请勿尝试自行执行此代码！\n",
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const image = await graphQA.getGraph().drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));\n",
        "```\n",
        "\n",
        "![graph_img_rag_qa](../../static/img/graph_img_rag_qa.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "653cf8dc-a201-43ea-9965-02fcfd2fc316",
      "metadata": {},
      "source": [
        "我们可以通过特别要求提供帖子末尾的上下文来测试我们的实现。请注意，模型在其回答中包含了不同的信息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ecb5f0d6-dcda-4929-ac72-d534d547a426",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  question: \u001b[32m'What does the end of the post say about Task Decomposition?'\u001b[39m\n",
            "}\n",
            "\n",
            "====\n",
            "\n",
            "{\n",
            "  analyzeQuery: { search: { query: \u001b[32m'Task Decomposition'\u001b[39m, section: \u001b[32m'end'\u001b[39m } }\n",
            "}\n",
            "\n",
            "====\n",
            "\n",
            "{ retrieveQA: { context: [ \u001b[36m[Document]\u001b[39m, \u001b[36m[Document]\u001b[39m ] } }\n",
            "\n",
            "====\n",
            "\n",
            "{\n",
            "  generateQA: {\n",
            "    answer: \u001b[32m'The end of the post emphasizes the importance of task decomposition by outlining a structured approach to organizing code into separate files and functions. It highlights the need for clarity and compatibility among different components, ensuring that each part of the architecture is well-defined and functional. This methodical breakdown aids in maintaining best practices and enhances code readability and manageability.'\u001b[39m\n",
            "  }\n",
            "}\n",
            "\n",
            "====\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let inputsQA = { question: \"What does the end of the post say about Task Decomposition?\" };\n",
        "\n",
        "console.log(inputsQA)\n",
        "console.log(\"\\n====\\n\");\n",
        "for await (\n",
        "  const chunk of await graphQA.stream(inputsQA, {\n",
        "    streamMode: \"updates\",\n",
        "  })\n",
        ") {\n",
        "  console.log(chunk);\n",
        "  console.log(\"\\n====\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5875a48a-c849-4da9-99e0-558b04884fb0",
      "metadata": {},
      "source": [
        "在流式步骤和 [LangSmith 追踪](https://smith.langchain.com/public/8ff4742c-a5d4-41b2-adf9-22915a876a30/r) 中，我们现在都可以观察到输入到检索步骤中的结构化查询。\n",
        "\n",
        "查询分析是一个具有多种解决方法的复杂问题。更多示例请参考 [操作指南](/docs/how_to/#query-analysis)。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e4d779",
      "metadata": {},
      "source": [
        "## 下一步\n",
        "\n",
        "我们已经介绍了构建基本问答应用程序的步骤：\n",
        "\n",
        "- 使用[文档加载器](/docs/concepts/document_loaders)加载数据\n",
        "- 使用[文本分割器](/docs/concepts/text_splitters)对索引数据进行分块，使其更易于模型使用\n",
        "- 对数据进行[嵌入](/docs/concepts/embedding_models)，并将数据存储在[向量数据库](/docs/how_to/vectorstores)中\n",
        "- 在响应传入问题时，[检索](/docs/concepts/retrievers)之前存储的数据块\n",
        "- 使用检索到的数据块作为上下文生成答案。\n",
        "\n",
        "在本教程的[第二部分](/docs/tutorials/qa_chat_history)中，我们将扩展此处的实现，以支持对话式交互和多步骤检索流程。\n",
        "\n",
        "延伸阅读：\n",
        "\n",
        "- [返回来源](/docs/how_to/qa_sources)：了解如何返回源文档\n",
        "- [流式传输](/docs/how_to/streaming)：了解如何流式传输输出和中间步骤\n",
        "- [添加聊天历史记录](/docs/how_to/message_history)：了解如何将聊天历史记录添加到您的应用中\n",
        "- [检索概念指南](/docs/concepts/retrieval)：关于特定检索技术的高层次概述"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}