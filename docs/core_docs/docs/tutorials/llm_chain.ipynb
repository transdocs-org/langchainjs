{
 "cells": [
  {
   "cell_type": "raw",
   "id": "63ee3f93",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 0\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316da0d",
   "metadata": {},
   "source": [
    "# 使用聊天模型和提示模板构建一个简单的LLM应用\n",
    "\n",
    "在这个快速入门中，我们将向您展示如何使用LangChain构建一个简单的LLM应用。该应用将把文本从英文翻译成另一种语言。这是一个相对简单的LLM应用——它仅包含一次LLM调用和一些提示。尽管如此，这对于开始使用LangChain来说是一个绝佳的方式——许多功能只需一些提示和一次LLM调用就可以实现！\n",
    "\n",
    "阅读完本教程后，您将对以下内容有高层次的了解：\n",
    "\n",
    "- 使用[语言模型](/docs/concepts/chat_models)\n",
    "\n",
    "- 使用[提示模板](/docs/concepts/prompt_templates)\n",
    "\n",
    "- 使用[LangSmith](https://docs.smith.langchain.com/)调试和追踪您的应用\n",
    "\n",
    "让我们开始吧！\n",
    "\n",
    "## 设置\n",
    "\n",
    "### 安装\n",
    "\n",
    "要安装LangChain，请运行以下命令：\n",
    "\n",
    "```{=mdx}\n",
    "import Npm2Yarn from '@theme/Npm2Yarn';\n",
    "import TabItem from '@theme/TabItem';\n",
    "import CodeBlock from \"@theme/CodeBlock\";\n",
    "\n",
    "<Npm2Yarn>\n",
    "  langchain @langchain/core\n",
    "</Npm2Yarn>\n",
    "```\n",
    "\n",
    "有关更多细节，请参阅我们的[安装指南](/docs/how_to/installation/)。\n",
    "\n",
    "### LangSmith\n",
    "\n",
    "您使用LangChain构建的许多应用将包含多个步骤和多次LLM调用。\n",
    "随着这些应用变得越来越复杂，能够检查您的链或代理内部发生了什么变得至关重要。\n",
    "实现此目的的最佳方式是使用[LangSmith](https://smith.langchain.com)。\n",
    "\n",
    "在上方链接注册后，请确保设置您的环境变量以开始记录追踪信息：\n",
    "\n",
    "```shell\n",
    "export LANGSMITH_TRACING=\"true\"\n",
    "export LANGSMITH_API_KEY=\"...\"\n",
    "\n",
    "# 如果您不在无服务器环境中，可减少追踪延迟\n",
    "# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5558ca9",
   "metadata": {},
   "source": [
    "## 使用语言模型\n",
    "\n",
    "首先，我们来学习如何单独使用语言模型。LangChain支持许多不同的语言模型，您可以互换使用它们。有关使用特定模型的详细信息，请参阅[支持的集成](/docs/integrations/chat/)。\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs openaiParams={`{ model: \"gpt-4\" }`} />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337d298c-ac1f-4470-a7db-4931b9bb0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// @lc-docs-hide-cell\n",
    "import { ChatOpenAI } from '@langchain/openai';\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o-mini\",\n",
    "  temperature: 0,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5642ff",
   "metadata": {},
   "source": [
    "首先，我们直接使用模型。[聊天模型](/docs/concepts/chat_models)是LangChain [Runnables](/docs/concepts/runnables/)的实例，这意味着它们提供了一个标准接口用于与之交互。要简单调用模型，我们可以将一组[消息](/docs/concepts/messages/)传递给`.invoke`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ba07c2-fac8-4f44-b70b-67a8598ac862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-AekSfJkg3QIOsk42BH6Qom4Gt159j\",\n",
      "  \"content\": \"Ciao!\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"promptTokens\": 20,\n",
      "      \"completionTokens\": 3,\n",
      "      \"totalTokens\": 23\n",
      "    },\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"usage\": {\n",
      "      \"prompt_tokens\": 20,\n",
      "      \"completion_tokens\": 3,\n",
      "      \"total_tokens\": 23,\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"cached_tokens\": 0,\n",
      "        \"audio_tokens\": 0\n",
      "      },\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"system_fingerprint\": \"fp_6fc10e10eb\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"output_tokens\": 3,\n",
      "    \"input_tokens\": 20,\n",
      "    \"total_tokens\": 23,\n",
      "    \"input_token_details\": {\n",
      "      \"audio\": 0,\n",
      "      \"cache_read\": 0\n",
      "    },\n",
      "    \"output_token_details\": {\n",
      "      \"audio\": 0,\n",
      "      \"reasoning\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage, SystemMessage } from \"@langchain/core/messages\"\n",
    "\n",
    "const messages = [\n",
    "  new SystemMessage(\"将以下内容从英文翻译成意大利语\"),\n",
    "  new HumanMessage(\"hi!\"),\n",
    "];\n",
    "\n",
    "await model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83373db",
   "metadata": {},
   "source": [
    "```{=mdx}\n",
    ":::tip\n",
    "\n",
    "如果我们启用了LangSmith，我们可以看到此运行记录已发送到LangSmith，并可以查看[LangSmith追踪](https://smith.langchain.com/public/45f1a650-38fb-41e1-9b61-becc0684f2ce/r)。LangSmith追踪报告了[令牌](/docs/concepts/tokens/)使用情况、延迟、[标准模型参数](/docs/concepts/chat_models/#standard-parameters)（如温度）以及其他信息。\n",
    "\n",
    ":::\n",
    "```\n",
    "\n",
    "请注意，聊天模型接收[消息](/docs/concepts/messages/)对象作为输入并生成消息对象作为输出。除了文本内容外，消息对象还传达对话[角色](/docs/concepts/messages/#role)，并包含重要数据，如[工具调用](/docs/concepts/tool_calling/)和令牌使用计数。\n",
    "\n",
    "LangChain还支持通过字符串或[OpenAI格式](/docs/concepts/messages/#openai-format)进行聊天模型输入。以下内容是等效的：\n",
    "\n",
    "```javascript\n",
    "await model.invoke(\"Hello\")\n",
    "\n",
    "await model.invoke([ {role: \"user\", content: \"Hello\" }])\n",
    "\n",
    "await model.invoke([new HumanMessage(\"hi!\")])\n",
    "```\n",
    "\n",
    "### 流式传输\n",
    "\n",
    "由于聊天模型是[Runnables](/docs/concepts/runnables/)，它们提供了一个标准接口，其中包括异步和流式调用模式。这允许我们从聊天模型中逐个流式传输令牌："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0caa31af-cdf1-4fc1-9a07-adb7a54dd6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\n",
      "C|\n",
      "iao|\n",
      "!|\n",
      "|\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "const stream = await model.stream(messages);\n",
    "\n",
    "const chunks = [];\n",
    "for await (const chunk of stream) {\n",
    "  chunks.push(chunk);\n",
    "  console.log(`${chunk.content}|`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8da31",
   "metadata": {},
   "source": [
    "## 提示模板\n",
    "\n",
    "目前我们是直接将一组消息列表传递给语言模型。这一组消息来自哪里呢？通常，它是由用户输入和应用逻辑组合构建的。这种应用逻辑通常将原始用户输入转换为准备好传递给语言模型的消息列表。常见的转换包括添加系统消息或使用用户输入格式化模板。\n",
    "\n",
    "[提示模板](/docs/concepts/prompt_templates/)是LangChain中的一个概念，旨在帮助完成这种转换。它们接收原始用户输入并返回准备好传递给语言模型的数据（提示）。\n",
    "\n",
    "让我们在此创建一个提示模板。它将接收两个用户变量：\n",
    "\n",
    "- `language`：要将文本翻译成的语言\n",
    "- `text`：要翻译的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e73cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e876c2a",
   "metadata": {},
   "source": [
    "首先，让我们创建一个字符串，该字符串将用于格式化系统消息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd75ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "const systemTemplate = \"将以下内容从英文翻译成 {language}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf6f13",
   "metadata": {},
   "source": [
    "接下来，我们可以创建提示模板。这将是`systemTemplate`以及用于放置文本的更简单模板的组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e566f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "const promptTemplate = ChatPromptTemplate.fromMessages(\n",
    "  [\n",
    "    [\"system\", systemTemplate],\n",
    "    [\"user\", \"{text}\"]\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9711ba6",
   "metadata": {},
   "source": [
    "请注意，`ChatPromptTemplate`支持单个模板中的多个[消息角色](/docs/concepts/messages/#role)。我们将`language`参数格式化到系统消息中，将用户`text`格式化到用户消息中。\n",
    "\n",
    "此提示模板的输入是一个字典。我们可以单独使用这个提示模板来查看它的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f781b3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue {\n",
      "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
      "  lc_kwargs: {\n",
      "    messages: [\n",
      "      SystemMessage {\n",
      "        \"content\": \"将以下内容从英文翻译成 意大利语\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      },\n",
      "      HumanMessage {\n",
      "        \"content\": \"hi!\",\n",
      "        \"additional_kwargs\": {},\n",
      "        \"response_metadata\": {}\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  lc_namespace: [ \u001b[32m'langchain_core'\u001b[39m, \u001b[32m'prompt_values'\u001b[39m ],\n",
      "  messages: [\n",
      "    SystemMessage {\n",
      "      \"content\": \"将以下内容从英文翻译成 意大利语\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    },\n",
      "    HumanMessage {\n",
      "      \"content\": \"hi!\",\n",
      "      \"additional_kwargs\": {},\n",
      "      \"response_metadata\": {}\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const promptValue = await promptTemplate.invoke({ language: \"意大利语\", text: \"hi!\" })\n",
    "\n",
    "promptValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49ba9e",
   "metadata": {},
   "source": [
    "我们可以看到它返回了一个包含两条消息的`ChatPromptValue`。如果我们想直接访问这些消息，可以这样做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2159b619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  SystemMessage {\n",
      "    \"content\": \"将以下内容从英文翻译成 意大利语\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  },\n",
      "  HumanMessage {\n",
      "    \"content\": \"hi!\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "promptValue.toChatMessages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4267a8",
   "metadata": {},
   "source": [
    "最后，我们可以在格式化后的提示上调用聊天模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f75bd30-c52c-45dc-ac4c-dcb4d81c99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao!\n"
     ]
    }
   ],
   "source": [
    "const response = await model.invoke(promptValue)\n",
    "console.log(`${response.content}`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19cecb",
   "metadata": {},
   "source": [
    "如果我们查看LangSmith追踪，可以看到所有三个组件都显示在[LangSmith追踪](https://smith.langchain.com/public/6529d912-8564-4686-8df8-999c427621a7/r)中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befdb168",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "就是这样！在本教程中，您已经学习了如何创建您的第一个简单的LLM应用。您已经学习了如何使用语言模型、如何创建提示模板，以及如何通过LangSmith获得对您创建的应用的可观测性。\n",
    "\n",
    "这仅是成为一名熟练的AI工程师所需知识的冰山一角。幸运的是——我们还有许多其他资源！\n",
    "\n",
    "有关LangChain核心概念的进一步阅读，我们提供了详细的[概念指南](/docs/concepts)。\n",
    "\n",
    "如果您对这些概念有更具体的问题，请查看以下如何指南部分：\n",
    "\n",
    "- [聊天模型](/docs/how_to/#chat-models)\n",
    "- [提示模板](/docs/how_to/#prompt-templates)\n",
    "\n",
    "以及LangSmith文档：\n",
    "\n",
    "- [LangSmith](https://docs.smith.langchain.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}