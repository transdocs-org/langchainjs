{
  "cells": [
    {
      "cell_type": "raw",
      "id": "023635f2-71cf-43f2-a2e2-a7b4ced30a74",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 2\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604",
      "metadata": {},
      "source": [
        "# 构建一个检索增强生成（RAG）应用：第2部分\n",
        "\n",
        "在许多问答应用中，我们希望允许用户进行来回对话，这意味着应用程序需要对过去的问题和答案有一定的“记忆”，并需要一些逻辑来将这些信息纳入当前的思考中。\n",
        "\n",
        "这是一个多部分教程的第二部分：\n",
        "\n",
        "- [第1部分](/docs/tutorials/rag) 介绍了RAG并演示了一个最小实现。\n",
        "- [第2部分](/docs/tutorials/qa_chat_history)（本指南）扩展了实现，以支持对话式交互和多步骤检索流程。\n",
        "\n",
        "在这里，我们重点介绍**添加用于整合历史消息的逻辑**。这涉及[聊天历史](/docs/concepts/chat_history)的管理。\n",
        "\n",
        "我们将介绍两种方法：\n",
        "\n",
        "1. [链（Chains）](/docs/tutorials/qa_chat_history/#chains)，其中我们最多执行一个检索步骤；\n",
        "2. [代理（Agents）](/docs/tutorials/qa_chat_history/#agents)，其中我们赋予LLM自主决定执行多个检索步骤的权限。\n",
        "\n",
        "```{=mdx}\n",
        ":::note\n",
        "\n",
        "这里介绍的方法利用了现代[聊天模型](/docs/concepts/chat_models)中的[工具调用](/docs/concepts/tool_calling/)功能。支持工具调用功能的模型列表请参见[此页面](/docs/integrations/chat/)。\n",
        "\n",
        ":::\n",
        "```\n",
        "\n",
        "对于外部知识源，我们将使用Lilian Weng撰写的同一篇博客文章[LLM驱动的自主代理](https://lilianweng.github.io/posts/2023-06-23-agent/)，该文章在RAG教程的[第1部分](/docs/tutorials/rag)中已使用过。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487d8d79-5ee9-4aa4-9fdf-cd5f4303e099",
      "metadata": {},
      "source": [
        "## 环境配置\n",
        "\n",
        "### 组件\n",
        "\n",
        "我们需要从 LangChain 的集成套件中选择三个组件。\n",
        "\n",
        "一个 [聊天模型](/docs/integrations/chat/)：\n",
        "\n",
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs customVarName=\"llm\" />\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fab0dd56-7437-4aeb-af20-7f420d47ca94",
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "import { ChatOpenAI } from '@langchain/openai';\n",
        "\n",
        "const llm = new ChatOpenAI({\n",
        "  model: \"gpt-4o-mini\",\n",
        "  temperature: 0,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da14773e-ac98-4a97-944b-4c6ec028d195",
      "metadata": {},
      "source": [
        "一个[嵌入模型](/docs/integrations/text_embedding/)：\n",
        "\n",
        "```{=mdx}\n",
        "import EmbeddingTabs from \"@theme/EmbeddingTabs\";\n",
        "\n",
        "<EmbeddingTabs/>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4691bd31-d8f4-4ba1-aec5-44935400f33c",
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
        "\n",
        "const embeddings = new OpenAIEmbeddings({model: \"text-embedding-3-large\"});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22fdc314-b91d-4820-b0a8-873b5b6e76f5",
      "metadata": {},
      "source": [
        "以及一个[向量存储](/docs/integrations/vectorstores/)：\n",
        "\n",
        "```{=mdx}\n",
        "import VectorStoreTabs from \"@theme/VectorStoreTabs\";\n",
        "\n",
        "<VectorStoreTabs/>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "137d3848-7265-4673-9779-4c5f604da469",
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
        "\n",
        "const vectorStore = new MemoryVectorStore(embeddings);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94bc335b-dc8a-4c40-aece-3aa9057cc6bd",
      "metadata": {},
      "source": [
        "### 依赖项\n",
        "\n",
        "此外，我们将使用以下包：\n",
        "\n",
        "```{=mdx}\n",
        "import Npm2Yarn from '@theme/Npm2Yarn';\n",
        "\n",
        "<Npm2Yarn>\n",
        "  langchain @langchain/community @langchain/langgraph cheerio\n",
        "</Npm2Yarn>\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e207ac1d-4a8e-4172-a9ee-3294519a9a40",
      "metadata": {},
      "source": [
        "### LangSmith\n",
        "\n",
        "使用LangChain构建的许多应用程序将包含多个步骤，并多次调用LLM。随着这些应用程序变得越来越复杂，能够检查链或代理内部确切发生的情况变得至关重要。要做到这一点，最好的方法是使用[LangSmith](https://docs.smith.langchain.com)。\n",
        "\n",
        "请注意，LangSmith并非必需，但它非常有用。如果您确实想使用LangSmith，在上方链接注册后，请确保设置您的环境变量以开始记录追踪信息：\n",
        "\n",
        "\n",
        "```bash\n",
        "export LANGSMITH_TRACING=true\n",
        "export LANGSMITH_API_KEY=您的密钥\n",
        "\n",
        "# 如果您不在无服务器环境中，可减少追踪延迟\n",
        "# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa6ba684-26cf-4860-904e-a4d51380c134",
      "metadata": {},
      "source": [
        "## 链（Chains）\n",
        "\n",
        "让我们首先回顾一下在[第1部分](/docs/tutorials/rag)中构建的向量存储，它索引了Lilian Weng撰写的一篇关于[由LLM驱动的自主代理](https://lilianweng.github.io/posts/2023-06-23-agent/)的博客文章。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ffe06d69-33c9-4ca3-98fb-8c70cde9dba2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import \"cheerio\";\n",
        "import { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n",
        "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
        "\n",
        "// Load and chunk contents of the blog\n",
        "const pTagSelector = \"p\";\n",
        "const cheerioLoader = new CheerioWebBaseLoader(\n",
        "  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "  {\n",
        "    selector: pTagSelector\n",
        "  }\n",
        ");\n",
        "\n",
        "const docs = await cheerioLoader.load();\n",
        "\n",
        "const splitter = new RecursiveCharacterTextSplitter({\n",
        "  chunkSize: 1000, chunkOverlap: 200\n",
        "});\n",
        "const allSplits = await splitter.splitDocuments(docs);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b4369949-39f1-4cdc-b652-179e0b891b51",
      "metadata": {},
      "outputs": [],
      "source": [
        "// Index chunks\n",
        "await vectorStore.addDocuments(allSplits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42c26d5f-1493-4ad6-9210-ea2723695149",
      "metadata": {},
      "source": [
        "在RAG教程的[第一部分](/docs/tutorials/rag)中，我们将用户输入、检索到的上下文和生成的答案表示为状态中的独立键。对话体验可以使用[消息](/docs/concepts/messages/)序列自然地表示。除了用户和助手发送的消息之外，检索到的文档和其他中间结果也可以通过[工具消息](/docs/concepts/messages/#toolmessage)被纳入消息序列中。这促使我们使用一个消息序列来表示RAG应用的状态。具体来说，我们将拥有\n",
        "\n",
        "1. 将用户输入表示为`HumanMessage`;\n",
        "2. 将向量存储查询表示为包含工具调用的`AIMessage`;\n",
        "3. 将检索到的文档表示为`ToolMessage`;\n",
        "4. 将最终回答表示为`AIMessage`。\n",
        "\n",
        "这种状态模型非常灵活，LangGraph为此提供了一个内置版本，以方便使用：\n",
        "```javascript\n",
        "import { MessagesAnnotation, StateGraph } from \"@langchain/langgraph\";\n",
        "\n",
        "const graph = new StateGraph(MessagesAnnotation)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35eeb6a1-29f2-4086-8b6f-8761cf24ce59",
      "metadata": {},
      "source": [
        "利用[工具调用](/docs/concepts/tool_calling/)与检索步骤进行交互还有另一个好处，即检索的查询是由我们的模型生成的。这在对话场景中尤其重要，因为用户的查询可能需要根据聊天历史记录进行上下文化处理。例如，请考虑以下对话：\n",
        "\n",
        "> 人类：\"任务分解是什么？\"\n",
        ">\n",
        "> AI：\"任务分解是指将复杂任务分解为更小、更简单的步骤，以使其对于代理或模型来说更易于处理。\"\n",
        ">\n",
        "> 人类：\"通常有哪些方法？\"\n",
        "\n",
        "在这种情况下，模型可以生成诸如`\"任务分解的常用方法\"`之类的查询。工具调用可以很好地实现这一点。就像在RAG教程的[查询分析](/docs/tutorials/rag#query-analysis)部分一样，这允许模型将用户查询重写为更有效的搜索查询。它还支持无需检索步骤的直接响应（例如，对用户的通用问候做出回应）。\n",
        "\n",
        "让我们将检索步骤变成一个[工具](/docs/concepts/tools)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2f3a87fd-018d-448f-bb63-570cf590d6ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { z } from \"zod\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "\n",
        "const retrieveSchema = z.object({query: z.string()});\n",
        "\n",
        "const retrieve = tool(\n",
        "  async ({ query }) => {\n",
        "    const retrievedDocs = await vectorStore.similaritySearch(query, 2);\n",
        "    const serialized = retrievedDocs.map(\n",
        "      doc => `Source: ${doc.metadata.source}\\nContent: ${doc.pageContent}`\n",
        "    ).join(\"\\n\");\n",
        "    return [\n",
        "      serialized,\n",
        "      retrievedDocs,\n",
        "    ];\n",
        "  },\n",
        "  {\n",
        "    name: \"retrieve\",\n",
        "    description: \"Retrieve information related to a query.\",\n",
        "    schema: retrieveSchema,\n",
        "    responseFormat: \"content_and_artifact\",\n",
        "  }\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b03f752-d46d-4070-b790-197b742c4dc2",
      "metadata": {},
      "source": [
        "有关创建工具的详细信息，请参阅[本指南](/docs/how_to/custom_tools/)。\n",
        "\n",
        "我们的图将包含三个节点：\n",
        "\n",
        "1. 一个处理用户输入的节点，该节点会生成检索器的查询或直接作出响应；\n",
        "2. 一个检索器工具节点，用于执行检索步骤；\n",
        "3. 一个使用检索到的上下文生成最终响应的节点。\n",
        "\n",
        "我们将在下方构建这些节点。请注意，我们使用了另一个预构建的 LangGraph 组件 [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)，它可以执行工具，并将结果作为 `ToolMessage` 添加到状态中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ca2d1da6-ee39-4823-a9f1-be95b1e8a1fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { \n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    ToolMessage\n",
        "} from \"@langchain/core/messages\";\n",
        "import { MessagesAnnotation } from \"@langchain/langgraph\";\n",
        "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "\n",
        "// Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
        "async function queryOrRespond(state: typeof MessagesAnnotation.State) {\n",
        "  const llmWithTools = llm.bindTools([retrieve])\n",
        "  const response = await llmWithTools.invoke(state.messages);\n",
        "  // MessagesState appends messages to state instead of overwriting\n",
        "  return { messages: [response] };\n",
        "}\n",
        "\n",
        "\n",
        "// Step 2: Execute the retrieval.\n",
        "const tools = new ToolNode([retrieve]);\n",
        "\n",
        "\n",
        "// Step 3: Generate a response using the retrieved content.\n",
        "async function generate(state: typeof MessagesAnnotation.State) {\n",
        "  // Get generated ToolMessages\n",
        "  let recentToolMessages = [];\n",
        "    for (let i = state[\"messages\"].length - 1; i >= 0; i--) {\n",
        "      let message = state[\"messages\"][i];\n",
        "      if (message instanceof ToolMessage) {\n",
        "        recentToolMessages.push(message);\n",
        "      } else {\n",
        "        break;\n",
        "      }\n",
        "    }\n",
        "  let toolMessages = recentToolMessages.reverse();\n",
        "  \n",
        "  // Format into prompt\n",
        "  const docsContent = toolMessages.map(doc => doc.content).join(\"\\n\");\n",
        "  const systemMessageContent = \n",
        "    \"You are an assistant for question-answering tasks. \" +\n",
        "    \"Use the following pieces of retrieved context to answer \" +\n",
        "    \"the question. If you don't know the answer, say that you \" +\n",
        "    \"don't know. Use three sentences maximum and keep the \" +\n",
        "    \"answer concise.\" +\n",
        "    \"\\n\\n\" +\n",
        "    `${docsContent}`;\n",
        "\n",
        "  const conversationMessages = state.messages.filter(message => \n",
        "    message instanceof HumanMessage || \n",
        "    message instanceof SystemMessage || \n",
        "    (message instanceof AIMessage && message.tool_calls.length == 0)\n",
        "  );\n",
        "  const prompt = [new SystemMessage(systemMessageContent), ...conversationMessages];\n",
        "\n",
        "  // Run\n",
        "  const response = await llm.invoke(prompt)\n",
        "  return { messages: [response] };\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b409ee5f-2973-47ee-a1bf-112731843c5d",
      "metadata": {},
      "source": [
        "最后，我们将应用程序编译成一个单独的 `graph` 对象。在这种情况下，我们只是将各个步骤按顺序连接起来。我们还允许第一个 `query_or_respond` 步骤在未生成工具调用时直接响应用户，从而实现‘短路’功能。这使得我们的应用程序能够支持对话式体验——例如，回应那些可能不需要检索步骤的通用问候语"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "60ed5dd0-8636-4a12-961b-b8355b579862",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StateGraph } from \"@langchain/langgraph\";\n",
        "import { toolsCondition } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "\n",
        "const graphBuilder = new StateGraph(MessagesAnnotation)\n",
        "  .addNode(\"queryOrRespond\", queryOrRespond)\n",
        "  .addNode(\"tools\", tools)\n",
        "  .addNode(\"generate\", generate)\n",
        "  .addEdge(\"__start__\", \"queryOrRespond\")\n",
        "  .addConditionalEdges(\n",
        "    \"queryOrRespond\",\n",
        "    toolsCondition,\n",
        "    {__end__: \"__end__\", tools: \"tools\"}\n",
        "  )\n",
        "  .addEdge(\"tools\", \"generate\")\n",
        "  .addEdge(\"generate\", \"__end__\")\n",
        "\n",
        "const graph = graphBuilder.compile();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0389e0-c454-4693-b6e2-979326015574",
      "metadata": {},
      "source": [
        "```javascript\n",
        "// 注意：tslab 只能在 Jupyter Notebook 内部工作。请勿担心自己运行此代码！\n",
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const image = await graph.getGraph().drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));\n",
        "```\n",
        "\n",
        "![graph_img_rag_part_2](../../static/img/graph_img_rag_part_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aee3fb6-c7a2-44a5-96f0-2951631caaf2",
      "metadata": {},
      "source": [
        "让我们测试我们的应用程序。\n",
        "\n",
        "```{=mdx}\n",
        "<details>\n",
        "<summary>点击展开 `prettyPrint` 代码。</summary>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cb29afc4-899b-4f11-89bf-092368d21bec",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const prettyPrint = (message: BaseMessage) => {\n",
        "  let txt = `[${message._getType()}]: ${message.content}`;\n",
        "  if (\n",
        "    (isAIMessage(message) && message.tool_calls?.length) ||\n",
        "    0 > 0\n",
        "  ) {\n",
        "    const tool_calls = (message as AIMessage)?.tool_calls\n",
        "      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)\n",
        "      .join(\"\\n\");\n",
        "    txt += ` \\nTools: \\n${tool_calls}`;\n",
        "  }\n",
        "  console.log(txt);\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a82d163c-80a7-4551-88f7-25643f8d98b9",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "</details>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c08d53-3f18-467b-83d0-07fb0b4d41be",
      "metadata": {},
      "source": [
        "请注意，它会适当地响应不需要额外检索步骤的消息："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "16304ac2-50de-4646-b31b-01ea523d1d5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human]: Hello\n",
            "-----\n",
            "\n",
            "[ai]: Hello! How can I assist you today?\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let inputs1 = { messages: [{ role: \"user\", content: \"Hello\" }] };\n",
        "\n",
        "for await (\n",
        "  const step of await graph.stream(inputs1, {\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "    const lastMessage = step.messages[step.messages.length - 1];\n",
        "    prettyPrint(lastMessage);\n",
        "    console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df5046d-4610-4ffa-9f30-d04453da05a9",
      "metadata": {},
      "source": [
        "在执行搜索时，我们可以流式传输各个步骤以观察查询生成、检索和答案生成过程："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4989fb9b-4867-482f-99b7-380a18c54504",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human]: What is Task Decomposition?\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- retrieve({\"query\":\"Task Decomposition\"})\n",
            "-----\n",
            "\n",
            "[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain\n",
            "Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: System message:Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
            "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.Then you will output the content of each file including ALL code.\n",
            "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
            "FILENAME is the lowercase file name including the file extension,\n",
            "LANG is the markup code block language for the code’s language, and CODE is the code:FILENAMEYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
            "Please note that the code should be fully functional. No placeholders.Follow a language and framework appropriate best practice file naming convention.\n",
            "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
            "-----\n",
            "\n",
            "[ai]: Task decomposition is the process of breaking down a complex task into smaller, more manageable steps or subgoals. This can be achieved through various methods, such as using prompts for large language models (LLMs), task-specific instructions, or human inputs. It helps in simplifying the problem-solving process and enhances understanding of the task at hand.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let inputs2 = { messages: [{ role: \"user\", content: \"What is Task Decomposition?\" }] };\n",
        "\n",
        "for await (\n",
        "  const step of await graph.stream(inputs2, {\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "    const lastMessage = step.messages[step.messages.length - 1];\n",
        "    prettyPrint(lastMessage);\n",
        "    console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a26a9c4d-0e5b-4db9-8b78-68624d829ac2",
      "metadata": {},
      "source": [
        "点击[此处](https://smith.langchain.com/public/c6ed4e16-b9ed-46cc-912e-6a580d3c47ed/r)查看LangSmith的追踪信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2300c04-019c-4c65-a104-3dbf17c924b7",
      "metadata": {},
      "source": [
        "### 对话历史的状态管理\n",
        "\n",
        "```{=mdx}\n",
        ":::note\n",
        "\n",
        "本教程的这一部分之前使用了 [RunnableWithMessageHistory](https://api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html) 抽象。您可以在 [v0.2 文档](https://js.langchain.com/v0.2/docs/tutorials/qa_chat_history) 中访问该版本的文档。\n",
        "\n",
        "从 LangChain 的 v0.3 版本开始，我们建议 LangChain 用户使用 [LangGraph 持久化](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) 来在新的 LangChain 应用中集成 `memory` 功能。\n",
        "\n",
        "如果您的代码已经依赖 `RunnableWithMessageHistory` 或 `BaseChatMessageHistory`，则**无需**进行任何更改。我们近期不计划弃用此功能，因为它适用于简单的聊天应用，并且任何使用 `RunnableWithMessageHistory` 的代码将继续按预期运行。\n",
        "\n",
        "请参阅 [如何迁移到 LangGraph Memory](/docs/versions/migrating_memory/) 了解详细信息。\n",
        ":::\n",
        "```\n",
        "\n",
        "在生产环境中，问答应用通常会将聊天历史持久化到数据库中，并能够适当地读取和更新它。\n",
        "\n",
        "[LangGraph](https://langchain-ai.github.io/langgraphjs/) 实现了一个内置的 [持久化层](https://langchain-ai.github.io/langgraphjs/concepts/persistence/)，使其非常适合支持多轮对话的聊天应用。\n",
        "\n",
        "为了管理多个对话轮次和线程，我们要做的所有事情就是在编译应用时指定一个 [checkpointer](https://langchain-ai.github.io/langgraphjs/concepts/persistence/)。由于我们图中的节点正在向状态追加消息，因此我们可以在多次调用之间保持一致的聊天历史。\n",
        "\n",
        "LangGraph 提供了一个简单的内存 checkpointer，我们在下面使用它。请参见其 [文档](https://langchain-ai.github.io/langgraphjs/concepts/persistence/) 获取更多详细信息，包括如何使用不同的持久化后端（例如 SQLite 或 Postgres）。\n",
        "\n",
        "如需详细了解如何管理消息历史，请前往 [如何添加消息历史（memory）](/docs/how_to/message_history) 指南。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "471c416b-f9b9-4ada-8e59-b4b47abd5d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { MemorySaver } from \"@langchain/langgraph\";\n",
        "\n",
        "const checkpointer = new MemorySaver();\n",
        "const graphWithMemory = graphBuilder.compile({ checkpointer });\n",
        "\n",
        "// Specify an ID for the thread\n",
        "const threadConfig = {\n",
        "    configurable: { thread_id: \"abc123\" },\n",
        "    streamMode: \"values\" as const\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f557b169-b33c-42d0-b97e-1b948d0a2914",
      "metadata": {},
      "source": [
        "我们现在可以像之前一样调用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8f6daf90-6069-44e9-a656-603fea0829f5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human]: What is Task Decomposition?\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- retrieve({\"query\":\"Task Decomposition\"})\n",
            "-----\n",
            "\n",
            "[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain\n",
            "Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: System message:Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
            "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.Then you will output the content of each file including ALL code.\n",
            "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
            "FILENAME is the lowercase file name including the file extension,\n",
            "LANG is the markup code block language for the code’s language, and CODE is the code:FILENAMEYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
            "Please note that the code should be fully functional. No placeholders.Follow a language and framework appropriate best practice file naming convention.\n",
            "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
            "-----\n",
            "\n",
            "[ai]: Task decomposition is the process of breaking down a complex task into smaller, more manageable steps or subgoals. This can be achieved through various methods, such as using prompts for large language models (LLMs), task-specific instructions, or human inputs. It helps in simplifying the problem-solving process and enhances understanding of the task at hand.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let inputs3 = { messages: [{ role: \"user\", content: \"What is Task Decomposition?\" }] };\n",
        "\n",
        "for await (\n",
        "  const step of await graphWithMemory.stream(inputs3, threadConfig)\n",
        ") {\n",
        "    const lastMessage = step.messages[step.messages.length - 1];\n",
        "    prettyPrint(lastMessage);\n",
        "    console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a87d33af-a954-49e7-bccf-f6dab3ce2411",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human]: Can you look up some common ways of doing it?\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- retrieve({\"query\":\"common methods of task decomposition\"})\n",
            "-----\n",
            "\n",
            "[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain\n",
            "Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: be provided by other developers (as in Plugins) or self-defined (as in function calls).HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.The system comprises of 4 stages:(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.Instruction:(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.Instruction:(3) Task execution: Expert models execute on the specific tasks and log results.Instruction:(4) Response generation:\n",
            "-----\n",
            "\n",
            "[ai]: Common ways of task decomposition include using large language models (LLMs) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", employing task-specific instructions (e.g., \"Write a story outline\"), and incorporating human inputs. Additionally, methods like the Tree of Thoughts approach explore multiple reasoning possibilities at each step, creating a structured tree of thoughts. These techniques facilitate breaking down tasks into manageable components for better execution.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let inputs4 = { messages: [{ role: \"user\", content: \"Can you look up some common ways of doing it?\" }] };\n",
        "\n",
        "for await (\n",
        "  const step of await graphWithMemory.stream(inputs4, threadConfig)\n",
        ") {\n",
        "    const lastMessage = step.messages[step.messages.length - 1];\n",
        "    prettyPrint(lastMessage);\n",
        "    console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bbbeef2-d9a1-4857-874f-9f3b5cc4eca9",
      "metadata": {},
      "source": [
        "请注意，模型在第二个问题中生成的查询包含了对话上下文。\n",
        "\n",
        "此处的 [LangSmith](https://smith.langchain.com/public/c8b2c1ba-8c8b-47ab-b298-3502e0688711/r) 追踪尤其具有参考价值，因为我们可以清楚地看到在每一步中我们的聊天模型可见的消息内容。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad23c71-3c99-4d9d-b494-9b7a08a557c0",
      "metadata": {},
      "source": [
        "## 代理\n",
        "\n",
        "[代理](/docs/concepts/agents) 利用LLM的推理能力在执行过程中进行决策。使用代理可以将更多检索过程的判断权下放。尽管其行为比上述“链”更难以预测，但它们能够执行多个检索步骤以服务于一个查询，或者对单次搜索进行迭代。\n",
        "\n",
        "下面我们构建了一个最简化的RAG代理。使用LangGraph的[预构建ReAct代理构造器](https://langchain-ai.github.io/langgraph/how-tos/#langgraph.prebuilt.chat_agent_executor.create_react_agent)，我们可以在一行代码中完成此操作。\n",
        "\n",
        "```{=mdx}\n",
        ":::提示\n",
        "\n",
        "查看LangGraph的 [Agentic RAG](https://langchain-ai.github.io/langgraphjs/tutorials/rag/langgraph_agentic_rag/) 教程以了解更多高级用法。\n",
        "\n",
        ":::\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6d519b4c-29e3-4e14-a5f8-178bd25a1728",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const agent = createReactAgent({ llm: llm, tools: [retrieve] });"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8f8734-5dcf-4058-a532-11c8a7d0efae",
      "metadata": {},
      "source": [
        "让我们检查一下这个图表："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00cbb651-db1c-405d-905d-6e200c7169a7",
      "metadata": {},
      "source": [
        "```javascript\n",
        "// 注意：tslab 只能在 Jupyter Notebook 内部运行。请勿担心自行运行此代码！\n",
        "import * as tslab from \"tslab\";\n",
        "\n",
        "const image = await agent.getGraph().drawMermaidPng();\n",
        "const arrayBuffer = await image.arrayBuffer();\n",
        "\n",
        "await tslab.display.png(new Uint8Array(arrayBuffer));\n",
        "```\n",
        "\n",
        "![graph_img_react](../../static/img/graph_img_react.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28623a52-7906-440f-8aaf-d6bb5ecbad98",
      "metadata": {},
      "source": [
        "与我们之前实现的主要区别在于，这里不是以一个最终生成步骤结束运行，而是将工具调用循环回到原始的LLM调用。这样，模型可以使用检索到的上下文来回答问题，或者生成另一个工具调用来获取更多信息。\n",
        "\n",
        "让我们来测试一下这个方法。我们构建了一个通常需要通过迭代检索步骤来解答的问题："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "85cd4d73-fb7b-4447-b20a-b9d24969ffc2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[human]: What is the standard method for Task Decomposition?\n",
            "Once you get the answer, look up common extensions of that method.\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- retrieve({\"query\":\"standard method for Task Decomposition\"})\n",
            "-----\n",
            "\n",
            "[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain\n",
            "Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: System message:Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
            "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.Then you will output the content of each file including ALL code.\n",
            "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
            "FILENAME is the lowercase file name including the file extension,\n",
            "LANG is the markup code block language for the code’s language, and CODE is the code:FILENAMEYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
            "Please note that the code should be fully functional. No placeholders.Follow a language and framework appropriate best practice file naming convention.\n",
            "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
            "-----\n",
            "\n",
            "[ai]:  \n",
            "Tools: \n",
            "- retrieve({\"query\":\"common extensions of Task Decomposition method\"})\n",
            "-----\n",
            "\n",
            "[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain\n",
            "Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Content: be provided by other developers (as in Plugins) or self-defined (as in function calls).HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.The system comprises of 4 stages:(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.Instruction:(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.Instruction:(3) Task execution: Expert models execute on the specific tasks and log results.Instruction:(4) Response generation:\n",
            "-----\n",
            "\n",
            "[ai]: ### Standard Method for Task Decomposition\n",
            "\n",
            "The standard method for task decomposition involves breaking down hard tasks into smaller, more manageable steps. This can be achieved through various approaches:\n",
            "\n",
            "1. **Chain of Thought (CoT)**: This method transforms large tasks into multiple manageable tasks, providing insight into the model's reasoning process.\n",
            "2. **Prompting**: Using simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to guide the decomposition.\n",
            "3. **Task-Specific Instructions**: Providing specific instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\n",
            "4. **Human Inputs**: Involving human input to assist in the decomposition process.\n",
            "\n",
            "### Common Extensions of Task Decomposition\n",
            "\n",
            "Several extensions have been developed to enhance the task decomposition process:\n",
            "\n",
            "1. **Tree of Thoughts (ToT)**: This method extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into multiple thought steps and generates various thoughts per step, creating a tree structure. The search process can utilize either breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.\n",
            "\n",
            "2. **LLM+P**: This approach involves using an external classical planner for long-horizon planning, integrating planning domains to enhance the decomposition process.\n",
            "\n",
            "3. **HuggingGPT**: This framework utilizes ChatGPT as a task planner to select models from the HuggingFace platform based on model descriptions. It consists of four stages:\n",
            "   - **Task Planning**: Parsing user requests into multiple tasks with attributes like task type, ID, dependencies, and arguments.\n",
            "   - **Model Selection**: Distributing tasks to expert models based on a multiple-choice question format.\n",
            "   - **Task Execution**: Expert models execute specific tasks and log results.\n",
            "   - **Response Generation**: Compiling the results into a coherent response.\n",
            "\n",
            "These extensions aim to improve the efficiency and effectiveness of task decomposition, making it easier to manage complex tasks.\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "let inputMessage = `What is the standard method for Task Decomposition?\n",
        "Once you get the answer, look up common extensions of that method.`\n",
        "\n",
        "let inputs5 = { messages: [{ role: \"user\", content: inputMessage }] };\n",
        "\n",
        "for await (\n",
        "  const step of await agent.stream(inputs5, {\n",
        "    streamMode: \"values\",\n",
        "  })\n",
        ") {\n",
        "    const lastMessage = step.messages[step.messages.length - 1];\n",
        "    prettyPrint(lastMessage);\n",
        "    console.log(\"-----\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ab58d2-92ef-4940-a535-7c8808e75523",
      "metadata": {},
      "source": [
        "请注意，该智能体：\n",
        "\n",
        "1. 生成一个查询以搜索任务分解的标准方法；\n",
        "2. 收到答案后，生成第二个查询以搜索其常见扩展；\n",
        "3. 在收到所有必要的上下文后，回答问题。\n",
        "\n",
        "我们可以在[LangSmith追踪](https://smith.langchain.com/public/67b7642b-78d0-482a-bb49-fe08674bf972/r)中看到完整的步骤序列，以及延迟和其他元数据。\n",
        "\n",
        "## 后续步骤\n",
        "\n",
        "我们已经介绍了构建基本对话式问答应用程序的步骤：\n",
        "\n",
        "- 我们使用链（chains）构建了一个可预测的应用程序，每个用户输入最多生成一个查询；\n",
        "- 我们使用智能体（agents）构建了一个可以在一系列查询上进行迭代的应用程序。\n",
        "\n",
        "要探索不同类型的检索器和检索策略，请访问如何指南中的[检索器](/docs/how_to/#retrievers)部分。\n",
        "\n",
        "有关LangChain对话内存抽象的详细演练，请访问[如何添加消息历史记录（内存）](/docs/how_to/message_history)指南。\n",
        "\n",
        "要了解更多关于智能体的信息，请查看[概念指南](/docs/concepts/agents)和LangGraph的[智能体架构](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/)页面。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}