{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 如何流式传输\n",
        "\n",
        ":::info 预备知识\n",
        "\n",
        "本指南假定您熟悉以下概念：\n",
        "\n",
        "- [聊天模型](/docs/concepts/chat_models)\n",
        "- [LangChain 表达语言](/docs/concepts/lcel)\n",
        "- [输出解析器](/docs/concepts/output_parsers)\n",
        "\n",
        ":::\n",
        "\n",
        "流式传输对于使基于大型语言模型（LLM）的应用程序对最终用户具有响应性至关重要。\n",
        "\n",
        "像 LLM、解析器、提示词、检索器和代理这样的重要 LangChain 原语实现了 LangChain 的 Runnable 接口。\n",
        "\n",
        "该接口提供了两种通用的流式传输方法：\n",
        "\n",
        "- `.stream()`：流式传输的默认实现，用于从链中流式传输最终输出。\n",
        "- `streamEvents()` 和 `streamLog()`：这些方法可以同时流式传输链中的中间步骤和最终输出。\n",
        "\n",
        "让我们来看看这两种方法！\n",
        "\n",
        ":::info\n",
        "有关 LangChain 中流式传输技术的更高层次概述，请参阅[概念指南的这一部分](/docs/concepts/streaming)。\n",
        ":::\n",
        "\n",
        "# 使用 Stream\n",
        "\n",
        "所有 `Runnable` 对象都实现了一个名为 stream 的方法。\n",
        "\n",
        "这些方法旨在以分块的形式流式传输最终输出，一旦有可用的分块就立即生成该分块。\n",
        "\n",
        "只有当程序中的所有步骤都知道如何处理**输入流**时，才能实现流式传输；也就是说，逐个处理输入分块，并生成相应的输出分块。\n",
        "\n",
        "这种处理的复杂性可能各不相同，从像输出由 LLM 生成的 token 这样简单的任务，到像在完整 JSON 完成之前流式传输 JSON 部分这样更具挑战性的任务。\n",
        "\n",
        "探索流式传输的最佳起点是 LLM 应用中最重要的组件之一 —— 模型本身！\n",
        "\n",
        "## LLM 和聊天模型\n",
        "\n",
        "大型语言模型可能需要几秒钟才能生成对查询的完整响应。这远慢于应用程序对最终用户具有响应性的**~200-300 毫秒**阈值。\n",
        "\n",
        "让应用程序感觉更具响应性的关键策略是显示中间进度；例如，逐个 token 地流式传输模型的输出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import \"dotenv/config\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs />\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const model = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "  temperature: 0,\n",
        "});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|\n",
            "Hello|\n",
            "!|\n",
            " I'm|\n",
            " a|\n",
            " large|\n",
            " language|\n",
            " model|\n",
            " developed|\n",
            " by|\n",
            " Open|\n",
            "AI|\n",
            " called|\n",
            " GPT|\n",
            "-|\n",
            "4|\n",
            ",|\n",
            " based|\n",
            " on|\n",
            " the|\n",
            " Gener|\n",
            "ative|\n",
            " Pre|\n",
            "-trained|\n",
            " Transformer|\n",
            " architecture|\n",
            ".|\n",
            " I'm|\n",
            " designed|\n",
            " to|\n",
            " understand|\n",
            " and|\n",
            " generate|\n",
            " human|\n",
            "-like|\n",
            " text|\n",
            " based|\n",
            " on|\n",
            " the|\n",
            " input|\n",
            " I|\n",
            " receive|\n",
            ".|\n",
            " My|\n",
            " primary|\n",
            " function|\n",
            " is|\n",
            " to|\n",
            " assist|\n",
            " with|\n",
            " answering|\n",
            " questions|\n",
            ",|\n",
            " providing|\n",
            " information|\n",
            ",|\n",
            " and|\n",
            " engaging|\n",
            " in|\n",
            " various|\n",
            " types|\n",
            " of|\n",
            " conversations|\n",
            ".|\n",
            " While|\n",
            " I|\n",
            " don't|\n",
            " have|\n",
            " personal|\n",
            " experiences|\n",
            " or|\n",
            " emotions|\n",
            ",|\n",
            " I'm|\n",
            " trained|\n",
            " on|\n",
            " diverse|\n",
            " datasets|\n",
            " that|\n",
            " enable|\n",
            " me|\n",
            " to|\n",
            " provide|\n",
            " useful|\n",
            " and|\n",
            " relevant|\n",
            " information|\n",
            " across|\n",
            " a|\n",
            " wide|\n",
            " array|\n",
            " of|\n",
            " topics|\n",
            ".|\n",
            " How|\n",
            " can|\n",
            " I|\n",
            " assist|\n",
            " you|\n",
            " today|\n",
            "?|\n",
            "|\n",
            "|\n"
          ]
        }
      ],
      "source": [
        "const stream = await model.stream(\"Hello! Tell me about yourself.\");\n",
        "const chunks = [];\n",
        "for await (const chunk of stream) {\n",
        "  chunks.push(chunk);\n",
        "  console.log(`${chunk.content}|`)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "让我们看一下其中一个原始数据块："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessageChunk {\n",
            "  lc_serializable: true,\n",
            "  lc_kwargs: {\n",
            "    content: '',\n",
            "    tool_call_chunks: [],\n",
            "    additional_kwargs: {},\n",
            "    id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: [],\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  lc_namespace: [ 'langchain_core', 'messages' ],\n",
            "  content: '',\n",
            "  name: undefined,\n",
            "  additional_kwargs: {},\n",
            "  response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n",
            "  id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',\n",
            "  tool_calls: [],\n",
            "  invalid_tool_calls: [],\n",
            "  tool_call_chunks: [],\n",
            "  usage_metadata: undefined\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们得到了一个名为 `AIMessageChunk` 的对象。这个 chunk 代表了 `AIMessage` 的一部分。\n",
        "\n",
        "消息块在设计上是可累加的——可以简单地使用 `.concat()` 方法将它们相加，以获得到目前为止的响应状态！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessageChunk {\n",
            "  lc_serializable: true,\n",
            "  lc_kwargs: {\n",
            "    content: \"Hello! I'm a\",\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n",
            "    tool_call_chunks: [],\n",
            "    id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: []\n",
            "  },\n",
            "  lc_namespace: [ 'langchain_core', 'messages' ],\n",
            "  content: \"Hello! I'm a\",\n",
            "  name: undefined,\n",
            "  additional_kwargs: {},\n",
            "  response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n",
            "  id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',\n",
            "  tool_calls: [],\n",
            "  invalid_tool_calls: [],\n",
            "  tool_call_chunks: [],\n",
            "  usage_metadata: undefined\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "let finalChunk = chunks[0];\n",
        "\n",
        "for (const chunk of chunks.slice(1, 5)) {\n",
        "  finalChunk = finalChunk.concat(chunk);\n",
        "}\n",
        "\n",
        "finalChunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 链（Chains）\n",
        "\n",
        "实际上，几乎所有LLM应用程序都包含多个步骤，而不仅仅是调用语言模型。\n",
        "\n",
        "让我们使用`LangChain表达式语言`（`LCEL`）构建一个简单的链，将提示（prompt）、模型和解析器（parser）组合在一起，并验证流式传输是否正常工作。\n",
        "\n",
        "我们将使用`StringOutputParser`来解析模型的输出。这是一个简单的解析器，可以从`AIMessageChunk`中提取内容字段，从而获取模型返回的`token`。\n",
        "\n",
        ":::{.callout-tip}\n",
        "LCEL是一种声明式方法，通过将不同的LangChain基本组件串联起来，以指定一个“程序”。使用LCEL创建的链可以自动实现流（stream）功能，从而允许流式传输最终输出。事实上，使用LCEL创建的链实现了完整的标准Runnable接口。\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|\n",
            "Sure|\n",
            ",|\n",
            " here's|\n",
            " a|\n",
            " joke|\n",
            " for|\n",
            " you|\n",
            ":\n",
            "\n",
            "|\n",
            "Why|\n",
            " did|\n",
            " the|\n",
            " par|\n",
            "rot|\n",
            " sit|\n",
            " on|\n",
            " the|\n",
            " stick|\n",
            "?\n",
            "\n",
            "|\n",
            "Because|\n",
            " it|\n",
            " wanted|\n",
            " to|\n",
            " be|\n",
            " a|\n",
            " \"|\n",
            "pol|\n",
            "ly|\n",
            "-stick|\n",
            "-al|\n",
            "\"|\n",
            " observer|\n",
            "!|\n",
            "|\n",
            "|\n"
          ]
        }
      ],
      "source": [
        "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "const prompt = ChatPromptTemplate.fromTemplate(\"Tell me a joke about {topic}\");\n",
        "\n",
        "const parser = new StringOutputParser();\n",
        "\n",
        "const chain = prompt.pipe(model).pipe(parser);\n",
        "\n",
        "const stream = await chain.stream({\n",
        "  topic: \"parrot\",\n",
        "});\n",
        "\n",
        "for await (const chunk of stream) {\n",
        "  console.log(`${chunk}|`)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-note}\n",
        "您不必使用 `LangChain 表达语言` 来使用 LangChain，而是可以通过以标准的 **命令式** 编程方式，\n",
        "单独调用每个组件的 `invoke`、`batch` 或 `stream` 方法，将结果赋值给变量，然后根据需要在后续流程中使用这些变量。\n",
        "\n",
        "如果这种方式能满足您的需求，那对我们来说完全没问题 👌！\n",
        ":::\n",
        "\n",
        "### 处理输入流\n",
        "\n",
        "如果您想在生成 JSON 数据的同时对其进行流式处理，该怎么办？\n",
        "\n",
        "如果您依赖 `JSON.parse` 来解析部分 JSON 数据，解析会失败，因为部分 JSON 并不是有效的完整 JSON。\n",
        "\n",
        "此时，您可能会完全不知道该如何处理，并认为 JSON 流式传输是不可能实现的。\n",
        "\n",
        "其实，有一种方法可以做到这一点——解析器需要在 **输入流** 上进行操作，并尝试将部分 JSON \"自动补全\" 成为一个有效的状态。\n",
        "\n",
        "让我们看看这样的解析器是如何工作的，以便理解其含义。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  countries: [\n",
            "    { name: 'France', population: 67390000 },\n",
            "    { name: 'Spain', population: 47350000 },\n",
            "    { name: 'Japan', population: 125800000 }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { JsonOutputParser } from \"@langchain/core/output_parsers\"\n",
        "\n",
        "const chain = model.pipe(new JsonOutputParser());\n",
        "const stream = await chain.stream(\n",
        "  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`\n",
        ");\n",
        "\n",
        "for await (const chunk of stream) {\n",
        "  console.log(chunk);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在，让我们来**打破**流式传输。我们将使用前面的例子，并在末尾追加一个提取函数，从最终的 JSON 中提取国家名称。由于这个新的最后一步只是一个没有定义流式行为的函数调用，因此前面步骤的流式输出会被聚合，然后作为单个输入传递给该函数。\n",
        "\n",
        ":::{.callout-warning}\n",
        "链中任何对**最终输入**而不是对**输入流**进行操作的步骤，都可能通过 `stream` 打破流式传输功能。\n",
        ":::\n",
        "\n",
        ":::{.callout-tip}\n",
        "稍后，我们将讨论 `streamEvents` API，它可以流式传输中间步骤的结果。即使链中包含仅对**最终输入**进行操作的步骤，该 API 仍能流式传输中间步骤的结果。\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"France\",\"Spain\",\"Japan\"]\n"
          ]
        }
      ],
      "source": [
        "// A function that operates on finalized inputs\n",
        "// rather than on an input_stream\n",
        "\n",
        "// A function that does not operates on input streams and breaks streaming.\n",
        "const extractCountryNames = (inputs: Record<string, any>) => {\n",
        "  if (!Array.isArray(inputs.countries)) {\n",
        "    return \"\";\n",
        "  }\n",
        "  return JSON.stringify(inputs.countries.map((country) => country.name));\n",
        "}\n",
        "\n",
        "const chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);\n",
        "\n",
        "const stream = await chain.stream(\n",
        "  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`\n",
        ");\n",
        "\n",
        "for await (const chunk of stream) {\n",
        "  console.log(chunk);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 非流式组件\n",
        "\n",
        "与上述示例类似，一些内置组件（如检索器）不提供任何流式传输功能。如果我们尝试对它们进行 `stream` 操作会发生什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  [\n",
            "    Document {\n",
            "      pageContent: 'mitochondria is the powerhouse of the cell',\n",
            "      metadata: {},\n",
            "      id: undefined\n",
            "    },\n",
            "    Document {\n",
            "      pageContent: 'buildings are made of brick',\n",
            "      metadata: {},\n",
            "      id: undefined\n",
            "    }\n",
            "  ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
        "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "const template = `Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "`;\n",
        "const prompt = ChatPromptTemplate.fromTemplate(template);\n",
        "\n",
        "const vectorstore = await MemoryVectorStore.fromTexts(\n",
        "  [\"mitochondria is the powerhouse of the cell\", \"buildings are made of brick\"],\n",
        "  [{}, {}],\n",
        "  new OpenAIEmbeddings(),\n",
        ");\n",
        "\n",
        "const retriever = vectorstore.asRetriever();\n",
        "\n",
        "const chunks = [];\n",
        "\n",
        "for await (const chunk of await retriever.stream(\"What is the powerhouse of the cell?\")) {\n",
        "  chunks.push(chunk);\n",
        "}\n",
        "\n",
        "console.log(chunks);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "流刚刚从该组件生成最终结果。\n",
        "\n",
        "这没问题！并非所有组件都必须实现流式传输——在某些情况下，流式传输可能是不必要的、困难的，或者根本没有意义。\n",
        "\n",
        ":::{.callout-tip}\n",
        "由某些非流式组件构建的LCEL链在很多情况下仍然能够进行流式传输，链中最后一个非流式步骤之后将开始部分输出的流式传输。\n",
        ":::\n",
        "\n",
        "以下是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|\n",
            "M|\n",
            "ito|\n",
            "ch|\n",
            "ond|\n",
            "ria|\n",
            " is|\n",
            " the|\n",
            " powerhouse|\n",
            " of|\n",
            " the|\n",
            " cell|\n",
            ".|\n",
            "|\n",
            "|\n"
          ]
        }
      ],
      "source": [
        "import { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\n",
        "import type { Document } from \"@langchain/core/documents\";\n",
        "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
        "\n",
        "const formatDocs = (docs: Document[]) => {\n",
        "  return docs.map((doc) => doc.pageContent).join(\"\\n-----\\n\")\n",
        "}\n",
        "\n",
        "const retrievalChain = RunnableSequence.from([\n",
        "  {\n",
        "    context: retriever.pipe(formatDocs),\n",
        "    question: new RunnablePassthrough()\n",
        "  },\n",
        "  prompt,\n",
        "  model,\n",
        "  new StringOutputParser(),\n",
        "]);\n",
        "\n",
        "const stream = await retrievalChain.stream(\"What is the powerhouse of the cell?\");\n",
        "\n",
        "for await (const chunk of stream) {\n",
        "  console.log(`${chunk}|`);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在我们已经了解了 `stream` 方法的工作原理，让我们进入事件流的世界吧！\n",
        "\n",
        "## 使用流事件\n",
        "\n",
        "事件流是一个**测试版** API。该 API 可能会根据反馈进行一些调整。\n",
        "\n",
        ":::{.callout-note}\n",
        "在 @langchain/core **0.1.27** 中引入。\n",
        ":::\n",
        "\n",
        "为了让 `streamEvents` 方法正常工作：\n",
        "\n",
        "* 任何自定义函数 / 可运行对象都必须传递回调\n",
        "* 在模型上设置适当的参数以强制 LLM 流式传输 token。\n",
        "* 如果有任何不符合预期的情况，请告诉我们！\n",
        "\n",
        "### 事件参考\n",
        "\n",
        "以下是一个参考表，展示了一些可运行对象可能发出的事件。\n",
        "\n",
        ":::{.callout-note}\n",
        "当正确实现流式传输时，在输入流完全被消费之前，可运行对象的输入通常是未知的。这意味着 `inputs` 通常只会包含在 `end` 事件中，而不是 `start` 事件中。\n",
        ":::\n",
        "\n",
        "| 事件                | 名称             | 数据块                           | 输入                                         | 输出                                          |\n",
        "|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|\n",
        "| on_llm_start         | [模型名称]       |                                 | {'input': 'hello'}                            |                                                 |\n",
        "| on_llm_stream        | [模型名称]       | 'Hello' `或` AIMessageChunk(content=\"hello\")  |                                               |                                   |\n",
        "| on_llm_end           | [模型名称]       |                                 | 'Hello human!'                                | {\"生成内容\": [...], \"模型输出\": None, ...}  |\n",
        "| on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
        "| on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
        "| on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
        "| on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
        "| on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
        "| on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
        "| on_retriever_start   | [检索器名称]     |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
        "| on_retriever_chunk   | [检索器名称]     | {文档: [...]}                    |                                               |                                                 |\n",
        "| on_retriever_end     | [检索器名称]     |                                 | {\"query\": \"hello\"}                            | {文档: [...]}                              |\n",
        "| on_prompt_start      | [模板名称]       |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
        "| on_prompt_end        | [模板名称]       |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
        "\n",
        "`streamEvents` 在 `v2` 中还将发出分发的自定义事件。更多信息请参阅[此指南](/docs/how_to/callbacks_custom_events/)。\n",
        "\n",
        "### 聊天模型\n",
        "\n",
        "让我们首先看一下聊天模型产生的事件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n"
          ]
        }
      ],
      "source": [
        "const events = [];\n",
        "\n",
        "const eventStream = await model.streamEvents(\"hello\", { version: \"v2\" });\n",
        "\n",
        "for await (const event of eventStream) {\n",
        "  events.push(event);\n",
        "}\n",
        "\n",
        "console.log(events.length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-note}\n",
        "\n",
        "嘿，API 中那个奇怪的 version=\"v2\" 参数是什么？！ 😾\n",
        "\n",
        "这是一个**测试版 API**，我们几乎肯定会对其进行一些更改。\n",
        "\n",
        "这个版本参数将允许我们尽量减少对您代码的破坏性更改。\n",
        "\n",
        "简而言之，我们现在打扰一下您，是为了以后不再打扰您。\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "让我们看一下部分开始事件和部分结束事件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    event: 'on_chat_model_start',\n",
            "    data: { input: 'hello' },\n",
            "    name: 'ChatOpenAI',\n",
            "    tags: [],\n",
            "    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',\n",
            "    metadata: {\n",
            "      ls_provider: 'openai',\n",
            "      ls_model_name: 'gpt-4o',\n",
            "      ls_model_type: 'chat',\n",
            "      ls_temperature: 1,\n",
            "      ls_max_tokens: undefined,\n",
            "      ls_stop: undefined\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    event: 'on_chat_model_stream',\n",
            "    data: { chunk: [AIMessageChunk] },\n",
            "    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',\n",
            "    name: 'ChatOpenAI',\n",
            "    tags: [],\n",
            "    metadata: {\n",
            "      ls_provider: 'openai',\n",
            "      ls_model_name: 'gpt-4o',\n",
            "      ls_model_type: 'chat',\n",
            "      ls_temperature: 1,\n",
            "      ls_max_tokens: undefined,\n",
            "      ls_stop: undefined\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    event: 'on_chat_model_stream',\n",
            "    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',\n",
            "    name: 'ChatOpenAI',\n",
            "    tags: [],\n",
            "    metadata: {\n",
            "      ls_provider: 'openai',\n",
            "      ls_model_name: 'gpt-4o',\n",
            "      ls_model_type: 'chat',\n",
            "      ls_temperature: 1,\n",
            "      ls_max_tokens: undefined,\n",
            "      ls_stop: undefined\n",
            "    },\n",
            "    data: { chunk: [AIMessageChunk] }\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "events.slice(0, 3);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    event: 'on_chat_model_stream',\n",
            "    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',\n",
            "    name: 'ChatOpenAI',\n",
            "    tags: [],\n",
            "    metadata: {\n",
            "      ls_provider: 'openai',\n",
            "      ls_model_name: 'gpt-4o',\n",
            "      ls_model_type: 'chat',\n",
            "      ls_temperature: 1,\n",
            "      ls_max_tokens: undefined,\n",
            "      ls_stop: undefined\n",
            "    },\n",
            "    data: { chunk: [AIMessageChunk] }\n",
            "  },\n",
            "  {\n",
            "    event: 'on_chat_model_end',\n",
            "    data: { output: [AIMessageChunk] },\n",
            "    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',\n",
            "    name: 'ChatOpenAI',\n",
            "    tags: [],\n",
            "    metadata: {\n",
            "      ls_provider: 'openai',\n",
            "      ls_model_name: 'gpt-4o',\n",
            "      ls_model_type: 'chat',\n",
            "      ls_temperature: 1,\n",
            "      ls_max_tokens: undefined,\n",
            "      ls_stop: undefined\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "events.slice(-2);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 链式结构\n",
        "\n",
        "让我们重新审视解析流式 JSON 的示例链，以探索流式事件 API。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83\n"
          ]
        }
      ],
      "source": [
        "const chain = model.pipe(new JsonOutputParser());\n",
        "const eventStream = await chain.streamEvents(\n",
        "  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n",
        "  { version: \"v2\" },\n",
        ");\n",
        "\n",
        "\n",
        "const events = [];\n",
        "for await (const event of eventStream) {\n",
        "  events.push(event);\n",
        "}\n",
        "\n",
        "console.log(events.length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如果您查看前几个事件，您会注意到有**3**个不同的开始事件，而不是**2**个开始事件。\n",
        "\n",
        "这三个开始事件分别对应于：\n",
        "\n",
        "1. 链（模型 + 解析器）\n",
        "2. 模型\n",
        "3. 解析器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    event: 'on_chain_start',\n",
            "    data: {\n",
            "      input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"'\n",
            "    },\n",
            "    name: 'RunnableSequence',\n",
            "    tags: [],\n",
            "    run_id: '5dd960b8-4341-4401-8993-7d04d49fcc08',\n",
            "    metadata: {}\n",
            "  },\n",
            "  {\n",
            "    event: 'on_chat_model_start',\n",
            "    data: { input: [Object] },\n",
            "    name: 'ChatOpenAI',\n",
            "    tags: [ 'seq:step:1' ],\n",
            "    run_id: '5d2917b1-886a-47a1-807d-8a0ba4cb4f65',\n",
            "    metadata: {\n",
            "      ls_provider: 'openai',\n",
            "      ls_model_name: 'gpt-4o',\n",
            "      ls_model_type: 'chat',\n",
            "      ls_temperature: 1,\n",
            "      ls_max_tokens: undefined,\n",
            "      ls_stop: undefined\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    event: 'on_parser_start',\n",
            "    data: {},\n",
            "    name: 'JsonOutputParser',\n",
            "    tags: [ 'seq:step:2' ],\n",
            "    run_id: '756c57d6-d455-484f-a556-79a82c4e1d40',\n",
            "    metadata: {}\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "events.slice(0, 3);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如果你查看最后3个事件，你觉得会看到什么？中间的事件呢？\n",
        "\n",
        "让我们使用这个API来输出模型和解析器的流事件。我们忽略开始事件、结束事件以及来自链的事件。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat model chunk: \n",
            "Chat model chunk: ```\n",
            "Chat model chunk: json\n",
            "Chat model chunk: \n",
            "\n",
            "Chat model chunk: {\n",
            "\n",
            "Chat model chunk:    \n",
            "Chat model chunk:  \"\n",
            "Chat model chunk: countries\n",
            "Chat model chunk: \":\n",
            "Chat model chunk:  [\n",
            "\n",
            "Chat model chunk:        \n",
            "Chat model chunk:  {\n",
            "\n",
            "Chat model chunk:            \n",
            "Chat model chunk:  \"\n",
            "Chat model chunk: name\n",
            "Chat model chunk: \":\n",
            "Chat model chunk:  \"\n",
            "Chat model chunk: France\n",
            "Chat model chunk: \",\n",
            "\n",
            "Chat model chunk:            \n",
            "Chat model chunk:  \"\n",
            "Chat model chunk: population\n",
            "Chat model chunk: \":\n",
            "Chat model chunk:  \n",
            "Chat model chunk: 652\n",
            "Chat model chunk: 735\n",
            "Chat model chunk: 11\n",
            "Chat model chunk: \n",
            "\n"
          ]
        }
      ],
      "source": [
        "let eventCount = 0;\n",
        "\n",
        "const eventStream = await chain.streamEvents(\n",
        "  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n",
        "  { version: \"v1\" },\n",
        ");\n",
        "\n",
        "for await (const event of eventStream) {\n",
        "  // Truncate the output\n",
        "  if (eventCount > 30) {\n",
        "    continue;\n",
        "  }\n",
        "  const eventType = event.event;\n",
        "  if (eventType === \"on_llm_stream\") {\n",
        "    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);\n",
        "  } else if (eventType === \"on_parser_stream\") {\n",
        "    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);\n",
        "  }\n",
        "  eventCount += 1;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "由于模型和解析器都支持流式传输，我们可以实时看到来自这两个组件的流式事件！很整洁！🦜\n",
        "\n",
        "### 过滤事件\n",
        "\n",
        "由于此API会产生大量事件，因此能够按事件进行过滤是非常有用的。\n",
        "\n",
        "你可以通过组件的`名称`、组件的`标签`或组件的`类型`进行过滤。\n",
        "\n",
        "#### 按名称\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  event: 'on_parser_start',\n",
            "  data: {\n",
            "    input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"'\n",
            "  },\n",
            "  name: 'my_parser',\n",
            "  tags: [ 'seq:step:2' ],\n",
            "  run_id: '0a605976-a8f8-4259-8ef6-b3d7e52b3d4e',\n",
            "  metadata: {}\n",
            "}\n",
            "{\n",
            "  event: 'on_parser_stream',\n",
            "  run_id: '0a605976-a8f8-4259-8ef6-b3d7e52b3d4e',\n",
            "  name: 'my_parser',\n",
            "  tags: [ 'seq:step:2' ],\n",
            "  metadata: {},\n",
            "  data: { chunk: { countries: [Array] } }\n",
            "}\n",
            "{\n",
            "  event: 'on_parser_end',\n",
            "  data: { output: { countries: [Array] } },\n",
            "  run_id: '0a605976-a8f8-4259-8ef6-b3d7e52b3d4e',\n",
            "  name: 'my_parser',\n",
            "  tags: [ 'seq:step:2' ],\n",
            "  metadata: {}\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const chain = model.withConfig({ runName: \"model\" })\n",
        "  .pipe(\n",
        "    new JsonOutputParser().withConfig({ runName: \"my_parser\" })\n",
        "  );\n",
        "\n",
        "\n",
        "const eventStream = await chain.streamEvents(\n",
        "  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n",
        "  { version: \"v2\" },\n",
        "  { includeNames: [\"my_parser\"] },\n",
        ");\n",
        "\n",
        "let eventCount = 0;\n",
        "\n",
        "for await (const event of eventStream) {\n",
        "  // Truncate the output\n",
        "  if (eventCount > 10) {\n",
        "    continue;\n",
        "  }\n",
        "  console.log(event);\n",
        "  eventCount += 1;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 按类型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  event: 'on_chat_model_start',\n",
            "  data: {\n",
            "    input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"'\n",
            "  },\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: '',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: '```',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: 'json',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: '\\n',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: '{\\n',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: ' ',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: ' \"',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: 'countries',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: '\":',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: ' [\\n',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',\n",
            "  name: 'model',\n",
            "  tags: [ 'seq:step:1' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const chain = model.withConfig({ runName: \"model\" })\n",
        "  .pipe(\n",
        "    new JsonOutputParser().withConfig({ runName: \"my_parser\" })\n",
        "  );\n",
        "\n",
        "\n",
        "const eventStream = await chain.streamEvents(\n",
        "  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n",
        "  { version: \"v2\" },\n",
        "  { includeTypes: [\"chat_model\"] },\n",
        ");\n",
        "\n",
        "let eventCount = 0;\n",
        "\n",
        "for await (const event of eventStream) {\n",
        "  // Truncate the output\n",
        "  if (eventCount > 10) {\n",
        "    continue;\n",
        "  }\n",
        "  console.log(event);\n",
        "  eventCount += 1;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 按标签\n",
        "\n",
        ":::{.callout-caution}\n",
        "\n",
        "标签会被给定可运行组件的子组件继承。\n",
        "\n",
        "如果您使用标签进行过滤，请确保这是您想要的效果。\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  event: 'on_chain_start',\n",
            "  data: {\n",
            "    input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"'\n",
            "  },\n",
            "  name: 'RunnableSequence',\n",
            "  tags: [ 'my_chain' ],\n",
            "  run_id: '1fed60d6-e0b7-4d5e-8ec7-cd7d3ee5c69f',\n",
            "  metadata: {}\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_start',\n",
            "  data: { input: { messages: [Array] } },\n",
            "  name: 'ChatOpenAI',\n",
            "  tags: [ 'seq:step:1', 'my_chain' ],\n",
            "  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_parser_start',\n",
            "  data: {},\n",
            "  name: 'my_parser',\n",
            "  tags: [ 'seq:step:2', 'my_chain' ],\n",
            "  run_id: 'caf24a1e-255c-4937-9f38-6e46275d854a',\n",
            "  metadata: {}\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: '',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',\n",
            "  name: 'ChatOpenAI',\n",
            "  tags: [ 'seq:step:1', 'my_chain' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: 'Certainly',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',\n",
            "  name: 'ChatOpenAI',\n",
            "  tags: [ 'seq:step:1', 'my_chain' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: '!',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',\n",
            "  name: 'ChatOpenAI',\n",
            "  tags: [ 'seq:step:1', 'my_chain' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: \" Here's\",\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',\n",
            "  name: 'ChatOpenAI',\n",
            "  tags: [ 'seq:step:1', 'my_chain' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: ' the',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',\n",
            "  name: 'ChatOpenAI',\n",
            "  tags: [ 'seq:step:1', 'my_chain' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: ' JSON',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',\n",
            "  name: 'ChatOpenAI',\n",
            "  tags: [ 'seq:step:1', 'my_chain' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: ' format',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',\n",
            "  name: 'ChatOpenAI',\n",
            "  tags: [ 'seq:step:1', 'my_chain' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n",
            "{\n",
            "  event: 'on_chat_model_stream',\n",
            "  data: {\n",
            "    chunk: AIMessageChunk {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: [Object],\n",
            "      lc_namespace: [Array],\n",
            "      content: ' output',\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: [Object],\n",
            "      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      tool_call_chunks: [],\n",
            "      usage_metadata: undefined\n",
            "    }\n",
            "  },\n",
            "  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',\n",
            "  name: 'ChatOpenAI',\n",
            "  tags: [ 'seq:step:1', 'my_chain' ],\n",
            "  metadata: {\n",
            "    ls_provider: 'openai',\n",
            "    ls_model_name: 'gpt-4o',\n",
            "    ls_model_type: 'chat',\n",
            "    ls_temperature: 1,\n",
            "    ls_max_tokens: undefined,\n",
            "    ls_stop: undefined\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const chain = model\n",
        "  .pipe(new JsonOutputParser().withConfig({ runName: \"my_parser\" }))\n",
        "  .withConfig({ tags: [\"my_chain\"] });\n",
        "\n",
        "\n",
        "const eventStream = await chain.streamEvents(\n",
        "  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n",
        "  { version: \"v2\" },\n",
        "  { includeTags: [\"my_chain\"] },\n",
        ");\n",
        "\n",
        "let eventCount = 0;\n",
        "\n",
        "for await (const event of eventStream) {\n",
        "  // Truncate the output\n",
        "  if (eventCount > 10) {\n",
        "    continue;\n",
        "  }\n",
        "  console.log(event);\n",
        "  eventCount += 1;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 通过HTTP流式传输事件\n",
        "\n",
        "为了方便起见，`streamEvents` 支持将流式中间事件编码为HTTP [服务器发送事件](https://developer.mozilla.org/zh-CN/docs/Web/API/Server-sent_events)，以字节形式进行编码。以下是其使用方式（使用 [`TextDecoder`](https://developer.mozilla.org/zh-CN/docs/Web/API/TextDecoder) 将二进制数据重新转换为可读字符串）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "event: data\n",
            "data: {\"event\":\"on_chain_start\",\"data\":{\"input\":\"Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \\\"countries\\\" which contains a list of countries. Each country should have the key \\\"name\\\" and \\\"population\\\"\"},\"name\":\"RunnableSequence\",\"tags\":[\"my_chain\"],\"run_id\":\"41cd92f8-9b8c-4365-8aa0-fda3abdae03d\",\"metadata\":{}}\n",
            "\n",
            "\n",
            "event: data\n",
            "data: {\"event\":\"on_chat_model_start\",\"data\":{\"input\":{\"messages\":[[{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain_core\",\"messages\",\"HumanMessage\"],\"kwargs\":{\"content\":\"Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \\\"countries\\\" which contains a list of countries. Each country should have the key \\\"name\\\" and \\\"population\\\"\",\"additional_kwargs\":{},\"response_metadata\":{}}}]]}},\"name\":\"ChatOpenAI\",\"tags\":[\"seq:step:1\",\"my_chain\"],\"run_id\":\"a6c2bc61-c868-4570-a143-164e64529ee0\",\"metadata\":{\"ls_provider\":\"openai\",\"ls_model_name\":\"gpt-4o\",\"ls_model_type\":\"chat\",\"ls_temperature\":1}}\n",
            "\n",
            "\n",
            "event: data\n",
            "data: {\"event\":\"on_parser_start\",\"data\":{},\"name\":\"my_parser\",\"tags\":[\"seq:step:2\",\"my_chain\"],\"run_id\":\"402533c5-0e4e-425d-a556-c30a350972d0\",\"metadata\":{}}\n",
            "\n",
            "\n",
            "event: data\n",
            "data: {\"event\":\"on_chat_model_stream\",\"data\":{\"chunk\":{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain_core\",\"messages\",\"AIMessageChunk\"],\"kwargs\":{\"content\":\"\",\"tool_call_chunks\":[],\"additional_kwargs\":{},\"id\":\"chatcmpl-9lO9BAQwbKDy2Ou2RNFUVi0VunAsL\",\"tool_calls\":[],\"invalid_tool_calls\":[],\"response_metadata\":{\"prompt\":0,\"completion\":0,\"finish_reason\":null}}}},\"run_id\":\"a6c2bc61-c868-4570-a143-164e64529ee0\",\"name\":\"ChatOpenAI\",\"tags\":[\"seq:step:1\",\"my_chain\"],\"metadata\":{\"ls_provider\":\"openai\",\"ls_model_name\":\"gpt-4o\",\"ls_model_type\":\"chat\",\"ls_temperature\":1}}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "const chain = model\n",
        "  .pipe(new JsonOutputParser().withConfig({ runName: \"my_parser\" }))\n",
        "  .withConfig({ tags: [\"my_chain\"] });\n",
        "\n",
        "\n",
        "const eventStream = await chain.streamEvents(\n",
        "  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n",
        "  {\n",
        "    version: \"v2\",\n",
        "    encoding: \"text/event-stream\",\n",
        "  },\n",
        ");\n",
        "\n",
        "let eventCount = 0;\n",
        "\n",
        "const textDecoder = new TextDecoder();\n",
        "\n",
        "for await (const event of eventStream) {\n",
        "  // Truncate the output\n",
        "  if (eventCount > 3) {\n",
        "    continue;\n",
        "  }\n",
        "  console.log(textDecoder.decode(event));\n",
        "  eventCount += 1;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这种格式的一个 nice 特性是，你可以将生成的流直接传递给带有正确头部的原生[HTTP响应对象](https://developer.mozilla.org/zh-CN/docs/Web/API/Response)（常见于像[Hono](https://hono.dev/)和[Next.js](https://nextjs.org/)这样的框架中使用），然后在前端解析该流。你的服务端处理程序看起来会像这样："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "const handler = async () => {\n",
        "  const eventStream = await chain.streamEvents(\n",
        "    `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n",
        "    {\n",
        "      version: \"v2\",\n",
        "      encoding: \"text/event-stream\",\n",
        "    },\n",
        "  );\n",
        "  return new Response(eventStream, {\n",
        "    headers: {\n",
        "      \"content-type\": \"text/event-stream\",\n",
        "    }\n",
        "  });\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "而你的前端可能看起来像这样（使用 [`@microsoft/fetch-event-source`](https://www.npmjs.com/package/@microsoft/fetch-event-source) 包来获取并解析事件源）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { fetchEventSource } from \"@microsoft/fetch-event-source\";\n",
        "\n",
        "const makeChainRequest = async () => {\n",
        "  await fetchEventSource(\"https://your_url_here\", {\n",
        "    method: \"POST\",\n",
        "    body: JSON.stringify({\n",
        "      foo: 'bar'\n",
        "    }),\n",
        "    onmessage: (message) => {\n",
        "      if (message.event === \"data\") {\n",
        "        console.log(message.data);\n",
        "      }\n",
        "    },\n",
        "    onerror: (err) => {\n",
        "      console.log(err);\n",
        "    }\n",
        "  });\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 非流式组件\n",
        "\n",
        "还记得一些组件因为不操作**输入流**而无法很好地进行流式传输吗？\n",
        "\n",
        "虽然这些组件在使用 `stream` 时可能会中断最终输出的流式传输，但 `streamEvents` 仍将从支持流式传输的中间步骤中产生流式事件！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"France\",\"Spain\",\"Japan\"]\n"
          ]
        }
      ],
      "source": [
        "// A function that operates on finalized inputs\n",
        "// rather than on an input_stream\n",
        "import { JsonOutputParser } from \"@langchain/core/output_parsers\"\n",
        "import { RunnablePassthrough } from \"@langchain/core/runnables\";\n",
        "\n",
        "// A function that does not operates on input streams and breaks streaming.\n",
        "const extractCountryNames = (inputs: Record<string, any>) => {\n",
        "  if (!Array.isArray(inputs.countries)) {\n",
        "    return \"\";\n",
        "  }\n",
        "  return JSON.stringify(inputs.countries.map((country) => country.name));\n",
        "}\n",
        "\n",
        "const chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);\n",
        "\n",
        "const stream = await chain.stream(\n",
        "  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`\n",
        ");\n",
        "\n",
        "for await (const chunk of stream) {\n",
        "  console.log(chunk);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "正如预期的那样，`stream` API 无法正常工作，因为 `extractCountryNames` 不支持在流上操作。\n",
        "\n",
        "现在，我们确认一下使用 `streamEvents` 时，是否仍然能看到模型和解析器的流式输出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "const eventStream = await chain.streamEvents(\n",
        "  `output a list of the countries france, spain and japan and their populations in JSON format.\n",
        "Use a dict with an outer key of \"countries\" which contains a list of countries.\n",
        "Each country should have the key \"name\" and \"population\"\n",
        "Your output should ONLY contain valid JSON data. Do not include any other text or content in your output.`,\n",
        "  { version: \"v2\" },\n",
        ");\n",
        "\n",
        "let eventCount = 0;\n",
        "\n",
        "for await (const event of eventStream) {\n",
        "  // Truncate the output\n",
        "  if (eventCount > 30) {\n",
        "    continue;\n",
        "  }\n",
        "  const eventType = event.event;\n",
        "  if (eventType === \"on_chat_model_stream\") {\n",
        "    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);\n",
        "  } else if (eventType === \"on_parser_stream\") {\n",
        "    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);\n",
        "  } else {\n",
        "    console.log(eventType)\n",
        "  }\n",
        "  eventCount += 1;\n",
        "}"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Chat model chunk:\n",
        "Chat model chunk: Here's\n",
        "Chat model chunk:  how\n",
        "Chat model chunk:  you\n",
        "Chat model chunk:  can\n",
        "Chat model chunk:  represent\n",
        "Chat model chunk:  the\n",
        "Chat model chunk:  countries\n",
        "Chat model chunk:  France\n",
        "Chat model chunk: ,\n",
        "Chat model chunk:  Spain\n",
        "Chat model chunk: ,\n",
        "Chat model chunk:  and\n",
        "Chat model chunk:  Japan\n",
        "Chat model chunk: ,\n",
        "Chat model chunk:  along\n",
        "Chat model chunk:  with\n",
        "Chat model chunk:  their\n",
        "Chat model chunk:  populations\n",
        "Chat model chunk: ,\n",
        "Chat model chunk:  in\n",
        "Chat model chunk:  JSON\n",
        "Chat model chunk:  format\n",
        "Chat model chunk: :\n",
        "\n",
        "\n",
        "Chat model chunk: ```\n",
        "Chat model chunk: json\n",
        "Chat model chunk:\n",
        "\n",
        "Chat model chunk: {"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 相关\n",
        "\n",
        "- [派发自定义事件](/docs/how_to/callbacks_custom_events)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}