{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "keywords: [chain, chaining, runnablesequence]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 如何链接可运行对象\n",
        "\n",
        "关于[LangChain 表达式语言](/docs/concepts/lcel)的一点是，任何两个可运行对象都可以被“链接”在一起形成一个序列。前一个可运行对象的 `.invoke()` 调用的输出将作为输入传递给下一个可运行对象。这可以通过使用 `.pipe()` 方法来完成。\n",
        "\n",
        "生成的 [`RunnableSequence`](https://api.js.langchain.com/classes/langchain_core.runnables.RunnableSequence.html) 本身也是一个可运行对象，这意味着它可以像其他任何可运行对象一样被调用、流式传输或进一步链接。以这种方式链接可运行对象的优势包括高效的流式传输（序列将在输出可用时立即流式传输输出），以及使用诸如 [LangSmith](/docs/how_to/debugging) 之类的工具进行调试和追踪。\n",
        "\n",
        ":::info 前提条件\n",
        "\n",
        "本指南假定您已熟悉以下概念：\n",
        "\n",
        "- [LangChain 表达式语言 (LCEL)](/docs/concepts/lcel)\n",
        "- [提示模板](/docs/concepts/prompt_templates)\n",
        "- [聊天模型](/docs/concepts/chat_models)\n",
        "- [输出解析器](/docs/concepts/output_parsers)\n",
        "\n",
        ":::\n",
        "\n",
        "## pipe 方法\n",
        "\n",
        "为了展示其工作原理，我们来看一个示例。我们将逐步介绍 LangChain 中的一个常见模式：使用 [提示模板](/docs/concepts/prompt_templates) 将输入格式化，然后将其输入到 [聊天模型](/docs/concepts/chat_models)，最后使用 [输出解析器](/docs/concepts/output_parsers) 将聊天消息输出转换为字符串。\n",
        "\n",
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs\n",
        "  customVarName=\"model\"\n",
        "/>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n",
        "import Npm2Yarn from \"@theme/Npm2Yarn\";\n",
        "\n",
        "<IntegrationInstallTooltip></IntegrationInstallTooltip>\n",
        "\n",
        "<Npm2Yarn>\n",
        "  langchain @langchain/core\n",
        "</Npm2Yarn>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "import { ChatOpenAI } from '@langchain/openai';\n",
        "\n",
        "const model = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "  temperature: 0,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\")\n",
        "\n",
        "const chain = prompt.pipe(model).pipe(new StringOutputParser())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "提示词和模型都可以运行，且提示词调用的输出类型与聊天模型的输入类型相同，因此我们可以将它们串联在一起。然后，我们可以像调用其他任何可运行对象一样调用这个序列："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Here's a bear joke for you:\\n\\nWhy did the bear dissolve in water?\\nBecause it was a polar bear!\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await chain.invoke({ topic: \"bears\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 类型转换\n",
        "\n",
        "我们甚至可以将这个链与其他可运行对象结合，创建另一个链。这可能需要使用其他类型的可运行对象进行输入/输出格式转换，具体取决于链组件所需的输入和输出。\n",
        "\n",
        "例如，假设我们想将生成笑话的链与另一个评估生成的笑话是否有趣的链组合起来。\n",
        "\n",
        "在这种情况下，我们需要谨慎处理输入到下一个链中的格式。在下面的示例中，链中的字典会被自动解析并转换为 [`RunnableParallel`](/docs/how_to/parallel)，它会并行运行其所有值，并返回包含结果的字典。\n",
        "\n",
        "这恰好是下一个提示模板所期望的相同格式。以下是其实际运行效果："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Haha, that\\'s a clever play on words! Using \"polar\" to imply the bear dissolved or became polar/polarized when put in water. Not the most hilarious joke ever, but it has a cute, groan-worthy pun that makes it mildly amusing. I appreciate a good pun or wordplay joke.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import { RunnableLambda } from \"@langchain/core/runnables\";\n",
        "\n",
        "const analysisPrompt = ChatPromptTemplate.fromTemplate(\"is this a funny joke? {joke}\")\n",
        "\n",
        "const composedChain = new RunnableLambda({\n",
        "  func: async (input: { topic: string }) => {\n",
        "    const result = await chain.invoke(input);\n",
        "    return { joke: result };\n",
        "  }\n",
        "}).pipe(analysisPrompt).pipe(model).pipe(new StringOutputParser())\n",
        "\n",
        "await composedChain.invoke({ topic: \"bears\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "函数也将被强制转换为可运行对象，因此您也可以向链中添加自定义逻辑。以下链的结果与之前具有相同的逻辑流程："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Haha, that's a cute and punny joke! I like how it plays on the idea of beets blushing or turning red like someone blushing. Food puns can be quite amusing. While not a total knee-slapper, it's a light-hearted, groan-worthy dad joke that would make me chuckle and shake my head. Simple vegetable humor!\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import { RunnableSequence } from \"@langchain/core/runnables\";\n",
        "\n",
        "const composedChainWithLambda = RunnableSequence.from([\n",
        "    chain,\n",
        "    (input) => ({ joke: input }),\n",
        "    analysisPrompt,\n",
        "    model,\n",
        "    new StringOutputParser()\n",
        "])\n",
        "\n",
        "await composedChainWithLambda.invoke({ topic: \"beets\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "参见上面运行的 LangSmith 追踪 [此处](https://smith.langchain.com/public/ef1bf347-a243-4da6-9be6-54f5d73e6da2/r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "但是，请注意，使用此类函数可能会影响流式传输等操作。详见[此部分](/docs/how_to/functions)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 下一步\n",
        "\n",
        "您现在已经了解了一些将两个可运行对象链接在一起的方法。\n",
        "\n",
        "要了解更多信息，请查看本部分中关于可运行对象的其他操作指南：[本节](/docs/how_to/#langchain-expression-language-lcel)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "typescript",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}