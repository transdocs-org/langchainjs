{
  "cells": [
    {
      "cell_type": "raw",
      "id": "beba2e0e",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 2\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0735c0",
      "metadata": {},
      "source": [
        "# 如何在聊天模型中使用少量示例\n",
        "\n",
        "本指南介绍了如何使用示例输入和输出来提示聊天模型。为模型提供几个这样的示例称为少量示例提示（few-shotting），是一种简单但强大的引导生成方法，有时可以显著提升模型性能。\n",
        "\n",
        "目前对于如何最好地进行少量示例提示似乎还没有明确的共识，最佳的提示编译方式可能因模型而异。因此，我们提供了像 [FewShotChatMessagePromptTemplate](https://api.js.langchain.com/classes/langchain_core.prompts.FewShotChatMessagePromptTemplate.html) 这样的少量示例提示模板，作为灵活的起点，您可以根据需要进行修改或替换。\n",
        "\n",
        "少量示例提示模板的目标是根据输入动态选择示例，然后将这些示例格式化为最终提示提供给模型。\n",
        "\n",
        "**注意：** 以下代码示例仅适用于聊天模型，因为 `FewShotChatMessagePromptTemplates` 被设计为输出格式化的 [聊天消息](/docs/concepts/messages)，而不仅仅是纯字符串。有关适用于补全模型（LLMs）的纯字符串模板的类似少量示例提示示例，请参阅 [少量示例提示模板](/docs/how_to/few_shot_examples/) 指南。\n",
        "\n",
        ":::info 预备知识\n",
        "\n",
        "本指南假设您已熟悉以下概念：\n",
        "\n",
        "- [提示模板](/docs/concepts/prompt_templates)\n",
        "- [示例选择器](/docs/concepts/example_selectors)\n",
        "- [聊天模型](/docs/concepts/chat_models)\n",
        "- [向量存储](/docs/concepts/#vectorstores)\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d716f2de-cc29-4823-9360-a808c7bfdb86",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 固定示例\n",
        "\n",
        "最基本的（也是最常见的）少量示例提示技术是使用固定的提示示例。这种方式可以让你选择一个链（chain），对其进行评估，并避免在生产环境中担心其他额外的变动因素。\n",
        "\n",
        "模板的基本组成部分包括：\n",
        "- `examples`：需要包含在最终提示中的示例对象数组。\n",
        "- `examplePrompt`：通过其 [`formatMessages`](https://api.js.langchain.com/classes/langchain_core.prompts.FewShotChatMessagePromptTemplate.html#formatMessages) 方法将每个示例转换为一条或多条消息。常见的示例是将每个示例转换为一条人类消息和一条AI消息回复，或者一条人类消息后跟一条函数调用消息。\n",
        "\n",
        "以下是一个简单的演示。首先，定义你想要包含的示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0fc5a02a-6249-4e92-95c3-30fff9671e8b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import {\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        "} from \"@langchain/core/prompts\"\n",
        "\n",
        "const examples = [\n",
        "    { input: \"2+2\", output: \"4\" },\n",
        "    { input: \"2+3\", output: \"5\" },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8710ecc-2aa0-4172-a74c-250f6bc3d9e2",
      "metadata": {},
      "source": [
        "接下来，将它们组装成少样本提示模板。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "65e72ad1-9060-47d0-91a1-bc130c8b98ac",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  HumanMessage {\n",
            "    lc_serializable: true,\n",
            "    lc_kwargs: { content: \"2+2\", additional_kwargs: {}, response_metadata: {} },\n",
            "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "    content: \"2+2\",\n",
            "    name: undefined,\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  AIMessage {\n",
            "    lc_serializable: true,\n",
            "    lc_kwargs: {\n",
            "      content: \"4\",\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {}\n",
            "    },\n",
            "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "    content: \"4\",\n",
            "    name: undefined,\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {},\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: []\n",
            "  },\n",
            "  HumanMessage {\n",
            "    lc_serializable: true,\n",
            "    lc_kwargs: { content: \"2+3\", additional_kwargs: {}, response_metadata: {} },\n",
            "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "    content: \"2+3\",\n",
            "    name: undefined,\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  AIMessage {\n",
            "    lc_serializable: true,\n",
            "    lc_kwargs: {\n",
            "      content: \"5\",\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {}\n",
            "    },\n",
            "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "    content: \"5\",\n",
            "    name: undefined,\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {},\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: []\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "// This is a prompt template used to format each individual example.\n",
        "const examplePrompt = ChatPromptTemplate.fromMessages(\n",
        "    [\n",
        "        [\"human\", \"{input}\"],\n",
        "        [\"ai\", \"{output}\"],\n",
        "    ]\n",
        ")\n",
        "const fewShotPrompt = new FewShotChatMessagePromptTemplate({\n",
        "    examplePrompt,\n",
        "    examples,\n",
        "    inputVariables: [], // no input variables\n",
        "})\n",
        "\n",
        "const result = await fewShotPrompt.invoke({});\n",
        "console.log(result.toChatMessages())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5490bd59-b28f-46a4-bbdf-0191802dd3c5",
      "metadata": {},
      "source": [
        "最后，我们按如下方式组装最终提示，将 `fewShotPrompt` 直接传递给 `fromMessages` 工厂方法，并将其与模型一起使用："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9f86d6d9-50de-41b6-b6c7-0f9980cc0187",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "const finalPrompt = ChatPromptTemplate.fromMessages(\n",
        "    [\n",
        "        [\"system\", \"You are a wondrous wizard of math.\"],\n",
        "        fewShotPrompt,\n",
        "        [\"human\", \"{input}\"],\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74c6026",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs />\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "97d443b1-6fae-4b36-bede-3ff7306288a3",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage {\n",
              "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
              "  lc_kwargs: {\n",
              "    content: \u001b[32m\"A triangle does not have a square. The square of a number is the result of multiplying the number by\"\u001b[39m... 8 more characters,\n",
              "    tool_calls: [],\n",
              "    invalid_tool_calls: [],\n",
              "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
              "    response_metadata: {}\n",
              "  },\n",
              "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
              "  content: \u001b[32m\"A triangle does not have a square. The square of a number is the result of multiplying the number by\"\u001b[39m... 8 more characters,\n",
              "  name: \u001b[90mundefined\u001b[39m,\n",
              "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
              "  response_metadata: {\n",
              "    tokenUsage: { completionTokens: \u001b[33m23\u001b[39m, promptTokens: \u001b[33m52\u001b[39m, totalTokens: \u001b[33m75\u001b[39m },\n",
              "    finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
              "  },\n",
              "  tool_calls: [],\n",
              "  invalid_tool_calls: []\n",
              "}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "const chain = finalPrompt.pipe(model);\n",
        "\n",
        "await chain.invoke({ input: \"What's the square of a triangle?\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ab7114-f07f-46be-8874-3705a25aba5f",
      "metadata": {},
      "source": [
        "## 动态少样本提示\n",
        "\n",
        "有时你可能希望根据输入内容从整体示例集中仅选择几个示例进行展示。在这种情况下，可以将传递给 `FewShotChatMessagePromptTemplate` 的 `examples` 替换为 `exampleSelector`。其他组件则与上述保持一致！我们动态的少样本提示模板如下所示：\n",
        "\n",
        "- `exampleSelector`：负责根据给定输入选择少样本示例（以及它们的返回顺序）。这些组件实现了 [BaseExampleSelector](https://api.js.langchain.com/classes/langchain_core.example_selectors.BaseExampleSelector.html) 接口。一个常见的例子是基于向量存储的 [SemanticSimilarityExampleSelector](https://api.js.langchain.com/classes/langchain_core.example_selectors.SemanticSimilarityExampleSelector.html)\n",
        "- `examplePrompt`：通过其 [`formatMessages`](https://api.js.langchain.com/classes/langchain_core.prompts.FewShotChatMessagePromptTemplate.html#formatMessages) 方法将每个示例转换为一个或多个消息。一个常见的例子是将每个示例转换为一个人类消息和一个AI消息响应，或者是一个人类消息后跟一个函数调用消息。\n",
        "\n",
        "这些组件同样可以与其他消息和聊天模板组合，以构建最终的提示。\n",
        "\n",
        "让我们通过一个 `SemanticSimilarityExampleSelector` 的示例来演示。由于此实现使用向量存储根据语义相似性选择示例，因此我们首先需要填充该存储。基本思路是我们希望搜索并返回与文本输入最相似的示例，因此我们会对提示示例的 `values` 进行嵌入，而不是考虑其键："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ad66f06a-66fd-4fcc-8166-5d0e3c801e57",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import { SemanticSimilarityExampleSelector } from \"@langchain/core/example_selectors\";\n",
        "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
        "import { OpenAIEmbeddings } from '@langchain/openai';\n",
        "\n",
        "const examples = [\n",
        "  { input: '2+2', output: '4' },\n",
        "  { input: '2+3', output: '5' },\n",
        "  { input: '2+4', output: '6' },\n",
        "  { input: 'What did the cow say to the moon?', output: 'nothing at all' },\n",
        "  {\n",
        "    input: 'Write me a poem about the moon',\n",
        "    output: 'One for the moon, and one for me, who are we to talk about the moon?',\n",
        "  },\n",
        "];\n",
        "\n",
        "const toVectorize = examples.map((example) => `${example.input} ${example.output}`);\n",
        "const embeddings = new OpenAIEmbeddings();\n",
        "const vectorStore = await MemoryVectorStore.fromTexts(toVectorize, examples, embeddings);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f7e384a-2031-432b-951c-7ea8cf9262f1",
      "metadata": {},
      "source": [
        "### 创建 `exampleSelector`\n",
        "\n",
        "有了创建好的向量存储后，我们就可以创建 `exampleSelector`。在这里，我们将单独调用它，并设置其 `k` 参数，以仅获取与输入最接近的两个示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7790303a-f722-452e-8921-b14bdf20bdff",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\n",
              "  {\n",
              "    input: \u001b[32m\"What did the cow say to the moon?\"\u001b[39m,\n",
              "    output: \u001b[32m\"nothing at all\"\u001b[39m\n",
              "  },\n",
              "  { input: \u001b[32m\"2+4\"\u001b[39m, output: \u001b[32m\"6\"\u001b[39m }\n",
              "]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "const exampleSelector = new SemanticSimilarityExampleSelector(\n",
        "    {\n",
        "        vectorStore,\n",
        "        k: 2\n",
        "    }\n",
        ")\n",
        "\n",
        "// The prompt template will load examples by passing the input do the `select_examples` method\n",
        "await exampleSelector.selectExamples({ input: \"horse\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc77c40f-3f58-40a2-b757-a2a2ea43f24a",
      "metadata": {},
      "source": [
        "### 创建提示模板\n",
        "\n",
        "我们现在使用上面创建的 `exampleSelector` 来组装提示模板。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "253c255e-41d7-45f6-9d88-c7a0ced4b1bd",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  HumanMessage {\n",
            "    lc_serializable: true,\n",
            "    lc_kwargs: { content: \"2+3\", additional_kwargs: {}, response_metadata: {} },\n",
            "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "    content: \"2+3\",\n",
            "    name: undefined,\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  AIMessage {\n",
            "    lc_serializable: true,\n",
            "    lc_kwargs: {\n",
            "      content: \"5\",\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {}\n",
            "    },\n",
            "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "    content: \"5\",\n",
            "    name: undefined,\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {},\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: []\n",
            "  },\n",
            "  HumanMessage {\n",
            "    lc_serializable: true,\n",
            "    lc_kwargs: { content: \"2+2\", additional_kwargs: {}, response_metadata: {} },\n",
            "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "    content: \"2+2\",\n",
            "    name: undefined,\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  AIMessage {\n",
            "    lc_serializable: true,\n",
            "    lc_kwargs: {\n",
            "      content: \"4\",\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: [],\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {}\n",
            "    },\n",
            "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "    content: \"4\",\n",
            "    name: undefined,\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {},\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: []\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import {\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        "} from \"@langchain/core/prompts\"\n",
        "\n",
        "// Define the few-shot prompt.\n",
        "const fewShotPrompt = new FewShotChatMessagePromptTemplate({\n",
        "    // The input variables select the values to pass to the example_selector\n",
        "    inputVariables: [\"input\"],\n",
        "    exampleSelector,\n",
        "    // Define how ech example will be formatted.\n",
        "    // In this case, each example will become 2 messages:\n",
        "    // 1 human, and 1 AI\n",
        "    examplePrompt: ChatPromptTemplate.fromMessages(\n",
        "        [[\"human\", \"{input}\"], [\"ai\", \"{output}\"]]\n",
        "    ),\n",
        "})\n",
        "\n",
        "const results = await fewShotPrompt.invoke({ input: \"What's 3+3?\" });\n",
        "const fewShotMessages = results.toChatMessages()\n",
        "console.log(fewShotMessages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339cae7d-0eb0-44a6-852f-0267c5ff72b3",
      "metadata": {},
      "source": [
        "我们可以将这个少量示例的聊天消息提示模板传递给另一个聊天提示模板："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e731cb45-f0ea-422c-be37-42af2a6cb2c4",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatPromptValue {\n",
            "  lc_serializable: true,\n",
            "  lc_kwargs: {\n",
            "    messages: [\n",
            "      HumanMessage {\n",
            "        lc_serializable: true,\n",
            "        lc_kwargs: {\n",
            "          content: \"2+3\",\n",
            "          additional_kwargs: {},\n",
            "          response_metadata: {}\n",
            "        },\n",
            "        lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "        content: \"2+3\",\n",
            "        name: undefined,\n",
            "        additional_kwargs: {},\n",
            "        response_metadata: {}\n",
            "      },\n",
            "      AIMessage {\n",
            "        lc_serializable: true,\n",
            "        lc_kwargs: {\n",
            "          content: \"5\",\n",
            "          tool_calls: [],\n",
            "          invalid_tool_calls: [],\n",
            "          additional_kwargs: {},\n",
            "          response_metadata: {}\n",
            "        },\n",
            "        lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "        content: \"5\",\n",
            "        name: undefined,\n",
            "        additional_kwargs: {},\n",
            "        response_metadata: {},\n",
            "        tool_calls: [],\n",
            "        invalid_tool_calls: []\n",
            "      },\n",
            "      HumanMessage {\n",
            "        lc_serializable: true,\n",
            "        lc_kwargs: {\n",
            "          content: \"2+2\",\n",
            "          additional_kwargs: {},\n",
            "          response_metadata: {}\n",
            "        },\n",
            "        lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "        content: \"2+2\",\n",
            "        name: undefined,\n",
            "        additional_kwargs: {},\n",
            "        response_metadata: {}\n",
            "      },\n",
            "      AIMessage {\n",
            "        lc_serializable: true,\n",
            "        lc_kwargs: {\n",
            "          content: \"4\",\n",
            "          tool_calls: [],\n",
            "          invalid_tool_calls: [],\n",
            "          additional_kwargs: {},\n",
            "          response_metadata: {}\n",
            "        },\n",
            "        lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "        content: \"4\",\n",
            "        name: undefined,\n",
            "        additional_kwargs: {},\n",
            "        response_metadata: {},\n",
            "        tool_calls: [],\n",
            "        invalid_tool_calls: []\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  lc_namespace: [ \"langchain_core\", \"prompt_values\" ],\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: { content: \"2+3\", additional_kwargs: {}, response_metadata: {} },\n",
            "      lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "      content: \"2+3\",\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: {\n",
            "        content: \"5\",\n",
            "        tool_calls: [],\n",
            "        invalid_tool_calls: [],\n",
            "        additional_kwargs: {},\n",
            "        response_metadata: {}\n",
            "      },\n",
            "      lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "      content: \"5\",\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {},\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: []\n",
            "    },\n",
            "    HumanMessage {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: { content: \"2+2\", additional_kwargs: {}, response_metadata: {} },\n",
            "      lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "      content: \"2+2\",\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: {\n",
            "        content: \"4\",\n",
            "        tool_calls: [],\n",
            "        invalid_tool_calls: [],\n",
            "        additional_kwargs: {},\n",
            "        response_metadata: {}\n",
            "      },\n",
            "      lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "      content: \"4\",\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {},\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: []\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const finalPrompt = ChatPromptTemplate.fromMessages(\n",
        "    [\n",
        "        [\"system\", \"You are a wondrous wizard of math.\"],\n",
        "        ...fewShotMessages,\n",
        "        [\"human\", \"{input}\"],\n",
        "    ]\n",
        ")\n",
        "\n",
        "const result = await finalPrompt.invoke({ input: \"What's 3+3?\" });\n",
        "console.log(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2408ea69-1880-4ef5-a0fa-ffa8d2026aa9",
      "metadata": {},
      "source": [
        "### 与聊天模型一起使用\n",
        "\n",
        "最后，你可以将你的模型连接到少量样本提示。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea48da1a",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "<ChatModelTabs\n",
        "  customVarName=\"模型\"\n",
        "/>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0568cbc6-5354-47f1-ab4d-dfcc616cf583",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage {\n",
              "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
              "  lc_kwargs: {\n",
              "    content: \u001b[32m\"6\"\u001b[39m,\n",
              "    tool_calls: [],\n",
              "    invalid_tool_calls: [],\n",
              "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
              "    response_metadata: {}\n",
              "  },\n",
              "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
              "  content: \u001b[32m\"6\"\u001b[39m,\n",
              "  name: \u001b[90mundefined\u001b[39m,\n",
              "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
              "  response_metadata: {\n",
              "    tokenUsage: { completionTokens: \u001b[33m1\u001b[39m, promptTokens: \u001b[33m51\u001b[39m, totalTokens: \u001b[33m52\u001b[39m },\n",
              "    finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
              "  },\n",
              "  tool_calls: [],\n",
              "  invalid_tool_calls: []\n",
              "}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "const chain = finalPrompt.pipe(model);\n",
        "\n",
        "await chain.invoke({ input: \"What's 3+3?\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87fad3c",
      "metadata": {},
      "source": [
        "## 下一步",
        "\n",
        "你现在已经学会了如何在聊天提示中添加少量示例。\n",
        "\n",
        "接下来，请查看本节中有关提示模板的其他操作指南、关于[使用文本生成模型进行少量示例](/docs/how_to/few_shot_examples)的相关操作指南，或[示例选择器操作指南](/docs/how_to/example_selectors/)的其他内容。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nb_converter": "script",
      "pygments_lexer": "typescript",
      "version": "5.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}