{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 如何进行检索\n",
        "\n",
        ":::info 预备知识\n",
        "\n",
        "本指南假定您熟悉以下内容：\n",
        "\n",
        "- [聊天机器人](/docs/tutorials/chatbot)\n",
        "- [检索增强生成](/docs/tutorials/rag)\n",
        "\n",
        ":::\n",
        "\n",
        "检索是一种常见的技术，聊天机器人使用它来利用聊天模型训练数据之外的信息增强其回复。本节将介绍如何在聊天机器人中实现检索，但值得注意的是，检索是一个非常微妙且深奥的主题。\n",
        "\n",
        "## 环境准备\n",
        "\n",
        "您需要安装一些包，并设置所需的LLM API密钥：\n",
        "\n",
        "```{=mdx}\n",
        "import Npm2Yarn from \"@theme/Npm2Yarn\";\n",
        "\n",
        "<Npm2Yarn>\n",
        "  @langchain/openai @langchain/core cheerio\n",
        "</Npm2Yarn>\n",
        "```\n",
        "\n",
        "接下来，我们还需要设置一个聊天模型，用于以下示例。\n",
        "\n",
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs customVarName=\"llm\" />\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llm = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "  temperature: 0,\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 创建一个检索器\n",
        "\n",
        "我们将使用[LangSmith 文档](https://docs.smith.langchain.com)作为源材料，并将内容存储在向量数据库中以供后续检索。请注意，此示例将略过有关解析和存储数据源的一些具体细节——你可以[在此处查看有关创建检索系统的深入文档](/docs/how_to/#qa-with-rag)。\n",
        "\n",
        "让我们使用文档加载器从文档中提取文本："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[33m36687\u001b[39m"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import \"cheerio\";\n",
        "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
        "\n",
        "const loader = new CheerioWebBaseLoader(\n",
        "  \"https://docs.smith.langchain.com/user_guide\"\n",
        ");\n",
        "\n",
        "const rawDocs = await loader.load();\n",
        "\n",
        "rawDocs[0].pageContent.length;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "接下来，我们将其拆分为较小的块，以便LLM的上下文窗口可以处理，并将其存储在向量数据库中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
        "\n",
        "const textSplitter = new RecursiveCharacterTextSplitter({\n",
        "  chunkSize: 500,\n",
        "  chunkOverlap: 0,\n",
        "});\n",
        "\n",
        "const allSplits = await textSplitter.splitDocuments(rawDocs);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "然后我们将这些块嵌入并存储在向量数据库中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
        "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
        "\n",
        "const vectorstore = await MemoryVectorStore.fromDocuments(\n",
        "  allSplits,\n",
        "  new OpenAIEmbeddings()\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "最后，让我们从初始化的向量存储中创建一个检索器："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  Document {\n",
            "    pageContent: \"These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L\"... 294 more characters,\n",
            "    metadata: {\n",
            "      source: \"https://docs.smith.langchain.com/user_guide\",\n",
            "      loc: { lines: { from: 7, to: 7 } }\n",
            "    }\n",
            "  },\n",
            "  Document {\n",
            "    pageContent: \"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi\"... 347 more characters,\n",
            "    metadata: {\n",
            "      source: \"https://docs.smith.langchain.com/user_guide\",\n",
            "      loc: { lines: { from: 6, to: 6 } }\n",
            "    }\n",
            "  },\n",
            "  Document {\n",
            "    pageContent: \"will help in curation of test cases that can help track regressions/improvements and development of \"... 393 more characters,\n",
            "    metadata: {\n",
            "      source: \"https://docs.smith.langchain.com/user_guide\",\n",
            "      loc: { lines: { from: 11, to: 11 } }\n",
            "    }\n",
            "  },\n",
            "  Document {\n",
            "    pageContent: \"that time period — this is especially handy for debugging production issues.LangSmith also allows fo\"... 396 more characters,\n",
            "    metadata: {\n",
            "      source: \"https://docs.smith.langchain.com/user_guide\",\n",
            "      loc: { lines: { from: 11, to: 11 } }\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "const retriever = vectorstore.asRetriever(4);\n",
        "\n",
        "const docs = await retriever.invoke(\"how can langsmith help with testing?\");\n",
        "\n",
        "console.log(docs);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以看到，调用上面的检索器会返回一些LangSmith文档的部分内容，这些内容包含有关测试的信息，我们的聊天机器人可以在回答问题时用作上下文。现在我们有了一个可以从LangSmith文档中返回相关数据的检索器！\n",
        "\n",
        "## 文档链\n",
        "\n",
        "既然我们已经有了一个可以返回LangChain文档的检索器，那么我们现在来创建一个链，使其能够使用这些文档作为上下文来回答问题。我们将使用一个`createStuffDocumentsChain`辅助函数，将所有输入文档“填充”到提示词中。它还将负责将文档格式化为字符串。\n",
        "\n",
        "除了聊天模型外，该函数还需要提供一个包含`context`变量的提示词，以及一个名为`messages`的聊天历史消息占位符。我们将创建一个合适的提示词并按如下方式传递："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\n",
        "import {\n",
        "  ChatPromptTemplate,\n",
        "  MessagesPlaceholder,\n",
        "} from \"@langchain/core/prompts\";\n",
        "\n",
        "const SYSTEM_TEMPLATE = `Answer the user's questions based on the below context. \n",
        "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "`;\n",
        "\n",
        "const questionAnsweringPrompt = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", SYSTEM_TEMPLATE],\n",
        "  new MessagesPlaceholder(\"messages\"),\n",
        "]);\n",
        "\n",
        "const documentChain = await createStuffDocumentsChain({\n",
        "  llm,\n",
        "  prompt: questionAnsweringPrompt,\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以单独调用此 `documentChain` 来回答问题。让我们使用上面检索到的文档和相同的问题 `langsmith 如何帮助测试？`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[32m\"Yes, LangSmith can help test your LLM applications. It allows developers to create datasets, which a\"\u001b[39m... 229 more characters"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "await documentChain.invoke({\n",
        "  messages: [new HumanMessage(\"Can LangSmith help test my LLM applications?\")],\n",
        "  context: docs,\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "看起来不错！作为对比，我们可以尝试不使用上下文文档并比较结果："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[32m\"I don't know.\"\u001b[39m"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await documentChain.invoke({\n",
        "  messages: [new HumanMessage(\"Can LangSmith help test my LLM applications?\")],\n",
        "  context: [],\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以看到，LLM 未返回任何结果。\n",
        "\n",
        "## 检索链\n",
        "\n",
        "让我们将此文档链与检索器结合。以下是一种可能的实现方式："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import type { BaseMessage } from \"@langchain/core/messages\";\n",
        "import {\n",
        "  RunnablePassthrough,\n",
        "  RunnableSequence,\n",
        "} from \"@langchain/core/runnables\";\n",
        "\n",
        "const parseRetrieverInput = (params: { messages: BaseMessage[] }) => {\n",
        "  return params.messages[params.messages.length - 1].content;\n",
        "};\n",
        "\n",
        "const retrievalChain = RunnablePassthrough.assign({\n",
        "  context: RunnableSequence.from([parseRetrieverInput, retriever]),\n",
        "}).assign({\n",
        "  answer: documentChain,\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "给定一个输入消息列表，我们提取列表中最后一条消息的内容，并将其传递给检索器以获取一些文档。然后，我们将这些文档作为上下文传递给我们的文档链以生成最终响应。",
        "\n",
        "调用此链将合并上述两个步骤："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  messages: [\n",
              "    HumanMessage {\n",
              "      lc_serializable: \u001b[33mtrue\u001b[39m,\n",
              "      lc_kwargs: {\n",
              "        content: \u001b[32m\"Can LangSmith help test my LLM applications?\"\u001b[39m,\n",
              "        additional_kwargs: {},\n",
              "        response_metadata: {}\n",
              "      },\n",
              "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
              "      content: \u001b[32m\"Can LangSmith help test my LLM applications?\"\u001b[39m,\n",
              "      name: \u001b[90mundefined\u001b[39m,\n",
              "      additional_kwargs: {},\n",
              "      response_metadata: {}\n",
              "    }\n",
              "  ],\n",
              "  context: [\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L\"\u001b[39m... 294 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each s\"\u001b[39m... 343 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi\"\u001b[39m... 347 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"The ability to rapidly understand how the model is performing — and debug where it is failing — is i\"\u001b[39m... 138 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  answer: \u001b[32m\"Yes, LangSmith can help test your LLM applications. It allows developers to create datasets, which a\"\u001b[39m... 297 more characters\n",
              "}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await retrievalChain.invoke({\n",
        "  messages: [new HumanMessage(\"Can LangSmith help test my LLM applications?\")],\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "看起来不错！\n",
        "\n",
        "## 查询转换\n",
        "\n",
        "我们的检索链能够回答有关 LangSmith 的问题，但存在一个问题——聊天机器人以对话方式与用户交互，因此必须处理后续问题。\n",
        "\n",
        "当前形式的链将难以应对这种情况。例如，对于我们最初的问题的一个后续问题 `告诉我更多！`。如果我们直接使用该查询调用检索器，我们将得到与 LLM 应用测试无关的文档："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\n",
              "  Document {\n",
              "    pageContent: \u001b[32m\"Oftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in\"\u001b[39m... 40 more characters,\n",
              "    metadata: {\n",
              "      source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "      loc: { lines: { from: \u001b[33m8\u001b[39m, to: \u001b[33m8\u001b[39m } }\n",
              "    }\n",
              "  },\n",
              "  Document {\n",
              "    pageContent: \u001b[32m\"This allows you to quickly test out different prompts and models. You can open the playground from a\"\u001b[39m... 37 more characters,\n",
              "    metadata: {\n",
              "      source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "      loc: { lines: { from: \u001b[33m10\u001b[39m, to: \u001b[33m10\u001b[39m } }\n",
              "    }\n",
              "  },\n",
              "  Document {\n",
              "    pageContent: \u001b[32m\"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi\"\u001b[39m... 347 more characters,\n",
              "    metadata: {\n",
              "      source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "      loc: { lines: { from: \u001b[33m6\u001b[39m, to: \u001b[33m6\u001b[39m } }\n",
              "    }\n",
              "  },\n",
              "  Document {\n",
              "    pageContent: \u001b[32m\"together, making it easier to track the performance of and annotate your application across multiple\"\u001b[39m... 244 more characters,\n",
              "    metadata: {\n",
              "      source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "      loc: { lines: { from: \u001b[33m11\u001b[39m, to: \u001b[33m11\u001b[39m } }\n",
              "    }\n",
              "  }\n",
              "]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await retriever.invoke(\"Tell me more!\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这是因为检索器没有内在的状态概念，只会提取与给定查询最相似的文档。为了解决这个问题，我们可以利用LLM将查询转换为一个不包含任何外部引用的独立查询。\n",
        "\n",
        "以下是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage {\n",
              "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
              "  lc_kwargs: {\n",
              "    content: \u001b[32m'\"LangSmith LLM application testing and evaluation features\"'\u001b[39m,\n",
              "    tool_calls: [],\n",
              "    invalid_tool_calls: [],\n",
              "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
              "    response_metadata: {}\n",
              "  },\n",
              "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
              "  content: \u001b[32m'\"LangSmith LLM application testing and evaluation features\"'\u001b[39m,\n",
              "  name: \u001b[90mundefined\u001b[39m,\n",
              "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
              "  response_metadata: {\n",
              "    tokenUsage: { completionTokens: \u001b[33m11\u001b[39m, promptTokens: \u001b[33m144\u001b[39m, totalTokens: \u001b[33m155\u001b[39m },\n",
              "    finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
              "  },\n",
              "  tool_calls: [],\n",
              "  invalid_tool_calls: []\n",
              "}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "const queryTransformPrompt = ChatPromptTemplate.fromMessages([\n",
        "  new MessagesPlaceholder(\"messages\"),\n",
        "  [\n",
        "    \"user\",\n",
        "    \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
        "  ],\n",
        "]);\n",
        "\n",
        "const queryTransformationChain = queryTransformPrompt.pipe(llm);\n",
        "\n",
        "await queryTransformationChain.invoke({\n",
        "  messages: [\n",
        "    new HumanMessage(\"Can LangSmith help test my LLM applications?\"),\n",
        "    new AIMessage(\n",
        "      \"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "    ),\n",
        "    new HumanMessage(\"Tell me more!\"),\n",
        "  ],\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "太棒了！这个转换后的查询将提取与LLM应用测试相关的上下文文档。\n",
        "\n",
        "让我们将其添加到检索链中。我们可以按如下方式封装检索器："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { RunnableBranch } from \"@langchain/core/runnables\";\n",
        "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
        "\n",
        "const queryTransformingRetrieverChain = RunnableBranch.from([\n",
        "  [\n",
        "    (params: { messages: BaseMessage[] }) => params.messages.length === 1,\n",
        "    RunnableSequence.from([parseRetrieverInput, retriever]),\n",
        "  ],\n",
        "  queryTransformPrompt\n",
        "    .pipe(llm)\n",
        "    .pipe(new StringOutputParser())\n",
        "    .pipe(retriever),\n",
        "]).withConfig({ runName: \"chat_retriever_chain\" });"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "然后，我们可以使用此查询转换链来改进检索链，使其更能处理此类后续问题：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "const conversationalRetrievalChain = RunnablePassthrough.assign({\n",
        "  context: queryTransformingRetrieverChain,\n",
        "}).assign({\n",
        "  answer: documentChain,\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "太棒了！让我们用与之前相同的输入来调用这个新链：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  messages: [\n",
              "    HumanMessage {\n",
              "      lc_serializable: \u001b[33mtrue\u001b[39m,\n",
              "      lc_kwargs: {\n",
              "        content: \u001b[32m\"Can LangSmith help test my LLM applications?\"\u001b[39m,\n",
              "        additional_kwargs: {},\n",
              "        response_metadata: {}\n",
              "      },\n",
              "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
              "      content: \u001b[32m\"Can LangSmith help test my LLM applications?\"\u001b[39m,\n",
              "      name: \u001b[90mundefined\u001b[39m,\n",
              "      additional_kwargs: {},\n",
              "      response_metadata: {}\n",
              "    }\n",
              "  ],\n",
              "  context: [\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L\"\u001b[39m... 294 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each s\"\u001b[39m... 343 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi\"\u001b[39m... 347 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"The ability to rapidly understand how the model is performing — and debug where it is failing — is i\"\u001b[39m... 138 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  answer: \u001b[32m\"Yes, LangSmith can help test your LLM applications. It allows developers to create datasets, which a\"\u001b[39m... 297 more characters\n",
              "}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await conversationalRetrievalChain.invoke({\n",
        "  messages: [new HumanMessage(\"Can LangSmith help test my LLM applications?\")],\n",
        "});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  messages: [\n",
              "    HumanMessage {\n",
              "      lc_serializable: \u001b[33mtrue\u001b[39m,\n",
              "      lc_kwargs: {\n",
              "        content: \u001b[32m\"Can LangSmith help test my LLM applications?\"\u001b[39m,\n",
              "        additional_kwargs: {},\n",
              "        response_metadata: {}\n",
              "      },\n",
              "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
              "      content: \u001b[32m\"Can LangSmith help test my LLM applications?\"\u001b[39m,\n",
              "      name: \u001b[90mundefined\u001b[39m,\n",
              "      additional_kwargs: {},\n",
              "      response_metadata: {}\n",
              "    },\n",
              "    AIMessage {\n",
              "      lc_serializable: \u001b[33mtrue\u001b[39m,\n",
              "      lc_kwargs: {\n",
              "        content: \u001b[32m\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp\"\u001b[39m... 317 more characters,\n",
              "        tool_calls: [],\n",
              "        invalid_tool_calls: [],\n",
              "        additional_kwargs: {},\n",
              "        response_metadata: {}\n",
              "      },\n",
              "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
              "      content: \u001b[32m\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp\"\u001b[39m... 317 more characters,\n",
              "      name: \u001b[90mundefined\u001b[39m,\n",
              "      additional_kwargs: {},\n",
              "      response_metadata: {},\n",
              "      tool_calls: [],\n",
              "      invalid_tool_calls: []\n",
              "    },\n",
              "    HumanMessage {\n",
              "      lc_serializable: \u001b[33mtrue\u001b[39m,\n",
              "      lc_kwargs: {\n",
              "        content: \u001b[32m\"Tell me more!\"\u001b[39m,\n",
              "        additional_kwargs: {},\n",
              "        response_metadata: {}\n",
              "      },\n",
              "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
              "      content: \u001b[32m\"Tell me more!\"\u001b[39m,\n",
              "      name: \u001b[90mundefined\u001b[39m,\n",
              "      additional_kwargs: {},\n",
              "      response_metadata: {}\n",
              "    }\n",
              "  ],\n",
              "  context: [\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L\"\u001b[39m... 294 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi\"\u001b[39m... 347 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each s\"\u001b[39m... 343 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"will help in curation of test cases that can help track regressions/improvements and development of \"\u001b[39m... 393 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://docs.smith.langchain.com/user_guide\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  answer: \u001b[32m\"LangSmith supports a variety of workflows to aid in the development of your applications, from creat\"\u001b[39m... 607 more characters\n",
              "}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await conversationalRetrievalChain.invoke({\n",
        "  messages: [\n",
        "    new HumanMessage(\"Can LangSmith help test my LLM applications?\"),\n",
        "    new AIMessage(\n",
        "      \"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "    ),\n",
        "    new HumanMessage(\"Tell me more!\"),\n",
        "  ],\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "你可以查看 [这个 LangSmith 跟踪](https://smith.langchain.com/public/dc4d6bd4-fea5-45df-be94-06ad18882ae9/r)，亲自了解内部查询转换步骤。\n",
        "\n",
        "## 流式传输\n",
        "\n",
        "由于此链是使用 LCEL 构建的，因此你可以使用熟悉的 `.stream()` 方法："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: {\n",
            "        content: \"Can LangSmith help test my LLM applications?\",\n",
            "        additional_kwargs: {},\n",
            "        response_metadata: {}\n",
            "      },\n",
            "      lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "      content: \"Can LangSmith help test my LLM applications?\",\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: {\n",
            "        content: \"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp\"... 317 more characters,\n",
            "        tool_calls: [],\n",
            "        invalid_tool_calls: [],\n",
            "        additional_kwargs: {},\n",
            "        response_metadata: {}\n",
            "      },\n",
            "      lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "      content: \"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp\"... 317 more characters,\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {},\n",
            "      tool_calls: [],\n",
            "      invalid_tool_calls: []\n",
            "    },\n",
            "    HumanMessage {\n",
            "      lc_serializable: true,\n",
            "      lc_kwargs: {\n",
            "        content: \"Tell me more!\",\n",
            "        additional_kwargs: {},\n",
            "        response_metadata: {}\n",
            "      },\n",
            "      lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
            "      content: \"Tell me more!\",\n",
            "      name: undefined,\n",
            "      additional_kwargs: {},\n",
            "      response_metadata: {}\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "{\n",
            "  context: [\n",
            "    Document {\n",
            "      pageContent: \"These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L\"... 294 more characters,\n",
            "      metadata: {\n",
            "        source: \"https://docs.smith.langchain.com/user_guide\",\n",
            "        loc: { lines: [Object] }\n",
            "      }\n",
            "    },\n",
            "    Document {\n",
            "      pageContent: \"We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set​Whi\"... 347 more characters,\n",
            "      metadata: {\n",
            "        source: \"https://docs.smith.langchain.com/user_guide\",\n",
            "        loc: { lines: [Object] }\n",
            "      }\n",
            "    },\n",
            "    Document {\n",
            "      pageContent: \"this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each s\"... 343 more characters,\n",
            "      metadata: {\n",
            "        source: \"https://docs.smith.langchain.com/user_guide\",\n",
            "        loc: { lines: [Object] }\n",
            "      }\n",
            "    },\n",
            "    Document {\n",
            "      pageContent: \"will help in curation of test cases that can help track regressions/improvements and development of \"... 393 more characters,\n",
            "      metadata: {\n",
            "        source: \"https://docs.smith.langchain.com/user_guide\",\n",
            "        loc: { lines: [Object] }\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "{ answer: \"\" }\n",
            "{ answer: \"Lang\" }\n",
            "{ answer: \"Smith\" }\n",
            "{ answer: \" offers\" }\n",
            "{ answer: \" a\" }\n",
            "{ answer: \" comprehensive\" }\n",
            "{ answer: \" suite\" }\n",
            "{ answer: \" of\" }\n",
            "{ answer: \" tools\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" workflows\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" support\" }\n",
            "{ answer: \" the\" }\n",
            "{ answer: \" development\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" testing\" }\n",
            "{ answer: \" of\" }\n",
            "{ answer: \" L\" }\n",
            "{ answer: \"LM\" }\n",
            "{ answer: \" applications\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" Here\" }\n",
            "{ answer: \" are\" }\n",
            "{ answer: \" some\" }\n",
            "{ answer: \" key\" }\n",
            "{ answer: \" features\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" functionalities\" }\n",
            "{ answer: \":\\n\\n\" }\n",
            "{ answer: \"1\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Test\" }\n",
            "{ answer: \" Case\" }\n",
            "{ answer: \" Management\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\\n\" }\n",
            "{ answer: \"  \" }\n",
            "{ answer: \" -\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Bulk\" }\n",
            "{ answer: \" Upload\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" Creation\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\" }\n",
            "{ answer: \" You\" }\n",
            "{ answer: \" can\" }\n",
            "{ answer: \" upload\" }\n",
            "{ answer: \" test\" }\n",
            "{ answer: \" cases\" }\n",
            "{ answer: \" in\" }\n",
            "{ answer: \" bulk\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" create\" }\n",
            "{ answer: \" them\" }\n",
            "{ answer: \" on\" }\n",
            "{ answer: \" the\" }\n",
            "{ answer: \" fly\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" or\" }\n",
            "{ answer: \" export\" }\n",
            "{ answer: \" them\" }\n",
            "{ answer: \" from\" }\n",
            "{ answer: \" application\" }\n",
            "{ answer: \" traces\" }\n",
            "{ answer: \".\\n\" }\n",
            "{ answer: \"  \" }\n",
            "{ answer: \" -\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Datas\" }\n",
            "{ answer: \"ets\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\" }\n",
            "{ answer: \" Lang\" }\n",
            "{ answer: \"Smith\" }\n",
            "{ answer: \" allows\" }\n",
            "{ answer: \" you\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" create\" }\n",
            "{ answer: \" datasets\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" which\" }\n",
            "{ answer: \" are\" }\n",
            "{ answer: \" collections\" }\n",
            "{ answer: \" of\" }\n",
            "{ answer: \" inputs\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" reference\" }\n",
            "{ answer: \" outputs\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" These\" }\n",
            "{ answer: \" datasets\" }\n",
            "{ answer: \" can\" }\n",
            "{ answer: \" be\" }\n",
            "{ answer: \" used\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" run\" }\n",
            "{ answer: \" tests\" }\n",
            "{ answer: \" on\" }\n",
            "{ answer: \" your\" }\n",
            "{ answer: \" L\" }\n",
            "{ answer: \"LM\" }\n",
            "{ answer: \" applications\" }\n",
            "{ answer: \".\\n\\n\" }\n",
            "{ answer: \"2\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Custom\" }\n",
            "{ answer: \" Evalu\" }\n",
            "{ answer: \"ations\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\\n\" }\n",
            "{ answer: \"  \" }\n",
            "{ answer: \" -\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"LL\" }\n",
            "{ answer: \"M\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" He\" }\n",
            "{ answer: \"uristic\" }\n",
            "{ answer: \" Based\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\" }\n",
            "{ answer: \" You\" }\n",
            "{ answer: \" can\" }\n",
            "{ answer: \" run\" }\n",
            "{ answer: \" custom\" }\n",
            "{ answer: \" evaluations\" }\n",
            "{ answer: \" using\" }\n",
            "{ answer: \" both\" }\n",
            "{ answer: \" L\" }\n",
            "{ answer: \"LM\" }\n",
            "{ answer: \"-based\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" heuristic\" }\n",
            "{ answer: \"-based\" }\n",
            "{ answer: \" methods\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" score\" }\n",
            "{ answer: \" test\" }\n",
            "{ answer: \" results\" }\n",
            "{ answer: \".\\n\\n\" }\n",
            "{ answer: \"3\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Comparison\" }\n",
            "{ answer: \" View\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\\n\" }\n",
            "{ answer: \"  \" }\n",
            "{ answer: \" -\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Pro\" }\n",
            "{ answer: \"tot\" }\n",
            "{ answer: \"yp\" }\n",
            "{ answer: \"ing\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" Regression\" }\n",
            "{ answer: \" Tracking\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\" }\n",
            "{ answer: \" When\" }\n",
            "{ answer: \" prot\" }\n",
            "{ answer: \"otyping\" }\n",
            "{ answer: \" different\" }\n",
            "{ answer: \" versions\" }\n",
            "{ answer: \" of\" }\n",
            "{ answer: \" your\" }\n",
            "{ answer: \" applications\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" Lang\" }\n",
            "{ answer: \"Smith\" }\n",
            "{ answer: \" provides\" }\n",
            "{ answer: \" a\" }\n",
            "{ answer: \" comparison\" }\n",
            "{ answer: \" view\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" see\" }\n",
            "{ answer: \" if\" }\n",
            "{ answer: \" there\" }\n",
            "{ answer: \" have\" }\n",
            "{ answer: \" been\" }\n",
            "{ answer: \" any\" }\n",
            "{ answer: \" regress\" }\n",
            "{ answer: \"ions\" }\n",
            "{ answer: \" with\" }\n",
            "{ answer: \" respect\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" your\" }\n",
            "{ answer: \" initial\" }\n",
            "{ answer: \" test\" }\n",
            "{ answer: \" cases\" }\n",
            "{ answer: \".\\n\\n\" }\n",
            "{ answer: \"4\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Native\" }\n",
            "{ answer: \" Rendering\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\\n\" }\n",
            "{ answer: \"  \" }\n",
            "{ answer: \" -\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Chat\" }\n",
            "{ answer: \" Messages\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" Functions\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" Documents\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\" }\n",
            "{ answer: \" Lang\" }\n",
            "{ answer: \"Smith\" }\n",
            "{ answer: \" provides\" }\n",
            "{ answer: \" native\" }\n",
            "{ answer: \" rendering\" }\n",
            "{ answer: \" of\" }\n",
            "{ answer: \" chat\" }\n",
            "{ answer: \" messages\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" functions\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" retrieved\" }\n",
            "{ answer: \" documents\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" making\" }\n",
            "{ answer: \" it\" }\n",
            "{ answer: \" easier\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" visualize\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" understand\" }\n",
            "{ answer: \" the\" }\n",
            "{ answer: \" outputs\" }\n",
            "{ answer: \".\\n\\n\" }\n",
            "{ answer: \"5\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Pro\" }\n",
            "{ answer: \"tot\" }\n",
            "{ answer: \"yp\" }\n",
            "{ answer: \"ing\" }\n",
            "{ answer: \" Support\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\\n\" }\n",
            "{ answer: \"  \" }\n",
            "{ answer: \" -\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Quick\" }\n",
            "{ answer: \" Experiment\" }\n",
            "{ answer: \"ation\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\" }\n",
            "{ answer: \" The\" }\n",
            "{ answer: \" platform\" }\n",
            "{ answer: \" supports\" }\n",
            "{ answer: \" quick\" }\n",
            "{ answer: \" experimentation\" }\n",
            "{ answer: \" with\" }\n",
            "{ answer: \" different\" }\n",
            "{ answer: \" prompts\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" model\" }\n",
            "{ answer: \" types\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" retrieval\" }\n",
            "{ answer: \" strategies\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" other\" }\n",
            "{ answer: \" parameters\" }\n",
            "{ answer: \".\\n\\n\" }\n",
            "{ answer: \"6\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Feedback\" }\n",
            "{ answer: \" Capture\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\\n\" }\n",
            "{ answer: \"  \" }\n",
            "{ answer: \" -\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Human\" }\n",
            "{ answer: \" Feedback\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\" }\n",
            "{ answer: \" When\" }\n",
            "{ answer: \" launching\" }\n",
            "{ answer: \" your\" }\n",
            "{ answer: \" application\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" an\" }\n",
            "{ answer: \" initial\" }\n",
            "{ answer: \" set\" }\n",
            "{ answer: \" of\" }\n",
            "{ answer: \" users\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" Lang\" }\n",
            "{ answer: \"Smith\" }\n",
            "{ answer: \" allows\" }\n",
            "{ answer: \" you\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" gather\" }\n",
            "{ answer: \" human\" }\n",
            "{ answer: \" feedback\" }\n",
            "{ answer: \" on\" }\n",
            "{ answer: \" the\" }\n",
            "{ answer: \" responses\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" This\" }\n",
            "{ answer: \" helps\" }\n",
            "{ answer: \" identify\" }\n",
            "{ answer: \" interesting\" }\n",
            "{ answer: \" runs\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" highlight\" }\n",
            "{ answer: \" edge\" }\n",
            "{ answer: \" cases\" }\n",
            "{ answer: \" causing\" }\n",
            "{ answer: \" problematic\" }\n",
            "{ answer: \" responses\" }\n",
            "{ answer: \".\\n\" }\n",
            "{ answer: \"  \" }\n",
            "{ answer: \" -\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Feedback\" }\n",
            "{ answer: \" Scores\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\" }\n",
            "{ answer: \" You\" }\n",
            "{ answer: \" can\" }\n",
            "{ answer: \" attach\" }\n",
            "{ answer: \" feedback\" }\n",
            "{ answer: \" scores\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" logged\" }\n",
            "{ answer: \" traces\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" often\" }\n",
            "{ answer: \" integrated\" }\n",
            "{ answer: \" into\" }\n",
            "{ answer: \" the\" }\n",
            "{ answer: \" system\" }\n",
            "{ answer: \".\\n\\n\" }\n",
            "{ answer: \"7\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Monitoring\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" Troubles\" }\n",
            "{ answer: \"hooting\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\\n\" }\n",
            "{ answer: \"  \" }\n",
            "{ answer: \" -\" }\n",
            "{ answer: \" **\" }\n",
            "{ answer: \"Logging\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" Visualization\" }\n",
            "{ answer: \"**\" }\n",
            "{ answer: \":\" }\n",
            "{ answer: \" Lang\" }\n",
            "{ answer: \"Smith\" }\n",
            "{ answer: \" logs\" }\n",
            "{ answer: \" all\" }\n",
            "{ answer: \" traces\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" visual\" }\n",
            "{ answer: \"izes\" }\n",
            "{ answer: \" latency\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" token\" }\n",
            "{ answer: \" usage\" }\n",
            "{ answer: \" statistics\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" helps\" }\n",
            "{ answer: \" troubleshoot\" }\n",
            "{ answer: \" specific\" }\n",
            "{ answer: \" issues\" }\n",
            "{ answer: \" as\" }\n",
            "{ answer: \" they\" }\n",
            "{ answer: \" arise\" }\n",
            "{ answer: \".\\n\\n\" }\n",
            "{ answer: \"Overall\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" Lang\" }\n",
            "{ answer: \"Smith\" }\n",
            "{ answer: \" is\" }\n",
            "{ answer: \" designed\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" support\" }\n",
            "{ answer: \" the\" }\n",
            "{ answer: \" entire\" }\n",
            "{ answer: \" lifecycle\" }\n",
            "{ answer: \" of\" }\n",
            "{ answer: \" L\" }\n",
            "{ answer: \"LM\" }\n",
            "{ answer: \" application\" }\n",
            "{ answer: \" development\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" from\" }\n",
            "{ answer: \" initial\" }\n",
            "{ answer: \" prot\" }\n",
            "{ answer: \"otyping\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" deployment\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" ongoing\" }\n",
            "{ answer: \" monitoring\" }\n",
            "{ answer: \",\" }\n",
            "{ answer: \" making\" }\n",
            "{ answer: \" it\" }\n",
            "{ answer: \" a\" }\n",
            "{ answer: \" powerful\" }\n",
            "{ answer: \" tool\" }\n",
            "{ answer: \" for\" }\n",
            "{ answer: \" developers\" }\n",
            "{ answer: \" looking\" }\n",
            "{ answer: \" to\" }\n",
            "{ answer: \" build\" }\n",
            "{ answer: \" and\" }\n",
            "{ answer: \" maintain\" }\n",
            "{ answer: \" high\" }\n",
            "{ answer: \"-quality\" }\n",
            "{ answer: \" L\" }\n",
            "{ answer: \"LM\" }\n",
            "{ answer: \" applications\" }\n",
            "{ answer: \".\" }\n",
            "{ answer: \"\" }\n"
          ]
        }
      ],
      "source": [
        "const stream = await conversationalRetrievalChain.stream({\n",
        "  messages: [\n",
        "    new HumanMessage(\"Can LangSmith help test my LLM applications?\"),\n",
        "    new AIMessage(\n",
        "      \"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "    ),\n",
        "    new HumanMessage(\"Tell me more!\"),\n",
        "  ],\n",
        "});\n",
        "\n",
        "for await (const chunk of stream) {\n",
        "  console.log(chunk);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 下一步",
        "\n",
        "你现在已经学习了一些将个人数据作为上下文添加到聊天机器人中的技术。\n",
        "\n",
        "本指南仅涉及检索技术的表面内容。如需了解更多关于数据摄入、准备和检索最相关数据的不同方法，请查看我们的[检索指南](/docs/how_to/#retrievers)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nb_converter": "script",
      "pygments_lexer": "typescript",
      "version": "5.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}