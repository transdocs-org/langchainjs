{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 4\n",
        "---\n",
        "\n",
        "# 如何创建自定义聊天模型类\n",
        "\n",
        "```{=mdx}\n",
        ":::info 前置条件\n",
        "\n",
        "本指南假定您熟悉以下概念：\n",
        "\n",
        "- [聊天模型](/docs/concepts/chat_models)\n",
        "\n",
        ":::\n",
        "```\n",
        "\n",
        "本笔记本介绍了如何创建自定义聊天模型包装器，以防您希望使用自己的聊天模型或与LangChain直接支持的不同的包装器。\n",
        "\n",
        "在扩展 [`SimpleChatModel` 类](https://api.js.langchain.com/classes/langchain_core.language_models_chat_models.SimpleChatModel.html) 后，聊天模型需要实现以下几项必要内容：\n",
        "\n",
        "- 一个 `_call` 方法，该方法接收消息列表和调用选项（包括 `stop` 序列等内容），并返回一个字符串。\n",
        "\n",
        "- 一个 `_llmType` 方法，该方法返回一个字符串。仅用于日志记录。\n",
        "\n",
        "您还可以实现以下可选方法：\n",
        "\n",
        "- 一个 `_streamResponseChunks` 方法，该方法返回一个 `AsyncGenerator` 并逐个生成 [`ChatGenerationChunks`](https://api.js.langchain.com/classes/langchain_core.outputs.ChatGenerationChunk.html)。这允许LLM支持流式输出。\n",
        "\n",
        "我们来实现一个非常简单的自定义聊天模型，它只是回显输入的前 `n` 个字符。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import {\n",
        "  SimpleChatModel,\n",
        "  type BaseChatModelParams,\n",
        "} from \"@langchain/core/language_models/chat_models\";\n",
        "import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n",
        "import { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\n",
        "import { ChatGenerationChunk } from \"@langchain/core/outputs\";\n",
        "\n",
        "interface CustomChatModelInput extends BaseChatModelParams {\n",
        "  n: number;\n",
        "}\n",
        "\n",
        "class CustomChatModel extends SimpleChatModel {\n",
        "  n: number;\n",
        "\n",
        "  constructor(fields: CustomChatModelInput) {\n",
        "    super(fields);\n",
        "    this.n = fields.n;\n",
        "  }\n",
        "\n",
        "  _llmType() {\n",
        "    return \"custom\";\n",
        "  }\n",
        "\n",
        "  async _call(\n",
        "    messages: BaseMessage[],\n",
        "    options: this[\"ParsedCallOptions\"],\n",
        "    runManager?: CallbackManagerForLLMRun\n",
        "  ): Promise<string> {\n",
        "    if (!messages.length) {\n",
        "      throw new Error(\"No messages provided.\");\n",
        "    }\n",
        "    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n",
        "    // await subRunnable.invoke(params, runManager?.getChild());\n",
        "    if (typeof messages[0].content !== \"string\") {\n",
        "      throw new Error(\"Multimodal messages are not supported.\");\n",
        "    }\n",
        "    return messages[0].content.slice(0, this.n);\n",
        "  }\n",
        "\n",
        "  async *_streamResponseChunks(\n",
        "    messages: BaseMessage[],\n",
        "    options: this[\"ParsedCallOptions\"],\n",
        "    runManager?: CallbackManagerForLLMRun\n",
        "  ): AsyncGenerator<ChatGenerationChunk> {\n",
        "    if (!messages.length) {\n",
        "      throw new Error(\"No messages provided.\");\n",
        "    }\n",
        "    if (typeof messages[0].content !== \"string\") {\n",
        "      throw new Error(\"Multimodal messages are not supported.\");\n",
        "    }\n",
        "    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n",
        "    // await subRunnable.invoke(params, runManager?.getChild());\n",
        "    for (const letter of messages[0].content.slice(0, this.n)) {\n",
        "      yield new ChatGenerationChunk({\n",
        "        message: new AIMessageChunk({\n",
        "          content: letter,\n",
        "        }),\n",
        "        text: letter,\n",
        "      });\n",
        "      // Trigger the appropriate callback for new chunks\n",
        "      await runManager?.handleLLMNewToken(letter);\n",
        "    }\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们现在可以像使用其他聊天模型一样使用它："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  lc_serializable: true,\n",
            "  lc_kwargs: {\n",
            "    content: 'I am',\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: [],\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  lc_namespace: [ 'langchain_core', 'messages' ],\n",
            "  content: 'I am',\n",
            "  name: undefined,\n",
            "  additional_kwargs: {},\n",
            "  response_metadata: {},\n",
            "  id: undefined,\n",
            "  tool_calls: [],\n",
            "  invalid_tool_calls: [],\n",
            "  usage_metadata: undefined\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const chatModel = new CustomChatModel({ n: 4 });\n",
        "\n",
        "await chatModel.invoke([[\"human\", \"I am an LLM\"]]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "支持流式传输："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessageChunk {\n",
            "  lc_serializable: true,\n",
            "  lc_kwargs: {\n",
            "    content: 'I',\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: [],\n",
            "    tool_call_chunks: [],\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  lc_namespace: [ 'langchain_core', 'messages' ],\n",
            "  content: 'I',\n",
            "  name: undefined,\n",
            "  additional_kwargs: {},\n",
            "  response_metadata: {},\n",
            "  id: undefined,\n",
            "  tool_calls: [],\n",
            "  invalid_tool_calls: [],\n",
            "  tool_call_chunks: [],\n",
            "  usage_metadata: undefined\n",
            "}\n",
            "AIMessageChunk {\n",
            "  lc_serializable: true,\n",
            "  lc_kwargs: {\n",
            "    content: ' ',\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: [],\n",
            "    tool_call_chunks: [],\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  lc_namespace: [ 'langchain_core', 'messages' ],\n",
            "  content: ' ',\n",
            "  name: undefined,\n",
            "  additional_kwargs: {},\n",
            "  response_metadata: {},\n",
            "  id: undefined,\n",
            "  tool_calls: [],\n",
            "  invalid_tool_calls: [],\n",
            "  tool_call_chunks: [],\n",
            "  usage_metadata: undefined\n",
            "}\n",
            "AIMessageChunk {\n",
            "  lc_serializable: true,\n",
            "  lc_kwargs: {\n",
            "    content: 'a',\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: [],\n",
            "    tool_call_chunks: [],\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  lc_namespace: [ 'langchain_core', 'messages' ],\n",
            "  content: 'a',\n",
            "  name: undefined,\n",
            "  additional_kwargs: {},\n",
            "  response_metadata: {},\n",
            "  id: undefined,\n",
            "  tool_calls: [],\n",
            "  invalid_tool_calls: [],\n",
            "  tool_call_chunks: [],\n",
            "  usage_metadata: undefined\n",
            "}\n",
            "AIMessageChunk {\n",
            "  lc_serializable: true,\n",
            "  lc_kwargs: {\n",
            "    content: 'm',\n",
            "    tool_calls: [],\n",
            "    invalid_tool_calls: [],\n",
            "    tool_call_chunks: [],\n",
            "    additional_kwargs: {},\n",
            "    response_metadata: {}\n",
            "  },\n",
            "  lc_namespace: [ 'langchain_core', 'messages' ],\n",
            "  content: 'm',\n",
            "  name: undefined,\n",
            "  additional_kwargs: {},\n",
            "  response_metadata: {},\n",
            "  id: undefined,\n",
            "  tool_calls: [],\n",
            "  invalid_tool_calls: [],\n",
            "  tool_call_chunks: [],\n",
            "  usage_metadata: undefined\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const stream = await chatModel.stream([[\"human\", \"I am an LLM\"]]);\n",
        "\n",
        "for await (const chunk of stream) {\n",
        "  console.log(chunk);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 更丰富的输出\n",
        "\n",
        "如果你想利用 LangChain 的回调系统来实现诸如令牌追踪之类的功能，你可以扩展 [`BaseChatModel`](https://api.js.langchain.com/classes/langchain_core.language_models_chat_models.BaseChatModel.html) 类并实现更低层级的\n",
        "`_generate` 方法。它同样以一个 `BaseMessage` 列表作为输入，但需要你构造并返回一个允许添加额外元数据的 `ChatGeneration` 对象。\n",
        "下面是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { AIMessage, BaseMessage } from \"@langchain/core/messages\";\n",
        "import { ChatResult } from \"@langchain/core/outputs\";\n",
        "import {\n",
        "  BaseChatModel,\n",
        "  BaseChatModelCallOptions,\n",
        "  BaseChatModelParams,\n",
        "} from \"@langchain/core/language_models/chat_models\";\n",
        "import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n",
        "\n",
        "interface AdvancedCustomChatModelOptions\n",
        "  extends BaseChatModelCallOptions {}\n",
        "\n",
        "interface AdvancedCustomChatModelParams extends BaseChatModelParams {\n",
        "  n: number;\n",
        "}\n",
        "\n",
        "class AdvancedCustomChatModel extends BaseChatModel<AdvancedCustomChatModelOptions> {\n",
        "  n: number;\n",
        "\n",
        "  static lc_name(): string {\n",
        "    return \"AdvancedCustomChatModel\";\n",
        "  }\n",
        "\n",
        "  constructor(fields: AdvancedCustomChatModelParams) {\n",
        "    super(fields);\n",
        "    this.n = fields.n;\n",
        "  }\n",
        "\n",
        "  async _generate(\n",
        "    messages: BaseMessage[],\n",
        "    options: this[\"ParsedCallOptions\"],\n",
        "    runManager?: CallbackManagerForLLMRun\n",
        "  ): Promise<ChatResult> {\n",
        "    if (!messages.length) {\n",
        "      throw new Error(\"No messages provided.\");\n",
        "    }\n",
        "    if (typeof messages[0].content !== \"string\") {\n",
        "      throw new Error(\"Multimodal messages are not supported.\");\n",
        "    }\n",
        "    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n",
        "    // await subRunnable.invoke(params, runManager?.getChild());\n",
        "    const content = messages[0].content.slice(0, this.n);\n",
        "    const tokenUsage = {\n",
        "      usedTokens: this.n,\n",
        "    };\n",
        "    return {\n",
        "      generations: [{ message: new AIMessage({ content }), text: content }],\n",
        "      llmOutput: { tokenUsage },\n",
        "    };\n",
        "  }\n",
        "\n",
        "  _llmType(): string {\n",
        "    return \"advanced_custom_chat_model\";\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这会将回调事件和 `streamEvents` 方法中的额外返回信息传递："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"event\": \"on_chat_model_end\",\n",
            "  \"data\": {\n",
            "    \"output\": {\n",
            "      \"lc\": 1,\n",
            "      \"type\": \"constructor\",\n",
            "      \"id\": [\n",
            "        \"langchain_core\",\n",
            "        \"messages\",\n",
            "        \"AIMessage\"\n",
            "      ],\n",
            "      \"kwargs\": {\n",
            "        \"content\": \"I am\",\n",
            "        \"tool_calls\": [],\n",
            "        \"invalid_tool_calls\": [],\n",
            "        \"additional_kwargs\": {},\n",
            "        \"response_metadata\": {\n",
            "          \"tokenUsage\": {\n",
            "            \"usedTokens\": 4\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"run_id\": \"11dbdef6-1b91-407e-a497-1a1ce2974788\",\n",
            "  \"name\": \"AdvancedCustomChatModel\",\n",
            "  \"tags\": [],\n",
            "  \"metadata\": {\n",
            "    \"ls_model_type\": \"chat\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const chatModel = new AdvancedCustomChatModel({ n: 4 });\n",
        "\n",
        "const eventStream = await chatModel.streamEvents([[\"human\", \"I am an LLM\"]], {\n",
        "  version: \"v2\",\n",
        "});\n",
        "\n",
        "for await (const event of eventStream) {\n",
        "  if (event.event === \"on_chat_model_end\") {\n",
        "    console.log(JSON.stringify(event, null, 2));\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 追踪（高级）\n",
        "\n",
        "如果您正在实现一个自定义的聊天模型，并希望将其与 [LangSmith](https://smith.langchain.com/) 等追踪服务一起使用，\n",
        "您可以通过在模型上实现 `invocationParams()` 方法，自动记录某次调用所使用的参数。\n",
        "\n",
        "此方法完全是可选的，但它返回的任何内容都将作为元数据记录到追踪中。\n",
        "\n",
        "以下是一种可能的使用模式："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n",
        "import { BaseChatModel, type BaseChatModelCallOptions, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\n",
        "import { BaseMessage } from \"@langchain/core/messages\";\n",
        "import { ChatResult } from \"@langchain/core/outputs\";\n",
        "\n",
        "interface CustomChatModelOptions extends BaseChatModelCallOptions {\n",
        "  // Some required or optional inner args\n",
        "  tools: Record<string, any>[];\n",
        "}\n",
        "\n",
        "interface CustomChatModelParams extends BaseChatModelParams {\n",
        "  temperature: number;\n",
        "  n: number;\n",
        "}\n",
        "\n",
        "class CustomChatModel extends BaseChatModel<CustomChatModelOptions> {\n",
        "  temperature: number;\n",
        "\n",
        "  n: number;\n",
        "\n",
        "  static lc_name(): string {\n",
        "    return \"CustomChatModel\";\n",
        "  }\n",
        "\n",
        "  constructor(fields: CustomChatModelParams) {\n",
        "    super(fields);\n",
        "    this.temperature = fields.temperature;\n",
        "    this.n = fields.n;\n",
        "  }\n",
        "\n",
        "  // Anything returned in this method will be logged as metadata in the trace.\n",
        "  // It is common to pass it any options used to invoke the function.\n",
        "  invocationParams(options?: this[\"ParsedCallOptions\"]) {\n",
        "    return {\n",
        "      tools: options?.tools,\n",
        "      n: this.n,\n",
        "    };\n",
        "  }\n",
        "\n",
        "  async _generate(\n",
        "    messages: BaseMessage[],\n",
        "    options: this[\"ParsedCallOptions\"],\n",
        "    runManager?: CallbackManagerForLLMRun\n",
        "  ): Promise<ChatResult> {\n",
        "    if (!messages.length) {\n",
        "      throw new Error(\"No messages provided.\");\n",
        "    }\n",
        "    if (typeof messages[0].content !== \"string\") {\n",
        "      throw new Error(\"Multimodal messages are not supported.\");\n",
        "    }\n",
        "    const additionalParams = this.invocationParams(options);\n",
        "    const content = await someAPIRequest(messages, additionalParams);\n",
        "    return {\n",
        "      generations: [{ message: new AIMessage({ content }), text: content }],\n",
        "      llmOutput: {},\n",
        "    };\n",
        "  }\n",
        "\n",
        "  _llmType(): string {\n",
        "    return \"advanced_custom_chat_model\";\n",
        "  }\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}