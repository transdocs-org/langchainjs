{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 3\n",
        "---\n",
        "\n",
        "# 如何创建一个自定义LLM类\n",
        "\n",
        "```{=mdx}\n",
        ":::info 预备知识\n",
        "\n",
        "本指南假定您已经了解以下概念：\n",
        "\n",
        "- [LLMs](/docs/concepts/text_llms)\n",
        "\n",
        ":::\n",
        "```\n",
        "\n",
        "这个笔记本介绍如何创建一个自定义LLM包装器，以防您希望使用自己的LLM或使用一个LangChain未直接支持的包装器。\n",
        "\n",
        "自定义LLM在继承 [`LLM` 类](https://api.js.langchain.com/classes/langchain_core.language_models_llms.LLM.html) 后，需要实现以下必要内容：\n",
        "\n",
        "- 一个 `_call` 方法，接收一个字符串和调用选项（包括诸如 `stop` 序列等内容），并返回一个字符串。\n",
        "- 一个 `_llmType` 方法，返回一个字符串。仅用于日志记录。\n",
        "\n",
        "您还可以实现以下可选方法：\n",
        "\n",
        "- 一个 `_streamResponseChunks` 方法，返回一个 `AsyncIterator` 并逐个产生 [`GenerationChunks`](https://api.js.langchain.com/classes/langchain_core.outputs.GenerationChunk.html)。这使得LLM能够支持流式输出。\n",
        "\n",
        "让我们实现一个非常简单的自定义LLM，它仅回显输入的前 `n` 个字符。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { LLM, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\n",
        "import type { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n",
        "import { GenerationChunk } from \"@langchain/core/outputs\";\n",
        "\n",
        "interface CustomLLMInput extends BaseLLMParams {\n",
        "  n: number;\n",
        "}\n",
        "\n",
        "class CustomLLM extends LLM {\n",
        "  n: number;\n",
        "\n",
        "  constructor(fields: CustomLLMInput) {\n",
        "    super(fields);\n",
        "    this.n = fields.n;\n",
        "  }\n",
        "\n",
        "  _llmType() {\n",
        "    return \"custom\";\n",
        "  }\n",
        "\n",
        "  async _call(\n",
        "    prompt: string,\n",
        "    options: this[\"ParsedCallOptions\"],\n",
        "    runManager: CallbackManagerForLLMRun\n",
        "  ): Promise<string> {\n",
        "    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n",
        "    // await subRunnable.invoke(params, runManager?.getChild());\n",
        "    return prompt.slice(0, this.n);\n",
        "  }\n",
        "\n",
        "  async *_streamResponseChunks(\n",
        "    prompt: string,\n",
        "    options: this[\"ParsedCallOptions\"],\n",
        "    runManager?: CallbackManagerForLLMRun\n",
        "  ): AsyncGenerator<GenerationChunk> {\n",
        "    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n",
        "    // await subRunnable.invoke(params, runManager?.getChild());\n",
        "    for (const letter of prompt.slice(0, this.n)) {\n",
        "      yield new GenerationChunk({\n",
        "        text: letter,\n",
        "      });\n",
        "      // Trigger the appropriate callback\n",
        "      await runManager?.handleLLMNewToken(letter);\n",
        "    }\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们现在可以像使用其他任何LLM一样使用它："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am\n"
          ]
        }
      ],
      "source": [
        "const llm = new CustomLLM({ n: 4 });\n",
        "\n",
        "await llm.invoke(\"I am an LLM\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "支持流式传输："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I\n",
            " \n",
            "a\n",
            "m\n"
          ]
        }
      ],
      "source": [
        "const stream = await llm.stream(\"I am an LLM\");\n",
        "\n",
        "for await (const chunk of stream) {\n",
        "  console.log(chunk);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 更丰富的输出\n",
        "\n",
        "如果你想利用 LangChain 的回调系统来实现诸如令牌追踪之类的功能，可以继承 [`BaseLLM`](https://api.js.langchain.com/classes/langchain_core.language_models_llms.BaseLLM.html) 类并实现更底层的\n",
        "`_generate` 方法。该方法的输入和输出不再只是单个字符串，而是可以接受多个输入字符串，并将每个输入字符串映射为多个字符串输出。\n",
        "此外，它返回的是一个包含附加元数据字段的 `Generation` 输出，而不仅仅是字符串。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n",
        "import { LLMResult } from \"@langchain/core/outputs\";\n",
        "import {\n",
        "  BaseLLM,\n",
        "  BaseLLMCallOptions,\n",
        "  BaseLLMParams,\n",
        "} from \"@langchain/core/language_models/llms\";\n",
        "\n",
        "interface AdvancedCustomLLMCallOptions extends BaseLLMCallOptions {}\n",
        "\n",
        "interface AdvancedCustomLLMParams extends BaseLLMParams {\n",
        "  n: number;\n",
        "}\n",
        "\n",
        "class AdvancedCustomLLM extends BaseLLM<AdvancedCustomLLMCallOptions> {\n",
        "  n: number;\n",
        "\n",
        "  constructor(fields: AdvancedCustomLLMParams) {\n",
        "    super(fields);\n",
        "    this.n = fields.n;\n",
        "  }\n",
        "\n",
        "  _llmType() {\n",
        "    return \"advanced_custom_llm\";\n",
        "  }\n",
        "\n",
        "  async _generate(\n",
        "    inputs: string[],\n",
        "    options: this[\"ParsedCallOptions\"],\n",
        "    runManager?: CallbackManagerForLLMRun\n",
        "  ): Promise<LLMResult> {\n",
        "    const outputs = inputs.map((input) => input.slice(0, this.n));\n",
        "    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n",
        "    // await subRunnable.invoke(params, runManager?.getChild());\n",
        "\n",
        "    // One input could generate multiple outputs.\n",
        "    const generations = outputs.map((output) => [\n",
        "      {\n",
        "        text: output,\n",
        "        // Optional additional metadata for the generation\n",
        "        generationInfo: { outputCount: 1 },\n",
        "      },\n",
        "    ]);\n",
        "    const tokenUsage = {\n",
        "      usedTokens: this.n,\n",
        "    };\n",
        "    return {\n",
        "      generations,\n",
        "      llmOutput: { tokenUsage },\n",
        "    };\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这会将额外的返回信息传递到回调事件和 `streamEvents` 方法中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"event\": \"on_llm_end\",\n",
            "  \"data\": {\n",
            "    \"output\": {\n",
            "      \"generations\": [\n",
            "        [\n",
            "          {\n",
            "            \"text\": \"I am\",\n",
            "            \"generationInfo\": {\n",
            "              \"outputCount\": 1\n",
            "            }\n",
            "          }\n",
            "        ]\n",
            "      ],\n",
            "      \"llmOutput\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"usedTokens\": 4\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"run_id\": \"a9ce50e4-f85b-41eb-bcbe-793efc52f9d8\",\n",
            "  \"name\": \"AdvancedCustomLLM\",\n",
            "  \"tags\": [],\n",
            "  \"metadata\": {}\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const llm = new AdvancedCustomLLM({ n: 4 });\n",
        "\n",
        "const eventStream = await llm.streamEvents(\"I am an LLM\", {\n",
        "  version: \"v2\",\n",
        "});\n",
        "\n",
        "for await (const event of eventStream) {\n",
        "  if (event.event === \"on_llm_end\") {\n",
        "    console.log(JSON.stringify(event, null, 2));\n",
        "  }\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}