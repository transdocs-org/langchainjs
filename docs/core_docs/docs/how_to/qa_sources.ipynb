{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 如何返回来源\n",
        "\n",
        ":::info 预备知识\n",
        "\n",
        "本指南假定您已熟悉以下内容：\n",
        "\n",
        "- [检索增强生成](/docs/tutorials/rag/)\n",
        "\n",
        ":::\n",
        "\n",
        "在问答应用中，向用户展示生成答案所使用的来源信息通常非常重要。实现这一点的最简单方法是让链结构返回每次生成中检索到的文档（Documents）。\n",
        "\n",
        "我们将使用Lilian Weng撰写的[LLM驱动的自主代理](https://lilianweng.github.io/posts/2023-06-23-agent/)博客文章作为本笔记本的检索内容。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 环境配置\n",
        "### 依赖项\n",
        "\n",
        "在本次演示中，我们将使用 OpenAI 的聊天模型和嵌入模型，以及一个用于记忆的向量存储。但这里展示的所有功能都适用于任何 [聊天模型](/docs/concepts/chat_models) 或 [LLM](/docs/concepts/text_llms)、[嵌入模型](/docs/concepts/embedding_models)、[向量存储](/docs/concepts/vectorstores) 或 [检索器](/docs/concepts/retrievers)。\n",
        "\n",
        "我们将使用以下软件包：\n",
        "\n",
        "```bash\n",
        "npm install --save langchain @langchain/openai cheerio\n",
        "```\n",
        "\n",
        "我们需要设置环境变量 `OPENAI_API_KEY`：\n",
        "\n",
        "```bash\n",
        "export OPENAI_API_KEY=你的密钥\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangSmith\n",
        "\n",
        "使用LangChain构建的许多应用程序将包含多个步骤，并多次调用LLM。随着这些应用程序变得越来越复杂，能够检查链或代理内部确切发生的情况变得至关重要。要做到这一点，最佳方式是使用[LangSmith](https://smith.langchain.com/)。\n",
        "\n",
        "请注意，LangSmith并非必需，但它非常有帮助。如果确实想要使用LangSmith，请在上方链接注册后，确保设置环境变量以开始记录追踪信息：\n",
        "\n",
        "\n",
        "```bash\n",
        "export LANGSMITH_TRACING=true\n",
        "export LANGSMITH_API_KEY=YOUR_KEY\n",
        "\n",
        "# 如果你不在无服务器环境中，可启用此选项以减少追踪延迟\n",
        "# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 无来源的链\n",
        "\n",
        "这是我们在Lilian Weng的[LLM驱动的自主代理](https://lilianweng.github.io/posts/2023-06-23-agent/)博客文章中，在[快速入门](/docs/tutorials/qa_chat_history/)部分构建的问答应用程序。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import \"cheerio\";\n",
        "import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n",
        "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
        "import { MemoryVectorStore } from \"langchain/vectorstores/memory\"\n",
        "import { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\n",
        "import { pull } from \"langchain/hub\";\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "import { formatDocumentsAsString } from \"langchain/util/document\";\n",
        "import { RunnableSequence, RunnablePassthrough } from \"@langchain/core/runnables\";\n",
        "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
        "\n",
        "const loader = new CheerioWebBaseLoader(\n",
        "  \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        ");\n",
        "\n",
        "const docs = await loader.load();\n",
        "\n",
        "const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\n",
        "const splits = await textSplitter.splitDocuments(docs);\n",
        "const vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());\n",
        "\n",
        "// Retrieve and generate using the relevant snippets of the blog.\n",
        "const retriever = vectorStore.asRetriever();\n",
        "const prompt = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\n",
        "const llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\n",
        "\n",
        "const ragChain = RunnableSequence.from([\n",
        "  {\n",
        "    context: retriever.pipe(formatDocumentsAsString),\n",
        "    question: new RunnablePassthrough(),\n",
        "  },\n",
        "  prompt,\n",
        "  llm,\n",
        "  new StringOutputParser()\n",
        "]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "让我们看看这个提示实际是什么样子："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: {question} \n",
            "Context: {context} \n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "console.log(prompt.promptMessages.map((msg) => msg.prompt.template).join(\"\\n\"));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u001b[32m\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. T\"\u001b[39m... 254 more characters"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await ragChain.invoke(\"What is task decomposition?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 添加来源\n",
        "\n",
        "使用 LCEL，我们可以轻松地将检索到的文档传递到整个链中，并在最终响应中返回它们："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  question: \u001b[32m\"What is Task Decomposition\"\u001b[39m,\n",
              "  context: [\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"Fig. 1. Overview of a LLM-powered autonomous agent system.\\n\"\u001b[39m +\n",
              "        \u001b[32m\"Component One: Planning#\\n\"\u001b[39m +\n",
              "        \u001b[32m\"A complicated ta\"\u001b[39m... 898 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m'Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are'\u001b[39m... 887 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"Agent System Overview\\n\"\u001b[39m +\n",
              "        \u001b[32m\"                \\n\"\u001b[39m +\n",
              "        \u001b[32m\"                    Component One: Planning\\n\"\u001b[39m +\n",
              "        \u001b[32m\"                 \"\u001b[39m... 850 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    },\n",
              "    Document {\n",
              "      pageContent: \u001b[32m\"Resources:\\n\"\u001b[39m +\n",
              "        \u001b[32m\"1. Internet access for searches and information gathering.\\n\"\u001b[39m +\n",
              "        \u001b[32m\"2. Long Term memory management\"\u001b[39m... 456 more characters,\n",
              "      metadata: {\n",
              "        source: \u001b[32m\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\u001b[39m,\n",
              "        loc: { lines: \u001b[36m[Object]\u001b[39m }\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  answer: \u001b[32m\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps fo\"\u001b[39m... 230 more characters\n",
              "}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import {\n",
        "  RunnableMap,\n",
        "  RunnablePassthrough,\n",
        "  RunnableSequence\n",
        "} from \"@langchain/core/runnables\";\n",
        "import { formatDocumentsAsString } from \"langchain/util/document\";\n",
        "\n",
        "const ragChainWithSources = RunnableMap.from({\n",
        "  // Return raw documents here for now since we want to return them at\n",
        "  // the end - we'll format in the next step of the chain\n",
        "  context: retriever,\n",
        "  question: new RunnablePassthrough(),\n",
        "}).assign({\n",
        "  answer: RunnableSequence.from([\n",
        "    (input) => {\n",
        "      return {\n",
        "        // Now we format the documents as strings for the prompt\n",
        "        context: formatDocumentsAsString(input.context),\n",
        "        question: input.question\n",
        "      };\n",
        "    },\n",
        "    prompt,\n",
        "    llm,\n",
        "    new StringOutputParser()\n",
        "  ]),\n",
        "})\n",
        "\n",
        "await ragChainWithSources.invoke(\"What is Task Decomposition\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[点击此处](https://smith.langchain.com/public/c3753531-563c-40d4-a6bf-21bfe8741d10/r)查看LangSmith追踪，了解链的内部机制。\n",
        "\n",
        "## 下一步\n",
        "\n",
        "你现在已了解如何从QA链中返回来源。\n",
        "\n",
        "接下来，请查看有关RAG的其他指南，例如[如何流式传输响应](/docs/how_to/qa_streaming)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nb_converter": "script",
      "pygments_lexer": "typescript",
      "version": "5.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}