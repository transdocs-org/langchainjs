{
  "cells": [
    {
      "cell_type": "raw",
      "id": "27598444",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e3f0f72",
      "metadata": {},
      "source": [
        "# 如何从模型中返回结构化数据\n",
        "```{=mdx}\n",
        "<span data-heading-keywords=\"with_structured_output\"></span>\n",
        "```\n",
        "\n",
        "让模型返回符合特定模式的输出通常非常有用。一个常见的使用场景是从任意文本中提取数据，以便插入到传统数据库中或用于其他下游系统。本指南将向你展示几种可以使用的不同策略。\n",
        "\n",
        ":::info 前提条件\n",
        "\n",
        "本指南假定你已熟悉以下概念：\n",
        "\n",
        "- [聊天模型](/docs/concepts/chat_models)\n",
        "\n",
        ":::\n",
        "\n",
        "## `.withStructuredOutput()` 方法\n",
        "\n",
        "模型在底层可以使用多种策略。对于一些最流行的模型提供商，包括 [Anthropic](/docs/integrations/platforms/anthropic/)、[Google VertexAI](/docs/integrations/platforms/google/)、[Mistral](/docs/integrations/chat/mistral/) 和 [OpenAI](/docs/integrations/platforms/openai/)，LangChain 实现了一个通用的接口，抽象了这些策略，称为 `.withStructuredOutput`。\n",
        "\n",
        "通过调用此方法（并传入 [JSON schema](https://json-schema.org/) 或 [Zod schema](https://zod.dev/)），模型将自动添加必要的模型参数和输出解析器，以获得符合请求模式的结构化输出。如果模型支持多种实现方式（例如，函数调用与 JSON 模式），你可以通过传入相应方法来配置使用哪种方式。\n",
        "\n",
        "让我们看一些实际示例！我们将使用 Zod 创建一个简单的响应模式。\n",
        "\n",
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs onlyWso={true} />\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "070bf702",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  setup: \u001b[32m\"Why don't cats play poker in the wild?\"\u001b[39m,\n",
              "  punchline: \u001b[32m\"Too many cheetahs.\"\u001b[39m,\n",
              "  rating: \u001b[33m7\u001b[39m\n",
              "}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import { z } from \"zod\";\n",
        "\n",
        "const joke = z.object({\n",
        "  setup: z.string().describe(\"The setup of the joke\"),\n",
        "  punchline: z.string().describe(\"The punchline to the joke\"),\n",
        "  rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\"),\n",
        "});\n",
        "\n",
        "const structuredLlm = model.withStructuredOutput(joke);\n",
        "\n",
        "await structuredLlm.invoke(\"Tell me a joke about cats\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe6efeab",
      "metadata": {},
      "source": [
        "一个关键点是，尽管我们将Zod模式设置为名为`joke`的变量，但Zod无法访问该变量名，因此无法将其传递给模型。虽然这不是必需的，但我们可以为模式传递一个名称，以便向模型提供更多关于该模式所代表内容的上下文，从而提升性能："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f3d01a1d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  setup: \u001b[32m\"Why don't cats play poker in the wild?\"\u001b[39m,\n",
              "  punchline: \u001b[32m\"Too many cheetahs!\"\u001b[39m,\n",
              "  rating: \u001b[33m7\u001b[39m\n",
              "}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "const structuredLlm = model.withStructuredOutput(joke, { name: \"joke\" });\n",
        "\n",
        "await structuredLlm.invoke(\"Tell me a joke about cats\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deddb6d3",
      "metadata": {},
      "source": [
        "结果是一个JSON对象。\n",
        "\n",
        "如果你不想使用Zod，也可以传入一个OpenAI风格的JSON模式字典。该对象应包含三个属性：\n",
        "\n",
        "- `name`：要输出的模式的名称。\n",
        "- `description`：对要输出的模式的高层描述。\n",
        "- `parameters`：你想要提取的模式的嵌套细节，格式为[JSON模式](https://json-schema.org/)字典。\n",
        "\n",
        "在这种情况下，响应也是一个字典："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6700994a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  setup: \u001b[32m\"Why was the cat sitting on the computer?\"\u001b[39m,\n",
              "  punchline: \u001b[32m\"Because it wanted to keep an eye on the mouse!\"\u001b[39m\n",
              "}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "const structuredLlm = model.withStructuredOutput(\n",
        "  {\n",
        "    \"name\": \"joke\",\n",
        "    \"description\": \"Joke to tell user.\",\n",
        "    \"parameters\": {\n",
        "      \"title\": \"Joke\",\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"setup\": {\"type\": \"string\", \"description\": \"The setup for the joke\"},\n",
        "        \"punchline\": {\"type\": \"string\", \"description\": \"The joke's punchline\"},\n",
        "      },\n",
        "      \"required\": [\"setup\", \"punchline\"],\n",
        "    },\n",
        "  }\n",
        ")\n",
        "\n",
        "await structuredLlm.invoke(\"Tell me a joke about cats\", { name: \"joke\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e28c14d3",
      "metadata": {},
      "source": [
        "如果你使用 JSON Schema，可以利用其他更复杂的模式描述来实现类似的效果。\n",
        "\n",
        "如果所选模型支持，你也可以直接使用工具调用，让模型在不同选项间进行选择。这需要更多的解析和设置工作。详见[此操作指南](/docs/how_to/tool_calling/)。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39d7a555",
      "metadata": {},
      "source": [
        "### 指定输出方式（高级）\n",
        "\n",
        "对于支持多种数据输出方式的模型，你可以按如下方式指定首选的输出方式："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "df0370e3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  setup: \u001b[32m\"Why don't cats play poker in the jungle?\"\u001b[39m,\n",
              "  punchline: \u001b[32m\"Too many cheetahs!\"\u001b[39m\n",
              "}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "const structuredLlm = model.withStructuredOutput(joke, {\n",
        "  method: \"json_mode\",\n",
        "  name: \"joke\",\n",
        "})\n",
        "\n",
        "await structuredLlm.invoke(\n",
        "  \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56278a82",
      "metadata": {},
      "source": [
        "在上面的例子中，我们使用了OpenAI的替代JSON模式功能，并结合了一个更具体的提示。\n",
        "\n",
        "关于你选择的模型的具体细节，请查阅其在[API参考页面](https://api.js.langchain.com/)中的条目。\n",
        "\n",
        "### （高级）原始输出\n",
        "\n",
        "LLM在生成结构化输出方面并非完美，特别是当模式变得复杂时。你可以通过传递`includeRaw: true`来避免抛出异常并自行处理原始输出。这将改变输出格式，使其包含原始消息输出和`parsed`值（如果解析成功）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "46b616a4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  raw: AIMessage {\n",
              "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
              "    lc_kwargs: {\n",
              "      content: \u001b[32m\"\"\u001b[39m,\n",
              "      tool_calls: [\n",
              "        {\n",
              "          name: \u001b[32m\"joke\"\u001b[39m,\n",
              "          args: \u001b[36m[Object]\u001b[39m,\n",
              "          id: \u001b[32m\"call_0pEdltlfSXjq20RaBFKSQOeF\"\u001b[39m\n",
              "        }\n",
              "      ],\n",
              "      invalid_tool_calls: [],\n",
              "      additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: [ \u001b[36m[Object]\u001b[39m ] },\n",
              "      response_metadata: {}\n",
              "    },\n",
              "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
              "    content: \u001b[32m\"\"\u001b[39m,\n",
              "    name: \u001b[90mundefined\u001b[39m,\n",
              "    additional_kwargs: {\n",
              "      function_call: \u001b[90mundefined\u001b[39m,\n",
              "      tool_calls: [\n",
              "        {\n",
              "          id: \u001b[32m\"call_0pEdltlfSXjq20RaBFKSQOeF\"\u001b[39m,\n",
              "          type: \u001b[32m\"function\"\u001b[39m,\n",
              "          function: \u001b[36m[Object]\u001b[39m\n",
              "        }\n",
              "      ]\n",
              "    },\n",
              "    response_metadata: {\n",
              "      tokenUsage: { completionTokens: \u001b[33m33\u001b[39m, promptTokens: \u001b[33m88\u001b[39m, totalTokens: \u001b[33m121\u001b[39m },\n",
              "      finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
              "    },\n",
              "    tool_calls: [\n",
              "      {\n",
              "        name: \u001b[32m\"joke\"\u001b[39m,\n",
              "        args: {\n",
              "          setup: \u001b[32m\"Why was the cat sitting on the computer?\"\u001b[39m,\n",
              "          punchline: \u001b[32m\"Because it wanted to keep an eye on the mouse!\"\u001b[39m,\n",
              "          rating: \u001b[33m7\u001b[39m\n",
              "        },\n",
              "        id: \u001b[32m\"call_0pEdltlfSXjq20RaBFKSQOeF\"\u001b[39m\n",
              "      }\n",
              "    ],\n",
              "    invalid_tool_calls: [],\n",
              "    usage_metadata: { input_tokens: \u001b[33m88\u001b[39m, output_tokens: \u001b[33m33\u001b[39m, total_tokens: \u001b[33m121\u001b[39m }\n",
              "  },\n",
              "  parsed: {\n",
              "    setup: \u001b[32m\"Why was the cat sitting on the computer?\"\u001b[39m,\n",
              "    punchline: \u001b[32m\"Because it wanted to keep an eye on the mouse!\"\u001b[39m,\n",
              "    rating: \u001b[33m7\u001b[39m\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "const joke = z.object({\n",
        "  setup: z.string().describe(\"The setup of the joke\"),\n",
        "  punchline: z.string().describe(\"The punchline to the joke\"),\n",
        "  rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\"),\n",
        "});\n",
        "\n",
        "const structuredLlm = model.withStructuredOutput(joke, { includeRaw: true, name: \"joke\" });\n",
        "\n",
        "await structuredLlm.invoke(\"Tell me a joke about cats\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e92a98a",
      "metadata": {},
      "source": [
        "## 提示技术\n",
        "\n",
        "你还可以提示模型以特定格式输出信息。这种方法依赖于设计良好的提示，并随后解析模型的输出。对于不支持 `.with_structured_output()` 或其他内置方法的模型，这是唯一的选择。\n",
        "\n",
        "### 使用 `JsonOutputParser`\n",
        "\n",
        "以下示例使用内置的 [`JsonOutputParser`](https://api.js.langchain.com/classes/langchain_core.output_parsers.JsonOutputParser.html) 来解析聊天模型的输出，该模型被提示以匹配给定的 JSON Schema。请注意，我们正在通过解析器上的一个方法，将 `format_instructions` 直接添加到提示中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6e514455",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { JsonOutputParser } from \"@langchain/core/output_parsers\";\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "type Person = {\n",
        "    name: string;\n",
        "    height_in_meters: number;\n",
        "};\n",
        "\n",
        "type People = {\n",
        "    people: Person[];\n",
        "};\n",
        "\n",
        "const formatInstructions = `Respond only in valid JSON. The JSON object you return should match the following schema:\n",
        "{{ people: [{{ name: \"string\", height_in_meters: \"number\" }}] }}\n",
        "\n",
        "Where people is an array of objects, each with a name and height_in_meters field.\n",
        "`\n",
        "\n",
        "// Set up a parser\n",
        "const parser = new JsonOutputParser<People>();\n",
        "\n",
        "// Prompt\n",
        "const prompt = await ChatPromptTemplate.fromMessages(\n",
        "    [\n",
        "        [\n",
        "            \"system\",\n",
        "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
        "        ],\n",
        "        [\n",
        "            \"human\",\n",
        "            \"{query}\",\n",
        "        ]\n",
        "    ]\n",
        ").partial({\n",
        "    format_instructions: formatInstructions,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "082fa166",
      "metadata": {},
      "source": [
        "让我们看看发送给模型的信息是什么："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3d73d33d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System: Answer the user query. Wrap the output in `json` tags\n",
            "Respond only in valid JSON. The JSON object you return should match the following schema:\n",
            "{{ people: [{{ name: \"string\", height_in_meters: \"number\" }}] }}\n",
            "\n",
            "Where people is an array of objects, each with a name and height_in_meters field.\n",
            "\n",
            "Human: Anna is 23 years old and she is 6 feet tall\n"
          ]
        }
      ],
      "source": [
        "const query = \"Anna is 23 years old and she is 6 feet tall\"\n",
        "\n",
        "console.log((await prompt.format({ query })).toString())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "081956b9",
      "metadata": {},
      "source": [
        "现在让我们调用它："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8d6b3d17",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{ people: [ { name: \u001b[32m\"Anna\"\u001b[39m, height_in_meters: \u001b[33m1.83\u001b[39m } ] }"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "const chain = prompt.pipe(model).pipe(parser);\n",
        "\n",
        "await chain.invoke({ query })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6732dd87",
      "metadata": {},
      "source": [
        "如需深入了解如何使用输出解析器配合提示技术生成结构化输出，请参阅[本指南](/docs/how_to/output_parser_structured)。\n",
        "\n",
        "### 自定义解析\n",
        "\n",
        "您还可以使用[LangChain 表达式语言 (LCEL)](/docs/concepts/lcel) 创建自定义提示和解析器，通过普通函数来解析模型的输出："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "525721b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { AIMessage } from \"@langchain/core/messages\";\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "type Person = {\n",
        "    name: string;\n",
        "    height_in_meters: number;\n",
        "};\n",
        "\n",
        "type People = {\n",
        "    people: Person[];\n",
        "};\n",
        "\n",
        "const schema = `{{ people: [{{ name: \"string\", height_in_meters: \"number\" }}] }}`\n",
        "\n",
        "// Prompt\n",
        "const prompt = await ChatPromptTemplate.fromMessages(\n",
        "    [\n",
        "        [\n",
        "            \"system\",\n",
        "            `Answer the user query. Output your answer as JSON that\n",
        "matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`.\n",
        "Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags`\n",
        "        ],\n",
        "        [\n",
        "            \"human\",\n",
        "            \"{query}\",\n",
        "        ]\n",
        "    ]\n",
        ").partial({\n",
        "    schema\n",
        "});\n",
        "\n",
        "/**\n",
        " * Custom extractor\n",
        " * \n",
        " * Extracts JSON content from a string where\n",
        " * JSON is embedded between ```json and ``` tags.\n",
        " */\n",
        "const extractJson = (output: AIMessage): Array<People> => {\n",
        "    const text = output.content as string;\n",
        "    // Define the regular expression pattern to match JSON blocks\n",
        "    const pattern = /```json(.*?)```/gs;\n",
        "\n",
        "    // Find all non-overlapping matches of the pattern in the string\n",
        "    const matches = text.match(pattern);\n",
        "\n",
        "    // Process each match, attempting to parse it as JSON\n",
        "    try {\n",
        "        return matches?.map(match => {\n",
        "            // Remove the markdown code block syntax to isolate the JSON string\n",
        "            const jsonStr = match.replace(/```json|```/g, '').trim();\n",
        "            return JSON.parse(jsonStr);\n",
        "        }) ?? [];\n",
        "    } catch (error) {\n",
        "        throw new Error(`Failed to parse: ${output}`);\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f1bc8f7",
      "metadata": {},
      "source": [
        "这是发送给模型的提示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c8a30d0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System: Answer the user query. Output your answer as JSON that\n",
            "matches the given schema: ```json\n",
            "{{ people: [{{ name: \"string\", height_in_meters: \"number\" }}] }}\n",
            "```.\n",
            "Make sure to wrap the answer in ```json and ``` tags\n",
            "Human: Anna is 23 years old and she is 6 feet tall\n"
          ]
        }
      ],
      "source": [
        "const query = \"Anna is 23 years old and she is 6 feet tall\"\n",
        "\n",
        "console.log((await prompt.format({ query })).toString())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec018893",
      "metadata": {},
      "source": [
        "调用它时的效果如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e1e7baf6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\n",
              "  { people: [ { name: \u001b[32m\"Anna\"\u001b[39m, height_in_meters: \u001b[33m1.83\u001b[39m } ] }\n",
              "]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import { RunnableLambda } from \"@langchain/core/runnables\";\n",
        "\n",
        "const chain = prompt.pipe(model).pipe(new RunnableLambda({ func: extractJson }));\n",
        "\n",
        "await chain.invoke({ query })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a39221a",
      "metadata": {},
      "source": [
        "## 下一步\n",
        "\n",
        "现在你已经学习了几种让模型输出结构化数据的方法。\n",
        "\n",
        "如需进一步学习，请查看本节中的其他操作指南或关于工具调用的概念指南。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nb_converter": "script",
      "pygments_lexer": "typescript",
      "version": "5.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}