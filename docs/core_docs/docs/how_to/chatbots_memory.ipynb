{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_position: 1\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 如何为聊天机器人添加记忆功能\n",
        "\n",
        "聊天机器人的一个关键特性是能够将先前对话内容作为上下文使用。这种状态管理可以采取多种形式，包括：\n",
        "\n",
        "- 简单地将之前的消息填充到聊天模型的提示中。\n",
        "- 上述方法的基础上，修剪旧消息以减少模型需要处理的干扰信息量。\n",
        "- 更复杂的修改方式，例如为长时间运行的对话合成摘要。\n",
        "\n",
        "我们将在下面详细介绍几种技术！\n",
        "\n",
        ":::note\n",
        "\n",
        "本操作指南之前构建了一个使用 [RunnableWithMessageHistory](https://v03.api.js.langchain.com/classes/_langchain_core.runnables.RunnableWithMessageHistory.html) 的聊天机器人。你可以在 [v0.2 文档](https://js.langchain.com/v0.2/docs/how_to/chatbots_memory/) 中找到该版本的教程。\n",
        "\n",
        "与 `RunnableWithMessageHistory` 相比，LangGraph 实现提供了许多优势，包括能够持久化应用程序状态的任意组件（而不仅仅是消息）。\n",
        "\n",
        ":::\n",
        "\n",
        "## 准备工作\n",
        "\n",
        "你需要安装一些包，选择你的聊天模型，并设置其环境变量。\n",
        "\n",
        "```{=mdx}\n",
        "import Npm2Yarn from \"@theme/Npm2Yarn\"\n",
        "\n",
        "<Npm2Yarn>\n",
        "  @langchain/core @langchain/langgraph\n",
        "</Npm2Yarn>\n",
        "```\n",
        "\n",
        "让我们设置一个聊天模型，用于下面的示例。\n",
        "\n",
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs />\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 消息传递\n",
        "\n",
        "最简单的记忆形式是将聊天历史消息传递到一个链中。以下是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llm = new ChatOpenAI({ model: \"gpt-4o\" })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-ABSxUXVIBitFRBh9MpasB5jeEHfCA\",\n",
            "  \"content\": \"I said \\\"J'adore la programmation,\\\" which means \\\"I love programming\\\" in French.\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"completionTokens\": 18,\n",
            "      \"promptTokens\": 58,\n",
            "      \"totalTokens\": 76\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"system_fingerprint\": \"fp_e375328146\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"input_tokens\": 58,\n",
            "    \"output_tokens\": 18,\n",
            "    \"total_tokens\": 76\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
        "import {\n",
        "  ChatPromptTemplate,\n",
        "  MessagesPlaceholder,\n",
        "} from \"@langchain/core/prompts\";\n",
        "\n",
        "const prompt = ChatPromptTemplate.fromMessages([\n",
        "  [\n",
        "    \"system\",\n",
        "    \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "  ],\n",
        "  new MessagesPlaceholder(\"messages\"),\n",
        "]);\n",
        "\n",
        "const chain = prompt.pipe(llm);\n",
        "\n",
        "await chain.invoke({\n",
        "  messages: [\n",
        "    new HumanMessage(\n",
        "      \"Translate this sentence from English to French: I love programming.\"\n",
        "    ),\n",
        "    new AIMessage(\"J'adore la programmation.\"),\n",
        "    new HumanMessage(\"What did you just say?\"),\n",
        "  ],\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以看到，通过将之前的对话传递给一个链式结构，它可以将其用作回答问题的上下文。这是聊天机器人记忆功能的基本概念——本指南的其余部分将演示传递或重新格式化消息的便捷技术。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 自动历史记录管理\n",
        "\n",
        "前面的示例显式地将消息传递给链（和模型）。这是一种完全可以接受的方法，但它确实需要对外部的新消息进行管理。LangChain 还提供了一种使用 LangGraph 的持久化功能来构建具有记忆能力的应用程序的方法。您可以通过在编译图时提供一个 `checkpointer` 来启用 LangGraph 应用程序中的持久化功能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\n",
        "\n",
        "\n",
        "// Define the function that calls the model\n",
        "const callModel = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const systemPrompt = \n",
        "    \"You are a helpful assistant. \" +\n",
        "    \"Answer all questions to the best of your ability.\";\n",
        "  const messages = [{ role: \"system\", content: systemPrompt }, ...state.messages];\n",
        "  const response = await llm.invoke(messages);\n",
        "  return { messages: response };\n",
        "};\n",
        "\n",
        "const workflow = new StateGraph(MessagesAnnotation)\n",
        "// Define the node and edge\n",
        "  .addNode(\"model\", callModel)\n",
        "  .addEdge(START, \"model\")\n",
        "  .addEdge(\"model\", END);\n",
        "\n",
        "// Add simple in-memory checkpointer\n",
        "// highlight-start\n",
        "const memory = new MemorySaver();\n",
        "const app = workflow.compile({ checkpointer: memory });\n",
        "// highlight-end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们会将最新的输入传递到此处的对话中，并让LangGraph使用检查点来跟踪对话历史记录："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"227b82a9-4084-46a5-ac79-ab9a3faa140e\",\n",
            "      \"content\": \"Translate to French: I love programming.\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-ABSxVrvztgnasTeMSFbpZQmyYqjJZ\",\n",
            "      \"content\": \"J'adore la programmation.\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 5,\n",
            "          \"promptTokens\": 35,\n",
            "          \"totalTokens\": 40\n",
            "        },\n",
            "        \"finish_reason\": \"stop\",\n",
            "        \"system_fingerprint\": \"fp_52a7f40b0b\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 35,\n",
            "        \"output_tokens\": 5,\n",
            "        \"total_tokens\": 40\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "await app.invoke(\n",
        "  {\n",
        "    messages: [\n",
        "      {\n",
        "        role: \"user\",\n",
        "        content: \"Translate to French: I love programming.\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    configurable: { thread_id: \"1\" }\n",
        "  }\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"1a0560a4-9dcb-47a1-b441-80717e229706\",\n",
            "      \"content\": \"Translate to French: I love programming.\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-ABSxVrvztgnasTeMSFbpZQmyYqjJZ\",\n",
            "      \"content\": \"J'adore la programmation.\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 5,\n",
            "          \"promptTokens\": 35,\n",
            "          \"totalTokens\": 40\n",
            "        },\n",
            "        \"finish_reason\": \"stop\",\n",
            "        \"system_fingerprint\": \"fp_52a7f40b0b\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    HumanMessage {\n",
            "      \"id\": \"4f233a7d-4b08-4f53-bb60-cf0141a59721\",\n",
            "      \"content\": \"What did I just ask you?\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-ABSxVs5QnlPfbihTOmJrCVg1Dh7Ol\",\n",
            "      \"content\": \"You asked me to translate \\\"I love programming\\\" into French.\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 13,\n",
            "          \"promptTokens\": 55,\n",
            "          \"totalTokens\": 68\n",
            "        },\n",
            "        \"finish_reason\": \"stop\",\n",
            "        \"system_fingerprint\": \"fp_9f2bfdaa89\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 55,\n",
            "        \"output_tokens\": 13,\n",
            "        \"total_tokens\": 68\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "await app.invoke(\n",
        "  {\n",
        "    messages: [\n",
        "      {\n",
        "        role: \"user\",\n",
        "        content: \"What did I just ask you?\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    configurable: { thread_id: \"1\" }\n",
        "  }\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 修改聊天历史\n",
        "\n",
        "修改存储的聊天消息可以帮助你的聊天机器人处理各种情况。以下是一些示例：\n",
        "\n",
        "### 裁剪消息\n",
        "\n",
        "LLM 和聊天模型具有有限的上下文窗口，即使你没有直接触及限制，你可能也希望限制模型需要处理的干扰信息量。一种解决方案是在将消息传递给模型之前先对其进行裁剪。让我们以上面声明的 `app` 为例说明："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"63057c3d-f980-4640-97d6-497a9f83ddee\",\n",
            "      \"content\": \"Hey there! I'm Nemo.\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"c9f0c20a-8f55-4909-b281-88f2a45c4f05\",\n",
            "      \"content\": \"Hello!\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    HumanMessage {\n",
            "      \"id\": \"fd7fb3a0-7bc7-4e84-99a9-731b30637b55\",\n",
            "      \"content\": \"How are you today?\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"09b0debb-1d4a-4856-8821-b037f5d96ecf\",\n",
            "      \"content\": \"Fine thanks!\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    HumanMessage {\n",
            "      \"id\": \"edc13b69-25a0-40ac-81b3-175e65dc1a9a\",\n",
            "      \"content\": \"What's my name?\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-ABSxWKCTdRuh2ZifXsvFHSo5z5I0J\",\n",
            "      \"content\": \"Your name is Nemo! How can I assist you today, Nemo?\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 14,\n",
            "          \"promptTokens\": 63,\n",
            "          \"totalTokens\": 77\n",
            "        },\n",
            "        \"finish_reason\": \"stop\",\n",
            "        \"system_fingerprint\": \"fp_a5d11b2ef2\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 63,\n",
            "        \"output_tokens\": 14,\n",
            "        \"total_tokens\": 77\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const demoEphemeralChatHistory = [\n",
        "  { role: \"user\", content: \"Hey there! I'm Nemo.\" },\n",
        "  { role: \"assistant\", content: \"Hello!\" },\n",
        "  { role: \"user\", content: \"How are you today?\" },\n",
        "  { role: \"assistant\", content: \"Fine thanks!\" },\n",
        "];\n",
        "\n",
        "await app.invoke(\n",
        "  {\n",
        "    messages: [\n",
        "      ...demoEphemeralChatHistory,\n",
        "      { role: \"user\", content: \"What's my name?\" }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    configurable: { thread_id: \"2\" }\n",
        "  }\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以看到应用记住了预加载的名称。\n",
        "\n",
        "但假设我们有一个非常小的上下文窗口，我们希望将传递给模型的消息数量裁剪为仅保留最近的两条消息。我们可以使用内置的[trimMessages](/docs/how_to/trim_messages/)工具，在消息到达我们的提示词之前根据其令牌数对其进行裁剪。在这个例子中，我们将每条消息计为1个“令牌”，并且只保留最后两条消息："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\n",
        "import { trimMessages } from \"@langchain/core/messages\";\n",
        "\n",
        "// Define trimmer\n",
        "// highlight-start\n",
        "// count each message as 1 \"token\" (tokenCounter: (msgs) => msgs.length) and keep only the last two messages\n",
        "const trimmer = trimMessages({ strategy: \"last\", maxTokens: 2, tokenCounter: (msgs) => msgs.length });\n",
        "// highlight-end\n",
        "\n",
        "// Define the function that calls the model\n",
        "const callModel2 = async (state: typeof MessagesAnnotation.State) => {\n",
        "  // highlight-start\n",
        "  const trimmedMessages = await trimmer.invoke(state.messages);\n",
        "  const systemPrompt = \n",
        "    \"You are a helpful assistant. \" +\n",
        "    \"Answer all questions to the best of your ability.\";\n",
        "  const messages = [{ role: \"system\", content: systemPrompt }, ...trimmedMessages];\n",
        "  // highlight-end\n",
        "  const response = await llm.invoke(messages);\n",
        "  return { messages: response };\n",
        "};\n",
        "\n",
        "const workflow2 = new StateGraph(MessagesAnnotation)\n",
        "  // Define the node and edge\n",
        "  .addNode(\"model\", callModel2)\n",
        "  .addEdge(START, \"model\")\n",
        "  .addEdge(\"model\", END);\n",
        "\n",
        "// Add simple in-memory checkpointer\n",
        "const app2 = workflow2.compile({ checkpointer: new MemorySaver() });"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "让我们调用这个新应用并检查响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    HumanMessage {\n",
            "      \"id\": \"0d9330a0-d9d1-4aaf-8171-ca1ac6344f7c\",\n",
            "      \"content\": \"What is my name?\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"3a24e88b-7525-4797-9fcd-d751a378d22c\",\n",
            "      \"content\": \"Fine thanks!\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    HumanMessage {\n",
            "      \"id\": \"276039c8-eba8-4c68-b015-81ec7704140d\",\n",
            "      \"content\": \"How are you today?\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"2ad4f461-20e1-4982-ba3b-235cb6b02abd\",\n",
            "      \"content\": \"Hello!\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {},\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": []\n",
            "    },\n",
            "    HumanMessage {\n",
            "      \"id\": \"52213cae-953a-463d-a4a0-a7368c9ee4db\",\n",
            "      \"content\": \"Hey there! I'm Nemo.\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-ABSxWe9BRDl1pmzkNIDawWwU3hvKm\",\n",
            "      \"content\": \"I'm sorry, but I don't have access to personal information about you unless you've shared it with me during our conversation. How can I assist you today?\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 30,\n",
            "          \"promptTokens\": 39,\n",
            "          \"totalTokens\": 69\n",
            "        },\n",
            "        \"finish_reason\": \"stop\",\n",
            "        \"system_fingerprint\": \"fp_3537616b13\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 39,\n",
            "        \"output_tokens\": 30,\n",
            "        \"total_tokens\": 69\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "await app2.invoke(\n",
        "  {\n",
        "    messages: [\n",
        "      ...demoEphemeralChatHistory,\n",
        "      { role: \"user\", content: \"What is my name?\" }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    configurable: { thread_id: \"3\" }\n",
        "  }\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们可以看到 `trimMessages` 被调用了，并且只有两个最近的消息会被传递给模型。在这种情况下，这意味着模型忘记了我们给它的名称。\n",
        "\n",
        "查看更多内容请访问我们的[消息裁剪指南](/docs/how_to/trim_messages/)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 总结记忆\n",
        "\n",
        "我们还可以以其他方式使用相同的模式。例如，我们可以在调用应用程序之前，使用额外的LLM调用来生成对话的摘要。让我们重新创建我们的聊天记录："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "const demoEphemeralChatHistory2 = [\n",
        "  { role: \"user\", content: \"Hey there! I'm Nemo.\" },\n",
        "  { role: \"assistant\", content: \"Hello!\" },\n",
        "  { role: \"user\", content: \"How are you today?\" },\n",
        "  { role: \"assistant\", content: \"Fine thanks!\" },\n",
        "];"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在，让我们更新模型调用函数，将之前的交互内容提炼成一个摘要："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\n",
        "import { RemoveMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "\n",
        "// Define the function that calls the model\n",
        "const callModel3 = async (state: typeof MessagesAnnotation.State) => {\n",
        "  const systemPrompt = \n",
        "    \"You are a helpful assistant. \" +\n",
        "    \"Answer all questions to the best of your ability. \" +\n",
        "    \"The provided chat history includes a summary of the earlier conversation.\";\n",
        "  const systemMessage = { role: \"system\", content: systemPrompt };\n",
        "  const messageHistory = state.messages.slice(0, -1); // exclude the most recent user input\n",
        "  \n",
        "  // Summarize the messages if the chat history reaches a certain size\n",
        "  if (messageHistory.length >= 4) {\n",
        "    const lastHumanMessage = state.messages[state.messages.length - 1];\n",
        "    // Invoke the model to generate conversation summary\n",
        "    const summaryPrompt = \n",
        "      \"Distill the above chat messages into a single summary message. \" +\n",
        "      \"Include as many specific details as you can.\";\n",
        "    const summaryMessage = await llm.invoke([\n",
        "      ...messageHistory,\n",
        "      { role: \"user\", content: summaryPrompt }\n",
        "    ]);\n",
        "\n",
        "    // Delete messages that we no longer want to show up\n",
        "    const deleteMessages = state.messages.map(m => new RemoveMessage({ id: m.id }));\n",
        "    // Re-add user message\n",
        "    const humanMessage = { role: \"user\", content: lastHumanMessage.content };\n",
        "    // Call the model with summary & response\n",
        "    const response = await llm.invoke([systemMessage, summaryMessage, humanMessage]);\n",
        "    return { messages: [summaryMessage, humanMessage, response, ...deleteMessages] };\n",
        "  } else {\n",
        "    const response = await llm.invoke([systemMessage, ...state.messages]);\n",
        "    return { messages: response };\n",
        "  }\n",
        "};\n",
        "\n",
        "const workflow3 = new StateGraph(MessagesAnnotation)\n",
        "  // Define the node and edge\n",
        "  .addNode(\"model\", callModel3)\n",
        "  .addEdge(START, \"model\")\n",
        "  .addEdge(\"model\", END);\n",
        "\n",
        "// Add simple in-memory checkpointer\n",
        "const app3 = workflow3.compile({ checkpointer: new MemorySaver() });"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "看看它是否记得我们给它的名字："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  messages: [\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-ABSxXjFDj6WRo7VLSneBtlAxUumPE\",\n",
            "      \"content\": \"Nemo greeted the assistant and asked how it was doing, to which the assistant responded that it was fine.\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 22,\n",
            "          \"promptTokens\": 60,\n",
            "          \"totalTokens\": 82\n",
            "        },\n",
            "        \"finish_reason\": \"stop\",\n",
            "        \"system_fingerprint\": \"fp_e375328146\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 60,\n",
            "        \"output_tokens\": 22,\n",
            "        \"total_tokens\": 82\n",
            "      }\n",
            "    },\n",
            "    HumanMessage {\n",
            "      \"id\": \"8b1309b7-c09e-47fb-9ab3-34047f6973e3\",\n",
            "      \"content\": \"What did I say my name was?\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {}\n",
            "    },\n",
            "    AIMessage {\n",
            "      \"id\": \"chatcmpl-ABSxYAQKiBsQ6oVypO4CLFDsi1HRH\",\n",
            "      \"content\": \"You mentioned that your name is Nemo.\",\n",
            "      \"additional_kwargs\": {},\n",
            "      \"response_metadata\": {\n",
            "        \"tokenUsage\": {\n",
            "          \"completionTokens\": 8,\n",
            "          \"promptTokens\": 73,\n",
            "          \"totalTokens\": 81\n",
            "        },\n",
            "        \"finish_reason\": \"stop\",\n",
            "        \"system_fingerprint\": \"fp_52a7f40b0b\"\n",
            "      },\n",
            "      \"tool_calls\": [],\n",
            "      \"invalid_tool_calls\": [],\n",
            "      \"usage_metadata\": {\n",
            "        \"input_tokens\": 73,\n",
            "        \"output_tokens\": 8,\n",
            "        \"total_tokens\": 81\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "await app3.invoke(\n",
        "  {\n",
        "    messages: [\n",
        "      ...demoEphemeralChatHistory2,\n",
        "      { role: \"user\", content: \"What did I say my name was?\" }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    configurable: { thread_id: \"4\" }\n",
        "  }\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "请注意，再次调用应用程序将继续累积历史记录，直到达到指定的消息数量（在我们的例子中是四条）。此时，我们将根据初始摘要和新消息生成另一个摘要，依此类推。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}