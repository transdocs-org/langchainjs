{
  "cells": [
    {
      "cell_type": "raw",
      "id": "67db2992",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: MistralAI\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9597802c",
      "metadata": {},
      "source": [
        "# MistralAI\n",
        "\n",
        "```{=mdx}\n",
        "\n",
        ":::tip\n",
        "想要在本地运行Mistral的模型吗？请查看我们的 [Ollama集成](/docs/integrations/chat/ollama)。\n",
        ":::\n",
        "\n",
        ":::caution\n",
        "您当前所在的页面是关于将Mistral模型用作 [文本补全模型](/docs/concepts/text_llms) 的文档。Mistral上许多流行的模型是 [聊天补全模型](/docs/concepts/chat_models)。\n",
        "\n",
        "您可能需要查看 [这个页面](/docs/integrations/chat/mistral/)。\n",
        ":::\n",
        "\n",
        "```\n",
        "\n",
        "[Mistral AI](https://mistral.ai/) 是一个提供其强大 [开源模型](https://docs.mistral.ai/getting-started/models/) 托管服务的平台。\n",
        "\n",
        "这将帮助您开始使用LangChain的MistralAI补全模型（LLMs）。有关 `MistralAI` 功能和配置选项的详细文档，请参考 [API参考文档](https://api.js.langchain.com/classes/langchain_mistralai.MistralAI.html)。\n",
        "\n",
        "## 概述\n",
        "### 集成详情\n",
        "\n",
        "| 类 | 包 | 本地 | 可序列化 | Python支持 | 包下载量 | 最新包 |\n",
        "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
        "| [MistralAI](https://api.js.langchain.com/classes/langchain_mistralai.MistralAI.html) | [`@langchain/mistralai`](https://www.npmjs.com/package/@langchain/mistralai) | ❌ | ✅ | ❌ | ![NPM - 下载量](https://img.shields.io/npm/dm/@langchain/mistralai?style=flat-square&label=%20&) | ![NPM - 版本](https://img.shields.io/npm/v/@langchain/mistralai?style=flat-square&label=%20&) |\n",
        "\n",
        "## 准备工作\n",
        "\n",
        "要访问MistralAI模型，您需要创建一个MistralAI账户，获取API密钥，并安装 `@langchain/mistralai` 集成包。\n",
        "\n",
        "### 凭证\n",
        "\n",
        "前往 [console.mistral.ai](https://console.mistral.ai/) 注册MistralAI并生成API密钥。完成此操作后，请设置 `MISTRAL_API_KEY` 环境变量：\n",
        "\n",
        "```bash\n",
        "export MISTRAL_API_KEY=\"your-api-key\"\n",
        "```\n",
        "\n",
        "如果您希望自动追踪您的模型调用，也可以取消以下 [LangSmith](https://docs.smith.langchain.com/) API密钥的注释：\n",
        "\n",
        "```bash\n",
        "# export LANGSMITH_TRACING=\"true\"\n",
        "# export LANGSMITH_API_KEY=\"your-api-key\"\n",
        "```\n",
        "\n",
        "### 安装\n",
        "\n",
        "LangChain的MistralAI集成位于 `@langchain/mistralai` 包中：\n",
        "\n",
        "```{=mdx}\n",
        "import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n",
        "import Npm2Yarn from \"@theme/Npm2Yarn\";\n",
        "\n",
        "<IntegrationInstallTooltip></IntegrationInstallTooltip>\n",
        "\n",
        "<Npm2Yarn>\n",
        "  @langchain/mistralai @langchain/core\n",
        "</Npm2Yarn>\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a760037",
      "metadata": {},
      "source": [
        "## 实例化\n",
        "\n",
        "现在我们可以实例化我们的模型对象并生成聊天补全："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a0562a13",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { MistralAI } from \"@langchain/mistralai\"\n",
        "\n",
        "const llm = new MistralAI({\n",
        "  model: \"codestral-latest\",\n",
        "  temperature: 0,\n",
        "  maxTokens: undefined,\n",
        "  maxRetries: 2,\n",
        "  // other params...\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ee90032",
      "metadata": {},
      "source": [
        "## 调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "035dea0f",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " has developed Mistral 7B, a large language model (LLM) that is open-source and available for commercial use. Mistral 7B is a 7 billion parameter model that is trained on a diverse and high-quality dataset, and it has been fine-tuned to perform well on a variety of tasks, including text generation, question answering, and code interpretation.\n",
            "\n",
            "MistralAI has made Mistral 7B available under a permissive license, allowing anyone to use the model for commercial purposes without having to pay any fees. This has made Mistral 7B a popular choice for businesses and organizations that want to leverage the power of large language models without incurring high costs.\n",
            "\n",
            "Mistral 7B has been trained on a diverse and high-quality dataset, which has enabled it to perform well on a variety of tasks. It has been fine-tuned to generate coherent and contextually relevant text, and it has been shown to be capable of answering complex questions and interpreting code.\n",
            "\n",
            "Mistral 7B is also a highly efficient model, capable of processing text at a fast pace. This makes it well-suited for applications that require real-time responses, such as chatbots and virtual assistants.\n",
            "\n",
            "Overall, Mistral 7B is a powerful and versatile large language model that is open-source and available for commercial use. Its ability to perform well on a variety of tasks, its efficiency, and its permissive license make it a popular choice for businesses and organizations that want to leverage the power of large language models.\n"
          ]
        }
      ],
      "source": [
        "const inputText = \"MistralAI is an AI company that \"\n",
        "\n",
        "const completion = await llm.invoke(inputText)\n",
        "completion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add38532",
      "metadata": {},
      "source": [
        "## 链式调用\n",
        "\n",
        "我们可以将完成模型与提示模板[链式调用](/docs/how_to/sequence/)，如下所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "078e9db2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "I love programming.\n",
            "\n",
            "Ich liebe Programmieren.\n",
            "\n",
            "In German, the phrase \"I love programming\" is translated as \"Ich liebe Programmieren.\" The word \"programming\" is translated to \"Programmieren,\" and \"I love\" is translated to \"Ich liebe.\"\n"
          ]
        }
      ],
      "source": [
        "import { PromptTemplate } from \"@langchain/core/prompts\"\n",
        "\n",
        "const prompt = PromptTemplate.fromTemplate(\"How to say {input} in {output_language}:\\n\")\n",
        "\n",
        "const chain = prompt.pipe(llm);\n",
        "await chain.invoke(\n",
        "  {\n",
        "    output_language: \"German\",\n",
        "    input: \"I love programming.\",\n",
        "  }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e99eef30",
      "metadata": {},
      "source": [
        "由于Mistral LLM是一个补全模型，因此它也允许您向提示中插入一个`suffix`（后缀）。调用模型时，可以通过如下调用选项传递后缀："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ec67551d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "console.log('hello world');\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "const suffixResponse = await llm.invoke(\n",
        "  \"You can print 'hello world' to the console in javascript like this:\\n```javascript\", {\n",
        "    suffix: \"```\"\n",
        "  }\n",
        ");\n",
        "console.log(suffixResponse);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9265343",
      "metadata": {},
      "source": [
        "如第一个示例所示，模型生成了请求的 `console.log('hello world')` 代码片段，但也包含了额外不需要的文本。通过添加后缀，我们可以限制模型仅将补全内容生成到该后缀为止（在此示例中为三个反引号）。这样，我们就可以通过自定义输出解析器轻松地解析补全内容，并仅提取所需的结果，而不包含后缀。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e2d34dc8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "console.log('hello world');\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import { MistralAI } from \"@langchain/mistralai\";\n",
        "\n",
        "const llmForFillInCompletion = new MistralAI({\n",
        "  model: \"codestral-latest\",\n",
        "  temperature: 0,\n",
        "});\n",
        "\n",
        "const suffix = \"```\";\n",
        "\n",
        "const customOutputParser = (input: string) => {\n",
        "  if (input.includes(suffix)) {\n",
        "    return input.split(suffix)[0];\n",
        "  }\n",
        "  throw new Error(\"Input does not contain suffix.\")\n",
        "};\n",
        "\n",
        "const resWithParser = await llmForFillInCompletion.invoke(\n",
        "  \"You can print 'hello world' to the console in javascript like this:\\n```javascript\", {\n",
        "    suffix,\n",
        "  }\n",
        ");\n",
        "\n",
        "console.log(customOutputParser(resWithParser));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 钩子函数\n",
        "\n",
        "Mistral AI 支持三种事件的自定义钩子函数：beforeRequest、requestError 和 response。每种钩子类型的函数签名示例如下所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "const beforeRequestHook = (req: Request): Request | void | Promise<Request | void> => {\n",
        "    // Code to run before a request is processed by Mistral\n",
        "};\n",
        "\n",
        "const requestErrorHook = (err: unknown, req: Request): void | Promise<void> => {\n",
        "    // Code to run when an error occurs as Mistral is processing a request\n",
        "};\n",
        "\n",
        "const responseHook = (res: Response, req: Request): void | Promise<void> => {\n",
        "    // Code to run before Mistral sends a successful response\n",
        "};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "将这些钩子添加到聊天模型时，可以将它们作为参数传递，它们会自动被添加："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatMistralAI } from \"@langchain/mistralai\" \n",
        "\n",
        "const modelWithHooks = new ChatMistralAI({\n",
        "    model: \"mistral-large-latest\",\n",
        "    temperature: 0,\n",
        "    maxRetries: 2,\n",
        "    beforeRequestHooks: [ beforeRequestHook ],\n",
        "    requestErrorHooks: [ requestErrorHook ],\n",
        "    responseHooks: [ responseHook ],\n",
        "    // other params...\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "或者在实例化后分配并手动添加它们："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatMistralAI } from \"@langchain/mistralai\" \n",
        "\n",
        "const model = new ChatMistralAI({\n",
        "    model: \"mistral-large-latest\",\n",
        "    temperature: 0,\n",
        "    maxRetries: 2,\n",
        "    // other params...\n",
        "});\n",
        "\n",
        "model.beforeRequestHooks = [ ...model.beforeRequestHooks, beforeRequestHook ];\n",
        "model.requestErrorHooks = [ ...model.requestErrorHooks, requestErrorHook ];\n",
        "model.responseHooks = [ ...model.responseHooks, responseHook ];\n",
        "\n",
        "model.addAllHooksToHttpClient();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "方法 addAllHooksToHttpClient 在分配完整的更新后的钩子列表之前，会清除所有当前已添加的钩子，以避免钩子重复。\n",
        "\n",
        "可以逐个移除钩子，也可以一次性从模型中清除所有钩子。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.removeHookFromHttpClient(beforeRequestHook);\n",
        "\n",
        "model.removeAllHooksFromHttpClient();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9bdfcef",
      "metadata": {},
      "source": [
        "## API 参考文档\n",
        "\n",
        "有关 MistralAI 所有功能和配置的详细文档，请访问 API 参考页面: https://api.js.langchain.com/classes/langchain_mistralai.MistralAI.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "e971737741ff4ec9aff7dc6155a1060a59a8a6d52c757dbbe66bf8ee389494b1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}