{
  "cells": [
    {
      "cell_type": "raw",
      "id": "afaf8039",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "sidebar_label: OpenAI\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49f1e0d",
      "metadata": {},
      "source": [
        "# ChatOpenAI\n",
        "\n",
        "[OpenAI](https://zh.wikipedia.org/wiki/OpenAI) 是一家人工智能 (AI) 研究实验室。\n",
        "\n",
        "本指南将帮助你开始使用 ChatOpenAI [聊天模型](/docs/concepts/chat_models)。如需了解 ChatOpenAI 所有功能和配置的详细文档，请前往 [API 参考文档](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html)。\n",
        "\n",
        "## 概述\n",
        "### 集成详情\n",
        "\n",
        "| 类别 | 包 | 本地支持 | 可序列化 | [PY 支持](https://python.langchain.com/docs/integrations/chat/openai) | 包下载量 | 最新版本 |\n",
        "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
        "| [ChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) | ❌ | ✅ | ✅ | ![NPM - 下载量](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20&) | ![NPM - 版本](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20&) |\n",
        "\n",
        "### 模型特性\n",
        "\n",
        "有关如何使用特定功能的指南，请参见下表表头中的链接。\n",
        "\n",
        "| [工具调用](/docs/how_to/tool_calling) | [结构化输出](/docs/how_to/structured_output/) | JSON 模式 | [图像输入](/docs/how_to/multimodal_inputs/) | 音频输入 | 视频输入 | [逐 token 流式传输](/docs/how_to/chat_streaming/) | [token 使用情况](/docs/how_to/chat_token_usage_tracking/) | [logprobs](/docs/how_to/logprobs/) |\n",
        "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |\n",
        "| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | \n",
        "\n",
        "## 准备工作\n",
        "\n",
        "要访问 OpenAI 的聊天模型，你需要创建一个 OpenAI 账户，获取 API 密钥，并安装 `@langchain/openai` 集成包。\n",
        "\n",
        "### 凭据\n",
        "\n",
        "访问 [OpenAI 官网](https://platform.openai.com/) 注册 OpenAI 账户并生成 API 密钥。完成此操作后，请设置 `OPENAI_API_KEY` 环境变量：\n",
        "\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"your-api-key\"\n",
        "```\n",
        "\n",
        "如果你想自动追踪模型调用，也可以取消下面的注释并设置你的 [LangSmith](https://docs.smith.langchain.com/) API 密钥：\n",
        "\n",
        "```bash\n",
        "# export LANGSMITH_TRACING=\"true\"\n",
        "# export LANGSMITH_API_KEY=\"your-api-key\"\n",
        "```\n",
        "\n",
        "### 安装\n",
        "\n",
        "LangChain 的 `ChatOpenAI` 集成位于 `@langchain/openai` 包中：\n",
        "\n",
        "```{=mdx}\n",
        "\n",
        "import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n",
        "import Npm2Yarn from \"@theme/Npm2Yarn\";\n",
        "\n",
        "<IntegrationInstallTooltip></IntegrationInstallTooltip>\n",
        "\n",
        "<Npm2Yarn>\n",
        "  @langchain/openai @langchain/core\n",
        "</Npm2Yarn>\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a38cde65-254d-4219-a441-068766c0d4b5",
      "metadata": {},
      "source": [
        "## 实例化\n",
        "\n",
        "现在我们可以实例化我们的模型对象并生成聊天补全："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\" \n",
        "\n",
        "const llm = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "  temperature: 0,\n",
        "  // other params...\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4f3e15",
      "metadata": {},
      "source": [
        "## 调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "62e0dbc3",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-ADItECqSPuuEuBHHPjeCkh9wIO1H5\",\n",
            "  \"content\": \"J'adore la programmation.\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"completionTokens\": 5,\n",
            "      \"promptTokens\": 31,\n",
            "      \"totalTokens\": 36\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"system_fingerprint\": \"fp_5796ac6771\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"input_tokens\": 31,\n",
            "    \"output_tokens\": 5,\n",
            "    \"total_tokens\": 36\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const aiMsg = await llm.invoke([\n",
        "  {\n",
        "    role: \"system\",\n",
        "    content: \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "  },\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: \"I love programming.\"\n",
        "  },\n",
        "])\n",
        "aiMsg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J'adore la programmation.\n"
          ]
        }
      ],
      "source": [
        "console.log(aiMsg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e2bfc0-7e78-4528-a73f-499ac150dca8",
      "metadata": {},
      "source": [
        "## 链式调用\n",
        "\n",
        "我们可以像这样将模型与提示模板[链式调用](/docs/how_to/sequence/)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e197d1d7-a070-4c96-9f8a-a0e86d046e0b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-ADItFaWFNqkSjSmlxeGk6HxcBHzVN\",\n",
            "  \"content\": \"Ich liebe Programmieren.\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"completionTokens\": 5,\n",
            "      \"promptTokens\": 26,\n",
            "      \"totalTokens\": 31\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"system_fingerprint\": \"fp_5796ac6771\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"input_tokens\": 26,\n",
            "    \"output_tokens\": 5,\n",
            "    \"total_tokens\": 31\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\"\n",
        "\n",
        "const prompt = ChatPromptTemplate.fromMessages(\n",
        "  [\n",
        "    [\n",
        "      \"system\",\n",
        "      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "    ],\n",
        "    [\"human\", \"{input}\"],\n",
        "  ]\n",
        ")\n",
        "\n",
        "const chain = prompt.pipe(llm);\n",
        "await chain.invoke(\n",
        "  {\n",
        "    input_language: \"English\",\n",
        "    output_language: \"German\",\n",
        "    input: \"I love programming.\",\n",
        "  }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ffc86b",
      "metadata": {},
      "source": [
        "## 自定义URL\n",
        "\n",
        "您可以通过传递一个`configuration`参数来自定义SDK发送请求的基础URL，如下所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a092b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llmWithCustomURL = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "  temperature: 0.9,\n",
        "  configuration: {\n",
        "    baseURL: \"https://your_custom_url.com\",\n",
        "  },\n",
        "});\n",
        "\n",
        "await llmWithCustomURL.invoke(\"Hi there!\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b60ccb",
      "metadata": {},
      "source": [
        "`配置`字段还接受官方SDK接受的其他`ClientOptions`参数。\n",
        "\n",
        "如果您托管在Azure OpenAI上，请查看[专用页面](/docs/integrations/chat/azure)。\n",
        "\n",
        "## 自定义请求头\n",
        "\n",
        "您可以在同一个`configuration`字段中指定自定义请求头："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd612609",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llmWithCustomHeaders = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "  temperature: 0.9,\n",
        "  configuration: {\n",
        "    defaultHeaders: {\n",
        "      \"Authorization\": `Bearer SOME_CUSTOM_VALUE`,\n",
        "    },\n",
        "  },\n",
        "});\n",
        "\n",
        "await llmWithCustomHeaders.invoke(\"Hi there!\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7af61d1d",
      "metadata": {},
      "source": [
        "## 禁用流式使用情况元数据\n",
        "\n",
        "某些代理或第三方提供商提供的 API 接口与 OpenAI 基本相同，但不支持较新添加的 `stream_options` 参数来返回流式使用情况。你可以通过如下方式禁用流式使用情况，从而使用 `ChatOpenAI` 访问这些提供商："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff40bd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llmWithoutStreamUsage = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "  temperature: 0.9,\n",
        "  streamUsage: false,\n",
        "  configuration: {\n",
        "    baseURL: \"https://proxy.com\",\n",
        "  },\n",
        "});\n",
        "\n",
        "await llmWithoutStreamUsage.invoke(\"Hi there!\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "013b6300",
      "metadata": {},
      "source": [
        "## 调用微调模型\n",
        "\n",
        "您可以通过传入相应的 `modelName` 参数来调用微调后的 OpenAI 模型。\n",
        "\n",
        "这通常采用 `ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}` 的形式。例如："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7448f6a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const fineTunedLlm = new ChatOpenAI({\n",
        "  temperature: 0.9,\n",
        "  model: \"ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}\",\n",
        "});\n",
        "\n",
        "await fineTunedLlm.invoke(\"Hi there!\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2270901",
      "metadata": {},
      "source": [
        "## 生成元数据\n",
        "\n",
        "如果您需要额外的信息，如logprobs或token使用情况，这些信息将在`.invoke`响应中的消息`response_metadata`字段内直接返回。\n",
        "\n",
        "```{=mdx}\n",
        "\n",
        ":::tip\n",
        "需要 `@langchain/core` 版本 >=0.1.48。\n",
        ":::\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2b675330",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  content: [\n",
            "    {\n",
            "      token: 'Hello',\n",
            "      logprob: -0.0004740447,\n",
            "      bytes: [ 72, 101, 108, 108, 111 ],\n",
            "      top_logprobs: []\n",
            "    },\n",
            "    {\n",
            "      token: '!',\n",
            "      logprob: -0.00004334534,\n",
            "      bytes: [ 33 ],\n",
            "      top_logprobs: []\n",
            "    },\n",
            "    {\n",
            "      token: ' How',\n",
            "      logprob: -0.000030113732,\n",
            "      bytes: [ 32, 72, 111, 119 ],\n",
            "      top_logprobs: []\n",
            "    },\n",
            "    {\n",
            "      token: ' can',\n",
            "      logprob: -0.0004797665,\n",
            "      bytes: [ 32, 99, 97, 110 ],\n",
            "      top_logprobs: []\n",
            "    },\n",
            "    {\n",
            "      token: ' I',\n",
            "      logprob: -7.89631e-7,\n",
            "      bytes: [ 32, 73 ],\n",
            "      top_logprobs: []\n",
            "    },\n",
            "    {\n",
            "      token: ' assist',\n",
            "      logprob: -0.114006,\n",
            "      bytes: [\n",
            "         32,  97, 115,\n",
            "        115, 105, 115,\n",
            "        116\n",
            "      ],\n",
            "      top_logprobs: []\n",
            "    },\n",
            "    {\n",
            "      token: ' you',\n",
            "      logprob: -4.3202e-7,\n",
            "      bytes: [ 32, 121, 111, 117 ],\n",
            "      top_logprobs: []\n",
            "    },\n",
            "    {\n",
            "      token: ' today',\n",
            "      logprob: -0.00004501419,\n",
            "      bytes: [ 32, 116, 111, 100, 97, 121 ],\n",
            "      top_logprobs: []\n",
            "    },\n",
            "    {\n",
            "      token: '?',\n",
            "      logprob: -0.000010206721,\n",
            "      bytes: [ 63 ],\n",
            "      top_logprobs: []\n",
            "    }\n",
            "  ],\n",
            "  refusal: null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "// See https://cookbook.openai.com/examples/using_logprobs for details\n",
        "const llmWithLogprobs = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "  logprobs: true,\n",
        "  // topLogprobs: 5,\n",
        "});\n",
        "\n",
        "const responseMessageWithLogprobs = await llmWithLogprobs.invoke(\"Hi there!\");\n",
        "console.dir(responseMessageWithLogprobs.response_metadata.logprobs, { depth: null });"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3",
      "metadata": {},
      "source": [
        "## 工具调用\n",
        "\n",
        "使用 OpenAI 模型进行工具调用的方式与其他模型类似，详见[其他模型](/docs/how_to/tool_calling)。此外，以下指南包含与 OpenAI 特别相关的信息：\n",
        "\n",
        "- [如何：禁用并行工具调用](/docs/how_to/tool_calling_parallel/)\n",
        "- [如何：强制调用工具](/docs/how_to/tool_choice/)\n",
        "- [如何：将模型特定的工具格式绑定到模型](/docs/how_to/tool_calling#binding-model-specific-formats-advanced)。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e6b50cc",
      "metadata": {},
      "source": [
        "### 自定义工具\n",
        "\n",
        "[自定义工具](https://platform.openai.com/docs/guides/function-calling#custom-tools) 支持使用任意字符串输入的工具。当你期望你的字符串参数较长或较复杂时，它们可能会特别有用。\n",
        "\n",
        "如果你使用的模型支持自定义工具，则可以使用 `ChatOpenAI` 类和 `customTool` 函数来创建自定义工具。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66589441",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI, customTool } from \"@langchain/openai\";\n",
        "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const codeTool = customTool(\n",
        "  async (input) => {\n",
        "    // ... Add code to execute the input\n",
        "    return \"Code executed successfully\";\n",
        "  },\n",
        "  {\n",
        "    name: \"execute_code\",\n",
        "    description: \"Execute a code snippet\",\n",
        "    format: { type: \"text\" },\n",
        "  }\n",
        ");\n",
        "\n",
        "const model = new ChatOpenAI({ model: \"gpt-5\" });\n",
        "\n",
        "const agent = createReactAgent({\n",
        "  llm: model,\n",
        "  tools: [codeTool],\n",
        "});\n",
        "\n",
        "const result = await agent.invoke(\"Use the tool to calculate 3^3\");\n",
        "console.log(result);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eba4fc5d",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>上下文无关文法</summary>\n",
        "\n",
        "OpenAI 支持为 `lark` 或 `regex` 格式的自定义工具输入指定[上下文无关文法](https://platform.openai.com/docs/guides/function-calling#context-free-grammars)。详细信息请参阅 [OpenAI 文档](https://platform.openai.com/docs/guides/function-calling#context-free-grammars)。`format` 参数可以按如下方式传入 `customTool`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0a54d72",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI, customTool } from \"@langchain/openai\";\n",
        "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const MATH_GRAMMAR = `\n",
        "start: expr\n",
        "expr: term (SP ADD SP term)* -> add\n",
        "| term\n",
        "term: factor (SP MUL SP factor)* -> mul\n",
        "| factor\n",
        "factor: INT\n",
        "SP: \\\" \\\"\n",
        "ADD: \\\"+\\\"\n",
        "MUL: \\\"*\\\"\n",
        "%import common.INT\n",
        "`;\n",
        "\n",
        "const doMath = customTool(\n",
        "  async (input) => {\n",
        "    // ... Add code to parse and execute the input\n",
        "    return \"27\";\n",
        "  },\n",
        "  {\n",
        "    name: \"do_math\",\n",
        "    description: \"Evaluate a math expression\",\n",
        "    format: { type: \"grammar\", grammar: MATH_GRAMMAR },\n",
        "  }\n",
        ");\n",
        "\n",
        "const model = new ChatOpenAI({ model: \"gpt-5\" });\n",
        "\n",
        "const agent = createReactAgent({\n",
        "  llm: model,\n",
        "  tools: [doMath],\n",
        "});\n",
        "\n",
        "const result = await agent.invoke(\"Use the tool to calculate 3^3\");\n",
        "console.log(result);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73cacf58",
      "metadata": {},
      "source": [
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3392390e",
      "metadata": {},
      "source": [
        "## ``strict: true``\n",
        "\n",
        "自2024年8月6日起，OpenAI在调用工具时支持一个`strict`参数，该参数将强制模型遵守工具参数的模式定义。更多信息请参见：https://platform.openai.com/docs/guides/function-calling。\n",
        "\n",
        "```{=mdx}\n",
        "\n",
        ":::info 需要 ``@langchain/openai >= 0.2.6``\n",
        "\n",
        "**注意**：如果设置 ``strict: true``，工具定义也将被验证，并且仅接受部分JSON Schema特性。关键的一点是，模式中不能包含可选参数（那些带有默认值的参数）。关于支持的模式类型的完整文档，请参阅：https://platform.openai.com/docs/guides/structured-outputs/supported-schemas。\n",
        ":::\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "以下是一个使用工具调用的示例。向 `.bindTools` 传递额外的 `strict: true` 参数，会将该参数传递给所有工具定义："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "90f0d465",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    name: 'get_current_weather',\n",
            "    args: { location: 'current' },\n",
            "    type: 'tool_call',\n",
            "    id: 'call_hVFyYNRwc6CoTgr9AQFQVjm9'\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const weatherTool = tool((_) => \"no-op\", {\n",
        "  name: \"get_current_weather\",\n",
        "  description: \"Get the current weather\",\n",
        "  schema: z.object({\n",
        "    location: z.string(),\n",
        "  }),\n",
        "})\n",
        "\n",
        "const llmWithStrictTrue = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "}).bindTools([weatherTool], {\n",
        "  strict: true,\n",
        "  tool_choice: weatherTool.name,\n",
        "});\n",
        "\n",
        "// Although the question is not about the weather, it will call the tool with the correct arguments\n",
        "// because we passed `tool_choice` and `strict: true`.\n",
        "const strictTrueResult = await llmWithStrictTrue.invoke(\"What is 127862 times 12898 divided by 2?\");\n",
        "\n",
        "console.dir(strictTrueResult.tool_calls, { depth: null });"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c46a668",
      "metadata": {},
      "source": [
        "如果您只想将此参数应用于部分工具，也可以直接传入 OpenAI 格式的工具模式："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e2da9ead",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    name: 'get_current_weather',\n",
            "    args: { location: 'London' },\n",
            "    type: 'tool_call',\n",
            "    id: 'call_EOSejtax8aYtqpchY8n8O82l'\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
        "\n",
        "const toolSchema = {\n",
        "  type: \"function\",\n",
        "  function: {\n",
        "    name: \"get_current_weather\",\n",
        "    description: \"Get the current weather\",\n",
        "    strict: true,\n",
        "    parameters: zodToJsonSchema(\n",
        "      z.object({\n",
        "        location: z.string(),\n",
        "      })\n",
        "    ),\n",
        "  },\n",
        "};\n",
        "\n",
        "const llmWithStrictTrueTools = new ChatOpenAI({\n",
        "  model: \"gpt-4o\",\n",
        "}).bindTools([toolSchema], {\n",
        "  strict: true,\n",
        "});\n",
        "\n",
        "const weatherToolResult = await llmWithStrictTrueTools.invoke([{\n",
        "  role: \"user\",\n",
        "  content: \"What is the current weather in London?\"\n",
        "}])\n",
        "\n",
        "weatherToolResult.tool_calls;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "045668fe",
      "metadata": {},
      "source": [
        "## 结构化输出\n",
        "\n",
        "我们也可以将 `strict: true` 传递给 [.withStructuredOutput()](https://js.langchain.com/docs/how_to/structured_output/#the-.withstructuredoutput-method) 方法。以下是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8e8171a5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ traits: [ `6'5\" tall`, 'love fruit' ] }\n"
          ]
        }
      ],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const traitSchema = z.object({\n",
        "  traits: z.array(z.string()).describe(\"A list of traits contained in the input\"),\n",
        "});\n",
        "\n",
        "const structuredLlm = new ChatOpenAI({\n",
        "  model: \"gpt-4o-mini\",\n",
        "}).withStructuredOutput(traitSchema, {\n",
        "  name: \"extract_traits\",\n",
        "  strict: true,\n",
        "});\n",
        "\n",
        "await structuredLlm.invoke([{\n",
        "  role: \"user\",\n",
        "  content: `I am 6'5\" tall and love fruit.`\n",
        "}]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7139f50e",
      "metadata": {},
      "source": [
        "## 响应API\n",
        "\n",
        ":::caution 兼容性\n",
        "\n",
        "以下内容适用于 `@langchain/openai>=0.4.5-rc.0`。有关升级指南，请点击[此处](/docs/how_to/installation/#installing-integration-packages)。\n",
        "\n",
        ":::\n",
        "\n",
        "OpenAI 提供了一个[响应](https://platform.openai.com/docs/guides/responses-vs-chat-completions) API，该 API 面向构建[代理](/docs/concepts/agents/)应用程序。它包含一套[内置工具](https://platform.openai.com/docs/guides/tools?api-mode=responses)，包括网页和文件搜索。它还支持[对话状态](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses)管理，允许您继续对话线程，而无需显式传递之前的消息。\n",
        "\n",
        "如果使用了这些功能之一，`ChatOpenAI` 将会路由到响应 API。在实例化 `ChatOpenAI` 时，您也可以指定 `useResponsesApi: true`。\n",
        "\n",
        "### 内置工具\n",
        "\n",
        "为 `ChatOpenAI` 装备内置工具可以使其回答基于外部信息，例如通过文件或网页中的上下文。模型生成的 [AIMessage](/docs/concepts/messages/#aimessage) 将包括有关内置工具调用的信息。\n",
        "\n",
        "#### 网页搜索\n",
        "\n",
        "要触发网页搜索，请像使用其他工具一样，将 `{\"type\": \"web_search_preview\"}` 传递给模型。\n",
        "\n",
        ":::tip\n",
        "\n",
        "您也可以将内置工具作为调用参数传入：\n",
        "\n",
        "```ts\n",
        "llm.invoke(\"...\", { tools: [{ type: \"web_search_preview\" }] });\n",
        "```\n",
        "\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b01f09a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llm = new ChatOpenAI({ model: \"gpt-4o-mini\" }).bindTools([\n",
        "  { type: \"web_search_preview\" },\n",
        "]);\n",
        "\n",
        "await llm.invoke(\"What was a positive news story from today?\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e62bae97",
      "metadata": {},
      "source": [
        "请注意，响应包含结构化的[内容块](/docs/concepts/messages/#content-1)，其中既有响应文本，也有来自OpenAI的[注释](https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses#output-and-citations)，用于引用来源。输出消息还将包含来自任何工具调用的信息。\n",
        "\n",
        "#### 文件搜索\n",
        "\n",
        "要触发文件搜索，请像使用其他工具一样，将一个[文件搜索工具](https://platform.openai.com/docs/guides/tools-file-search)传递给模型。你需要填充一个由OpenAI管理的向量存储，并在工具定义中包含该向量存储的ID。更多详细信息请参见[OpenAI文档](https://platform.openai.com/docs/guides/tools-file-search)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70c671c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llm = new ChatOpenAI({ model: \"gpt-4o-mini\" }).bindTools([\n",
        "  { type: \"file_search\", vector_store_ids: [\"vs...\"] },\n",
        "]);\n",
        "\n",
        "await llm.invoke(\"Is deep research by OpenAI?\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a8378c",
      "metadata": {},
      "source": [
        "与[网页搜索](#web-search)一样，响应将包含带有引用内容的区块。它还将包含来自内置工具调用的信息。\n",
        "\n",
        "#### 计算机使用\n",
        "\n",
        "ChatOpenAI 支持 `computer-use-preview` 模型，这是一个专为内置计算机使用工具设计的模型。要启用该功能，请像使用其他工具一样传递一个[计算机使用工具](https://platform.openai.com/docs/guides/tools-computer-use)。\n",
        "\n",
        "目前，计算机使用的工具输出位于 `AIMessage.additional_kwargs.tool_outputs` 中。要回复计算机使用工具调用，您需要在创建相应的 `ToolMessage` 时设置 `additional_kwargs.type: \"computer_call_output\"`。\n",
        "\n",
        "更多详细信息请参见[OpenAI 文档](https://platform.openai.com/docs/guides/tools-computer-use)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176eb3ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { AIMessage, ToolMessage } from \"@langchain/core/messages\";\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "import * as fs from \"node:fs/promises\";\n",
        "\n",
        "const findComputerCall = (message: AIMessage) => {\n",
        "  const toolOutputs = message.additional_kwargs.tool_outputs as\n",
        "    | { type: \"computer_call\"; call_id: string; action: { type: string } }[]\n",
        "    | undefined;\n",
        "\n",
        "  return toolOutputs?.find((toolOutput) => toolOutput.type === \"computer_call\");\n",
        "};\n",
        "\n",
        "const llm = new ChatOpenAI({ model: \"computer-use-preview\" })\n",
        "  .bindTools([\n",
        "    {\n",
        "      type: \"computer-preview\",\n",
        "      display_width: 1024,\n",
        "      display_height: 768,\n",
        "      environment: \"browser\",\n",
        "    },\n",
        "  ])\n",
        "  .bind({ truncation: \"auto\" });\n",
        "\n",
        "let message = await llm.invoke(\"Check the latest OpenAI news on bing.com.\");\n",
        "const computerCall = findComputerCall(message);\n",
        "\n",
        "if (computerCall) {\n",
        "  // Act on a computer call action\n",
        "  const screenshot = await fs.readFile(\"./screenshot.png\", {\n",
        "    encoding: \"base64\",\n",
        "  });\n",
        "\n",
        "  message = await llm.invoke(\n",
        "    [\n",
        "      new ToolMessage({\n",
        "        additional_kwargs: { type: \"computer_call_output\" },\n",
        "        tool_call_id: computerCall.call_id,\n",
        "        content: [\n",
        "          {\n",
        "            type: \"computer_screenshot\",\n",
        "            image_url: `data:image/png;base64,${screenshot}`,\n",
        "          },\n",
        "        ],\n",
        "      }),\n",
        "    ],\n",
        "    { previous_response_id: message.response_metadata[\"id\"] }\n",
        "  );\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2918529b",
      "metadata": {},
      "source": [
        "#### 代码解释器\n",
        "\n",
        "ChatOpenAI 允许你使用内置的 [代码解释器工具](https://platform.openai.com/docs/guides/tools-code-interpreter)，以支持沙盒化的代码生成与执行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9185748",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llm = new ChatOpenAI({\n",
        "  model: \"o4-mini\",\n",
        "  useResponsesApi: true,\n",
        "});\n",
        "\n",
        "const llmWithTools = llm.bindToools([\n",
        "  {\n",
        "    type: \"code_interpreter\",\n",
        "    // Creates a new container\n",
        "    container: { type: \"auto\" }\n",
        "  },\n",
        "]);\n",
        "\n",
        "const response = await llmWithTools.invoke(\n",
        "  \"Write and run code to answer the question: what is 3^3?\"\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1771a7ab",
      "metadata": {},
      "source": [
        "请注意，上述命令会创建一个新的[容器](https://platform.openai.com/docs/guides/tools-code-interpreter#containers)。我们可以通过指定现有容器ID在多次调用中重复使用容器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eeea048",
      "metadata": {},
      "outputs": [],
      "source": [
        "const tool_outputs: Record<string, any>[] = response.additional_kwargs.tool_outputs\n",
        "const container_id = tool_outputs[0].container_id\n",
        "\n",
        "const llmWithTools = llm.bindTools([\n",
        "  {\n",
        "    type: \"code_interpreter\",\n",
        "    // Re-uses container from the last call\n",
        "    container: container_id,\n",
        "  },\n",
        "]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e8f980b",
      "metadata": {},
      "source": [
        "#### 远程 MCP\n",
        "\n",
        "ChatOpenAI 支持内置的 [远程 MCP 工具](https://platform.openai.com/docs/guides/tools-remote-mcp)，该工具允许在 OpenAI 服务器上执行模型生成的对 MCP 服务器的调用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a2fe41",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llm = new ChatOpenAI({\n",
        "    model: \"o4-mini\",\n",
        "    useResponsesApi: true,\n",
        "});\n",
        "\n",
        "const llmWithMcp = llm.bindTools([\n",
        "    {\n",
        "        type: \"mcp\",\n",
        "        server_label: \"deepwiki\",\n",
        "        server_url: \"https://mcp.deepwiki.com/mcp\",\n",
        "        require_approval: \"never\"\n",
        "    }\n",
        "]);\n",
        "\n",
        "const response = await llmWithMcp.invoke(\n",
        "    \"What transport protocols does the 2025-03-26 version of the MCP spec (modelcontextprotocol/modelcontextprotocol) support?\"\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7bb9788",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "<details>\n",
        "<summary>MCP 批准</summary>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "收到指令后，OpenAI将在调用远程MCP服务器之前请求批准。\n",
        "\n",
        "在上面的命令中，我们指示模型不需要任何批准。我们也可以将模型配置为始终请求批准，或对特定工具始终请求批准：\n",
        "\n",
        "```typescript\n",
        "...\n",
        "const llmWithMcp = llm.bindTools([\n",
        "  {\n",
        "    type: \"mcp\",\n",
        "    server_label: \"deepwiki\",\n",
        "    server_url: \"https://mcp.deepwiki.com/mcp\",\n",
        "    require_approval: {\n",
        "      always: {\n",
        "        tool_names: [\"read_wiki_structure\"],\n",
        "      },\n",
        "    },\n",
        "  },\n",
        "]);\n",
        "const response = await llmWithMcp.invoke(\n",
        "    \"MCP规范的2025-03-26版本（modelcontextprotocol/modelcontextprotocol）支持哪些传输协议？\"\n",
        ");\n",
        "```\n",
        "\n",
        "通过此配置，响应可以包含类型为`mcp_approval_request`的工具输出。要为审批请求提交批准，您可以将其结构化为后续消息中的一个内容块：\n",
        "\n",
        "```typescript\n",
        "const approvals = [];\n",
        "if (Array.isArray(response.additional_kwargs.tool_outputs)) {\n",
        "  for (const content of response.additional_kwargs.tool_outputs) {\n",
        "    if (content.type === \"mcp_approval_request\") {\n",
        "      approvals.push({\n",
        "        type: \"mcp_approval_response\",\n",
        "        approval_request_id: content.id,\n",
        "        approve: true,\n",
        "      });\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "const nextResponse = await model.invoke(\n",
        "  [\n",
        "    response,\n",
        "    new HumanMessage({ content: approvals }),\n",
        "  ],\n",
        ");\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d0a44ae",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "</details>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8695925",
      "metadata": {},
      "source": [
        "#### 图像生成\n",
        "\n",
        "ChatOpenAI 允许你使用内置的[图像生成工具](https://platform.openai.com/docs/guides/tools-image-generation)，通过响应 API 在多轮对话中生成图像。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ff1906",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const llm = new ChatOpenAI({\n",
        "  model: \"gpt-4.1\",\n",
        "  useResponsesApi: true,\n",
        "});\n",
        "\n",
        "const llmWithImageGeneration = llm.bindTools([\n",
        "  {\n",
        "    type: \"image_generation\",\n",
        "    quality: \"low\",\n",
        "  }\n",
        "]);\n",
        "\n",
        "const response = await llmWithImageGeneration.invoke(\n",
        "  \"Draw a random short word in green font.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0194ec1f",
      "metadata": {},
      "source": [
        "### 推理模型\n",
        "\n",
        "```{=mdx}\n",
        ":::caution 兼容性\n",
        "\n",
        "以下内容适用于 `@langchain/openai>=0.4.0`。有关升级的指南，请参见[此处](/docs/how_to/installation/#installing-integration-packages)。\n",
        "\n",
        ":::\n",
        "```\n",
        "\n",
        "当使用如 `o1` 这类推理模型时，`withStructuredOutput` 的默认方法是 OpenAI 内置的结构化输出方法（等同于在 `withStructuredOutput` 中传递 `method: \"jsonSchema\"` 选项）。JSON Schema 的使用与其他模型大致相同，但有一个重要注意事项：在定义 Schema 时，`z.optional()` 不会被识别，此时应使用 `z.nullable()`。\n",
        "\n",
        "以下是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d2a04807",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ color: 'No color mentioned' }\n"
          ]
        }
      ],
      "source": [
        "import { z } from \"zod\";\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "// Will not work\n",
        "const reasoningModelSchemaOptional = z.object({\n",
        "  color: z.optional(z.string()).describe(\"A color mentioned in the input\"),\n",
        "});\n",
        "\n",
        "const reasoningModelOptionalSchema = new ChatOpenAI({\n",
        "  model: \"o1\",\n",
        "}).withStructuredOutput(reasoningModelSchemaOptional, {\n",
        "  name: \"extract_color\",\n",
        "});\n",
        "\n",
        "await reasoningModelOptionalSchema.invoke([{\n",
        "  role: \"user\",\n",
        "  content: `I am 6'5\" tall and love fruit.`\n",
        "}]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69854ed4",
      "metadata": {},
      "source": [
        "这里是一个使用 `z.nullable()` 的示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5f4bb1bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ color: null }\n"
          ]
        }
      ],
      "source": [
        "import { z } from \"zod\";\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "// Will not work\n",
        "const reasoningModelSchemaNullable = z.object({\n",
        "  color: z.nullable(z.string()).describe(\"A color mentioned in the input\"),\n",
        "});\n",
        "\n",
        "const reasoningModelNullableSchema = new ChatOpenAI({\n",
        "  model: \"o1\",\n",
        "}).withStructuredOutput(reasoningModelSchemaNullable, {\n",
        "  name: \"extract_color\",\n",
        "});\n",
        "\n",
        "await reasoningModelNullableSchema.invoke([{\n",
        "  role: \"user\",\n",
        "  content: `I am 6'5\" tall and love fruit.`\n",
        "}]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af20e756",
      "metadata": {},
      "source": [
        "## 提示缓存\n",
        "\n",
        "较新的OpenAI模型会自动[缓存部分提示内容](https://openai.com/index/api-prompt-caching/)，如果您的输入超过一定大小（撰写本文时为1024个token），则会为需要长上下文的使用场景降低成本。\n",
        "\n",
        "**注意：** 每次查询缓存的token数量在`AIMessage.usage_metadata`中尚未标准化，而是包含在`AIMessage.response_metadata`字段中。\n",
        "\n",
        "以下是一个示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cb4e4fd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "\n",
        "const CACHED_TEXT = `## Components\n",
        "\n",
        "LangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.\n",
        "Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\n",
        "\n",
        "### Chat models\n",
        "\n",
        "<span data-heading-keywords=\"chat model,chat models\"></span>\n",
        "\n",
        "Language models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).\n",
        "These are generally newer models (older models are generally \\`LLMs\\`, see below).\n",
        "Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.\n",
        "\n",
        "Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input.\n",
        "This gives them the same interface as LLMs (and simpler to use).\n",
        "When a string is passed in as input, it will be converted to a \\`HumanMessage\\` under the hood before being passed to the underlying model.\n",
        "\n",
        "LangChain does not host any Chat Models, rather we rely on third party integrations.\n",
        "\n",
        "We have some standardized parameters when constructing ChatModels:\n",
        "\n",
        "- \\`model\\`: the name of the model\n",
        "\n",
        "Chat Models also accept other parameters that are specific to that integration.\n",
        "\n",
        ":::important\n",
        "Some chat models have been fine-tuned for **tool calling** and provide a dedicated API for it.\n",
        "Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.\n",
        "Please see the [tool calling section](/docs/concepts/tool_calling) for more information.\n",
        ":::\n",
        "\n",
        "For specifics on how to use chat models, see the [relevant how-to guides here](/docs/how_to/#chat-models).\n",
        "\n",
        "#### Multimodality\n",
        "\n",
        "Some chat models are multimodal, accepting images, audio and even video as inputs.\n",
        "These are still less common, meaning model providers haven't standardized on the \"best\" way to define the API.\n",
        "Multimodal outputs are even less common. As such, we've kept our multimodal abstractions fairly light weight\n",
        "and plan to further solidify the multimodal APIs and interaction patterns as the field matures.\n",
        "\n",
        "In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format.\n",
        "So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.\n",
        "\n",
        "For specifics on how to use multimodal models, see the [relevant how-to guides here](/docs/how_to/#multimodal).\n",
        "\n",
        "### LLMs\n",
        "\n",
        "<span data-heading-keywords=\"llm,llms\"></span>\n",
        "\n",
        ":::caution\n",
        "Pure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as [chat completion models](/docs/concepts/chat_models),\n",
        "even for non-chat use cases.\n",
        "\n",
        "You are probably looking for [the section above instead](/docs/concepts/chat_models).\n",
        ":::\n",
        "\n",
        "Language models that takes a string as input and returns a string.\n",
        "These are traditionally older models (newer models generally are [Chat Models](/docs/concepts/chat_models), see above).\n",
        "\n",
        "Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.\n",
        "This gives them the same interface as [Chat Models](/docs/concepts/chat_models).\n",
        "When messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.\n",
        "\n",
        "LangChain does not host any LLMs, rather we rely on third party integrations.\n",
        "\n",
        "For specifics on how to use LLMs, see the [relevant how-to guides here](/docs/how_to/#llms).\n",
        "\n",
        "### Message types\n",
        "\n",
        "Some language models take an array of messages as input and return a message.\n",
        "There are a few different types of messages.\n",
        "All messages have a \\`role\\`, \\`content\\`, and \\`response_metadata\\` property.\n",
        "\n",
        "The \\`role\\` describes WHO is saying the message.\n",
        "LangChain has different message classes for different roles.\n",
        "\n",
        "The \\`content\\` property describes the content of the message.\n",
        "This can be a few different things:\n",
        "\n",
        "- A string (most models deal this type of content)\n",
        "- A List of objects (this is used for multi-modal input, where the object contains information about that input type and that input location)\n",
        "\n",
        "#### HumanMessage\n",
        "\n",
        "This represents a message from the user.\n",
        "\n",
        "#### AIMessage\n",
        "\n",
        "This represents a message from the model. In addition to the \\`content\\` property, these messages also have:\n",
        "\n",
        "**\\`response_metadata\\`**\n",
        "\n",
        "The \\`response_metadata\\` property contains additional metadata about the response. The data here is often specific to each model provider.\n",
        "This is where information like log-probs and token usage may be stored.\n",
        "\n",
        "**\\`tool_calls\\`**\n",
        "\n",
        "These represent a decision from an language model to call a tool. They are included as part of an \\`AIMessage\\` output.\n",
        "They can be accessed from there with the \\`.tool_calls\\` property.\n",
        "\n",
        "This property returns a list of \\`ToolCall\\`s. A \\`ToolCall\\` is an object with the following arguments:\n",
        "\n",
        "- \\`name\\`: The name of the tool that should be called.\n",
        "- \\`args\\`: The arguments to that tool.\n",
        "- \\`id\\`: The id of that tool call.\n",
        "\n",
        "#### SystemMessage\n",
        "\n",
        "This represents a system message, which tells the model how to behave. Not every model provider supports this.\n",
        "\n",
        "#### ToolMessage\n",
        "\n",
        "This represents the result of a tool call. In addition to \\`role\\` and \\`content\\`, this message has:\n",
        "\n",
        "- a \\`tool_call_id\\` field which conveys the id of the call to the tool that was called to produce this result.\n",
        "- an \\`artifact\\` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\n",
        "\n",
        "#### (Legacy) FunctionMessage\n",
        "\n",
        "This is a legacy message type, corresponding to OpenAI's legacy function-calling API. \\`ToolMessage\\` should be used instead to correspond to the updated tool-calling API.\n",
        "\n",
        "This represents the result of a function call. In addition to \\`role\\` and \\`content\\`, this message has a \\`name\\` parameter which conveys the name of the function that was called to produce this result.\n",
        "\n",
        "### Prompt templates\n",
        "\n",
        "<span data-heading-keywords=\"prompt,prompttemplate,chatprompttemplate\"></span>\n",
        "\n",
        "Prompt templates help to translate user input and parameters into instructions for a language model.\n",
        "This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
        "\n",
        "Prompt Templates take as input an object, where each key represents a variable in the prompt template to fill in.\n",
        "\n",
        "Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or an array of messages.\n",
        "The reason this PromptValue exists is to make it easy to switch between strings and messages.\n",
        "\n",
        "There are a few different types of prompt templates:\n",
        "\n",
        "#### String PromptTemplates\n",
        "\n",
        "These prompt templates are used to format a single string, and generally are used for simpler inputs.\n",
        "For example, a common way to construct and use a PromptTemplate is as follows:\n",
        "\n",
        "\\`\\`\\`typescript\n",
        "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "const promptTemplate = PromptTemplate.fromTemplate(\n",
        "  \"Tell me a joke about {topic}\"\n",
        ");\n",
        "\n",
        "await promptTemplate.invoke({ topic: \"cats\" });\n",
        "\\`\\`\\`\n",
        "\n",
        "#### ChatPromptTemplates\n",
        "\n",
        "These prompt templates are used to format an array of messages. These \"templates\" consist of an array of templates themselves.\n",
        "For example, a common way to construct and use a ChatPromptTemplate is as follows:\n",
        "\n",
        "\\`\\`\\`typescript\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "const promptTemplate = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", \"You are a helpful assistant\"],\n",
        "  [\"user\", \"Tell me a joke about {topic}\"],\n",
        "]);\n",
        "\n",
        "await promptTemplate.invoke({ topic: \"cats\" });\n",
        "\\`\\`\\`\n",
        "\n",
        "In the above example, this ChatPromptTemplate will construct two messages when called.\n",
        "The first is a system message, that has no variables to format.\n",
        "The second is a HumanMessage, and will be formatted by the \\`topic\\` variable the user passes in.\n",
        "\n",
        "#### MessagesPlaceholder\n",
        "\n",
        "<span data-heading-keywords=\"messagesplaceholder\"></span>\n",
        "\n",
        "This prompt template is responsible for adding an array of messages in a particular place.\n",
        "In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\n",
        "But what if we wanted the user to pass in an array of messages that we would slot into a particular spot?\n",
        "This is how you use MessagesPlaceholder.\n",
        "\n",
        "\\`\\`\\`typescript\n",
        "import {\n",
        "  ChatPromptTemplate,\n",
        "  MessagesPlaceholder,\n",
        "} from \"@langchain/core/prompts\";\n",
        "import { HumanMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const promptTemplate = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", \"You are a helpful assistant\"],\n",
        "  new MessagesPlaceholder(\"msgs\"),\n",
        "]);\n",
        "\n",
        "promptTemplate.invoke({ msgs: [new HumanMessage({ content: \"hi!\" })] });\n",
        "\\`\\`\\`\n",
        "\n",
        "This will produce an array of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\n",
        "If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\n",
        "This is useful for letting an array of messages be slotted into a particular spot.\n",
        "\n",
        "An alternative way to accomplish the same thing without using the \\`MessagesPlaceholder\\` class explicitly is:\n",
        "\n",
        "\\`\\`\\`typescript\n",
        "const promptTemplate = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", \"You are a helpful assistant\"],\n",
        "  [\"placeholder\", \"{msgs}\"], // <-- This is the changed part\n",
        "]);\n",
        "\\`\\`\\`\n",
        "\n",
        "For specifics on how to use prompt templates, see the [relevant how-to guides here](/docs/how_to/#prompt-templates).\n",
        "\n",
        "### Example Selectors\n",
        "\n",
        "One common prompting technique for achieving better performance is to include examples as part of the prompt.\n",
        "This gives the language model concrete examples of how it should behave.\n",
        "Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\n",
        "Example Selectors are classes responsible for selecting and then formatting examples into prompts.\n",
        "\n",
        "For specifics on how to use example selectors, see the [relevant how-to guides here](/docs/how_to/#example-selectors).\n",
        "\n",
        "### Output parsers\n",
        "\n",
        "<span data-heading-keywords=\"output parser\"></span>\n",
        "\n",
        ":::note\n",
        "\n",
        "The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\n",
        "More and more models are supporting function (or tool) calling, which handles this automatically.\n",
        "It is recommended to use function/tool calling rather than output parsing.\n",
        "See documentation for that [here](/docs/concepts/tool_calling).\n",
        "\n",
        ":::\n",
        "\n",
        "Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\n",
        "Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\n",
        "\n",
        "There are two main methods an output parser must implement:\n",
        "\n",
        "- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
        "- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
        "\n",
        "And then one optional one:\n",
        "\n",
        "- \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n",
        "\n",
        "Output parsers accept a string or \\`BaseMessage\\` as input and can return an arbitrary type.\n",
        "\n",
        "LangChain has many different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:\n",
        "\n",
        "**Name**: The name of the output parser\n",
        "\n",
        "**Supports Streaming**: Whether the output parser supports streaming.\n",
        "\n",
        "**Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific arguments.\n",
        "\n",
        "**Output Type**: The output type of the object returned by the parser.\n",
        "\n",
        "**Description**: Our commentary on this output parser and when to use it.\n",
        "\n",
        "The current date is ${new Date().toISOString()}`;\n",
        "\n",
        "// Noop statement to hide output\n",
        "void 0;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7a43595c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USAGE: {\n",
            "  prompt_tokens: 2624,\n",
            "  completion_tokens: 263,\n",
            "  total_tokens: 2887,\n",
            "  prompt_tokens_details: { cached_tokens: 0 },\n",
            "  completion_tokens_details: { reasoning_tokens: 0 }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const modelWithCaching = new ChatOpenAI({\n",
        "  model: \"gpt-4o-mini-2024-07-18\",\n",
        "});\n",
        "\n",
        "// CACHED_TEXT is some string longer than 1024 tokens\n",
        "const LONG_TEXT = `You are a pirate. Always respond in pirate dialect.\n",
        "\n",
        "Use the following as context when answering questions:\n",
        "\n",
        "${CACHED_TEXT}`;\n",
        "\n",
        "const longMessages = [\n",
        "  {\n",
        "    role: \"system\",\n",
        "    content: LONG_TEXT,\n",
        "  },\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: \"What types of messages are supported in LangChain?\",\n",
        "  },\n",
        "];\n",
        "\n",
        "const originalRes = await modelWithCaching.invoke(longMessages);\n",
        "\n",
        "console.log(\"USAGE:\", originalRes.response_metadata.usage);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "76c8005e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USAGE: {\n",
            "  prompt_tokens: 2624,\n",
            "  completion_tokens: 272,\n",
            "  total_tokens: 2896,\n",
            "  prompt_tokens_details: { cached_tokens: 2432 },\n",
            "  completion_tokens_details: { reasoning_tokens: 0 }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const resWitCaching = await modelWithCaching.invoke(longMessages);\n",
        "\n",
        "console.log(\"USAGE:\", resWitCaching.response_metadata.usage);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f755a0b3",
      "metadata": {},
      "source": [
        "## 预测输出\n",
        "\n",
        "某些OpenAI模型（例如其`gpt-4o`和`gpt-4o-mini`系列）支持[预测输出](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs)，这允许您提前传入LLM预期输出中已知的部分，以减少延迟。这在编辑文本或代码等场景中非常有用，因为模型输出中只有一小部分会发生变化。\n",
        "\n",
        "以下是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4d5a5582",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AQLyQKnazr7lEV7ejLTo1UqhzHDBl\",\n",
            "  \"content\": \"/// <summary>\\n/// Represents a user with a first name, last name, and email.\\n/// </summary>\\npublic class User\\n{\\n/// <summary>\\n/// Gets or sets the user's first name.\\n/// </summary>\\npublic string FirstName { get; set; }\\n\\n/// <summary>\\n/// Gets or sets the user's last name.\\n/// </summary>\\npublic string LastName { get; set; }\\n\\n/// <summary>\\n/// Gets or sets the user's email.\\n/// </summary>\\npublic string Email { get; set; }\\n}\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"promptTokens\": 148,\n",
            "      \"completionTokens\": 217,\n",
            "      \"totalTokens\": 365\n",
            "    },\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"usage\": {\n",
            "      \"prompt_tokens\": 148,\n",
            "      \"completion_tokens\": 217,\n",
            "      \"total_tokens\": 365,\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"cached_tokens\": 0\n",
            "      },\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"accepted_prediction_tokens\": 36,\n",
            "        \"rejected_prediction_tokens\": 116\n",
            "      }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_0ba0d124f1\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"output_tokens\": 217,\n",
            "    \"input_tokens\": 148,\n",
            "    \"total_tokens\": 365,\n",
            "    \"input_token_details\": {\n",
            "      \"cache_read\": 0\n",
            "    },\n",
            "    \"output_token_details\": {\n",
            "      \"reasoning\": 0\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const modelWithPredictions = new ChatOpenAI({\n",
        "  model: \"gpt-4o-mini\",\n",
        "});\n",
        "\n",
        "const codeSample = `\n",
        "/// <summary>\n",
        "/// Represents a user with a first name, last name, and username.\n",
        "/// </summary>\n",
        "public class User\n",
        "{\n",
        "/// <summary>\n",
        "/// Gets or sets the user's first name.\n",
        "/// </summary>\n",
        "public string FirstName { get; set; }\n",
        "\n",
        "/// <summary>\n",
        "/// Gets or sets the user's last name.\n",
        "/// </summary>\n",
        "public string LastName { get; set; }\n",
        "\n",
        "/// <summary>\n",
        "/// Gets or sets the user's username.\n",
        "/// </summary>\n",
        "public string Username { get; set; }\n",
        "}\n",
        "`;\n",
        "\n",
        "// Can also be attached ahead of time\n",
        "// using `model.bind({ prediction: {...} })`;\n",
        "await modelWithPredictions.invoke(\n",
        "  [\n",
        "    {\n",
        "      role: \"user\",\n",
        "      content:\n",
        "        \"Replace the Username property with an Email property. Respond only with code, and with no markdown formatting.\",\n",
        "    },\n",
        "    {\n",
        "      role: \"user\",\n",
        "      content: codeSample,\n",
        "    },\n",
        "  ],\n",
        "  {\n",
        "    prediction: {\n",
        "      type: \"content\",\n",
        "      content: codeSample,\n",
        "    },\n",
        "  }\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f901e4",
      "metadata": {},
      "source": [
        "请注意，目前预测功能将作为额外的令牌进行计费，这会增加您的使用量和成本，但换来了更低的延迟。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8b3c94",
      "metadata": {},
      "source": [
        "## 音频输出\n",
        "\n",
        "某些OpenAI模型（例如 `gpt-4o-audio-preview`）支持生成音频输出。此示例展示了如何使用该功能："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b4d579b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  id: 'audio_67129e9466f48190be70372922464162',\n",
            "  data: 'UklGRgZ4BABXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGHA',\n",
            "  expires_at: 1729277092,\n",
            "  transcript: \"Why did the cat sit on the computer's keyboard? Because it wanted to keep an eye on the mouse!\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const modelWithAudioOutput = new ChatOpenAI({\n",
        "  model: \"gpt-4o-audio-preview\",\n",
        "  // You may also pass these fields to `.bind` as a call argument.\n",
        "  modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n",
        "  audio: {\n",
        "    voice: \"alloy\",\n",
        "    format: \"wav\",\n",
        "  },\n",
        "});\n",
        "\n",
        "const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n",
        "const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;\n",
        "\n",
        "console.log({\n",
        "  ...castAudioContent,\n",
        "  data: castAudioContent.data.slice(0, 100) // Sliced for brevity\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfea3608",
      "metadata": {},
      "source": [
        "我们看到音频数据是在 `data` 字段内返回的。同时，还提供了一个 `expires_at` 日期字段。该字段表示音频响应在服务器上不再可用于多轮对话的日期。\n",
        "\n",
        "### 流式音频输出\n",
        "\n",
        "OpenAI 也支持流式音频输出。以下是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0fa68183",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  id: 'audio_67129e976ce081908103ba4947399a3eaudio_67129e976ce081908103ba4947399a3e',\n",
            "  transcript: 'Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!',\n",
            "  index: 0,\n",
            "  data: 'CgAGAAIADAAAAA0AAwAJAAcACQAJAAQABQABAAgABQAPAAAACAADAAUAAwD8/wUA+f8MAPv/CAD7/wUA///8/wUA/f8DAPj/AgD6',\n",
            "  expires_at: 1729277096\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { AIMessageChunk } from \"@langchain/core/messages\";\n",
        "import { concat } from \"@langchain/core/utils/stream\"\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const modelWithStreamingAudioOutput = new ChatOpenAI({\n",
        "  model: \"gpt-4o-audio-preview\",\n",
        "  modalities: [\"text\", \"audio\"],\n",
        "  audio: {\n",
        "    voice: \"alloy\",\n",
        "    format: \"pcm16\", // Format must be `pcm16` for streaming\n",
        "  },\n",
        "});\n",
        "\n",
        "const audioOutputStream = await modelWithStreamingAudioOutput.stream(\"Tell me a joke about cats.\");\n",
        "let finalAudioOutputMsg: AIMessageChunk | undefined;\n",
        "for await (const chunk of audioOutputStream) {\n",
        "  finalAudioOutputMsg = finalAudioOutputMsg ? concat(finalAudioOutputMsg, chunk) : chunk;\n",
        "}\n",
        "const castStreamedAudioContent = finalAudioOutputMsg?.additional_kwargs.audio as Record<string, any>;\n",
        "\n",
        "console.log({\n",
        "  ...castStreamedAudioContent,\n",
        "  data: castStreamedAudioContent.data.slice(0, 100) // Sliced for brevity\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b84aac",
      "metadata": {},
      "source": [
        "### 音频输入\n",
        "\n",
        "这些模型也支持将音频作为输入传递。为此，你必须指定如下的 `input_audio` 字段："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1a69dad8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "That's a great joke! It's always fun to imagine why cats do the funny things they do. Keeping an eye on the \"mouse\" is a creatively punny way to describe it!\n"
          ]
        }
      ],
      "source": [
        "import { HumanMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const userInput = new HumanMessage({\n",
        "  content: [{\n",
        "    type: \"input_audio\",\n",
        "    input_audio: {\n",
        "      data: castAudioContent.data, // Re-use the base64 data from the first example\n",
        "      format: \"wav\",\n",
        "    },\n",
        "  }]\n",
        "})\n",
        "\n",
        "// Re-use the same model instance\n",
        "const userInputAudioRes = await modelWithAudioOutput.invoke([userInput]);\n",
        "\n",
        "console.log((userInputAudioRes.additional_kwargs.audio as Record<string, any>).transcript);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3",
      "metadata": {},
      "source": [
        "## API 参考文档\n",
        "\n",
        "有关所有 ChatOpenAI 功能和配置的详细文档，请访问 API 参考页面：https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}