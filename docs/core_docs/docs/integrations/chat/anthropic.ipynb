{
  "cells": [
    {
      "cell_type": "raw",
      "id": "afaf8039",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "sidebar_label: Anthropic\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49f1e0d",
      "metadata": {},
      "source": [
        "# ChatAnthropic\n",
        "\n",
        "[Anthropic](https://www.anthropic.com/) 是一家人工智能安全和研究公司，也是 Claude 的创建者。\n",
        "\n",
        "这将帮助你快速开始使用 Anthropic 的[聊天模型](/docs/concepts/chat_models)。如需关于所有 `ChatAnthropic` 功能和配置的详细文档，请前往 [API 参考](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html)。\n",
        "\n",
        "## 概览\n",
        "### 集成详情\n",
        "\n",
        "| 类别 | 包 | 本地支持 | 可序列化 | [Python 支持](https://python.langchain.com/docs/integrations/chat/anthropic/) | 包下载量 | 最新包版本 |\n",
        "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
        "| [ChatAnthropic](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html) | [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic) | ❌ | ✅ | ✅ | ![NPM - 下载量](https://img.shields.io/npm/dm/@langchain/anthropic?style=flat-square&label=%20&) | ![NPM - 版本](https://img.shields.io/npm/v/@langchain/anthropic?style=flat-square&label=%20&) |\n",
        "\n",
        "### 模型功能\n",
        "\n",
        "有关如何使用特定功能的指南，请参见下表标题中的链接。\n",
        "\n",
        "| [工具调用](/docs/how_to/tool_calling) | [结构化输出](/docs/how_to/structured_output/) | JSON 模式 | [图像输入](/docs/how_to/multimodal_inputs/) | 音频输入 | 视频输入 | [逐令牌流式传输](/docs/how_to/chat_streaming/) | [令牌使用情况](/docs/how_to/chat_token_usage_tracking/) | [对数概率](/docs/how_to/logprobs/) |\n",
        "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |\n",
        "| ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | \n",
        "\n",
        "## 安装准备\n",
        "\n",
        "你需要注册并获取 [Anthropic API 密钥](https://www.anthropic.com/)，并安装 `@langchain/anthropic` 集成包。\n",
        "\n",
        "### 凭证信息\n",
        "\n",
        "前往 [Anthropic 网站](https://www.anthropic.com/) 注册 Anthropic 账号并生成 API 密钥。完成之后，请设置 `ANTHROPIC_API_KEY` 环境变量：\n",
        "\n",
        "```bash\n",
        "export ANTHROPIC_API_KEY=\"your-api-key\"\n",
        "```\n",
        "\n",
        "如果你想获得模型调用的自动追踪功能，也可以取消以下代码的注释并设置你的 [LangSmith](https://docs.smith.langchain.com/) API 密钥：\n",
        "\n",
        "```bash\n",
        "# export LANGSMITH_TRACING=\"true\"\n",
        "# export LANGSMITH_API_KEY=\"your-api-key\"\n",
        "```\n",
        "\n",
        "### 安装\n",
        "\n",
        "LangChain 的 `ChatAnthropic` 集成位于 `@langchain/anthropic` 包中：\n",
        "\n",
        "```{=mdx}\n",
        "\n",
        "import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n",
        "import Npm2Yarn from \"@theme/Npm2Yarn\";\n",
        "\n",
        "<IntegrationInstallTooltip></IntegrationInstallTooltip>\n",
        "\n",
        "<Npm2Yarn>\n",
        "  @langchain/anthropic @langchain/core\n",
        "</Npm2Yarn>\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a38cde65-254d-4219-a441-068766c0d4b5",
      "metadata": {},
      "source": [
        "## 实例化\n",
        "\n",
        "现在我们可以实例化我们的模型对象并生成聊天补全："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatAnthropic } from \"@langchain/anthropic\" \n",
        "\n",
        "const llm = new ChatAnthropic({\n",
        "    model: \"claude-3-haiku-20240307\",\n",
        "    temperature: 0,\n",
        "    maxTokens: undefined,\n",
        "    maxRetries: 2,\n",
        "    // other params...\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4f3e15",
      "metadata": {},
      "source": [
        "## 调用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "62e0dbc3",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"msg_013WBXXiggy6gMbAUY6NpsuU\",\n",
            "  \"content\": \"Voici la traduction en français :\\n\\nJ'adore la programmation.\",\n",
            "  \"additional_kwargs\": {\n",
            "    \"id\": \"msg_013WBXXiggy6gMbAUY6NpsuU\",\n",
            "    \"type\": \"message\",\n",
            "    \"role\": \"assistant\",\n",
            "    \"model\": \"claude-3-haiku-20240307\",\n",
            "    \"stop_reason\": \"end_turn\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"input_tokens\": 29,\n",
            "      \"output_tokens\": 20\n",
            "    }\n",
            "  },\n",
            "  \"response_metadata\": {\n",
            "    \"id\": \"msg_013WBXXiggy6gMbAUY6NpsuU\",\n",
            "    \"model\": \"claude-3-haiku-20240307\",\n",
            "    \"stop_reason\": \"end_turn\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"input_tokens\": 29,\n",
            "      \"output_tokens\": 20\n",
            "    },\n",
            "    \"type\": \"message\",\n",
            "    \"role\": \"assistant\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"input_tokens\": 29,\n",
            "    \"output_tokens\": 20,\n",
            "    \"total_tokens\": 49\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const aiMsg = await llm.invoke([\n",
        "    [\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ],\n",
        "    [\"human\", \"I love programming.\"],\n",
        "])\n",
        "aiMsg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Voici la traduction en français :\n",
            "\n",
            "J'adore la programmation.\n"
          ]
        }
      ],
      "source": [
        "console.log(aiMsg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e2bfc0-7e78-4528-a73f-499ac150dca8",
      "metadata": {},
      "source": [
        "## 链式调用\n",
        "\n",
        "我们可以像这样将模型与提示模板[链式调用](/docs/how_to/sequence/)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e197d1d7-a070-4c96-9f8a-a0e86d046e0b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"msg_01Ca52fpd1mcGRhH4spzAWr4\",\n",
            "  \"content\": \"Ich liebe das Programmieren.\",\n",
            "  \"additional_kwargs\": {\n",
            "    \"id\": \"msg_01Ca52fpd1mcGRhH4spzAWr4\",\n",
            "    \"type\": \"message\",\n",
            "    \"role\": \"assistant\",\n",
            "    \"model\": \"claude-3-haiku-20240307\",\n",
            "    \"stop_reason\": \"end_turn\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"input_tokens\": 23,\n",
            "      \"output_tokens\": 11\n",
            "    }\n",
            "  },\n",
            "  \"response_metadata\": {\n",
            "    \"id\": \"msg_01Ca52fpd1mcGRhH4spzAWr4\",\n",
            "    \"model\": \"claude-3-haiku-20240307\",\n",
            "    \"stop_reason\": \"end_turn\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"input_tokens\": 23,\n",
            "      \"output_tokens\": 11\n",
            "    },\n",
            "    \"type\": \"message\",\n",
            "    \"role\": \"assistant\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"input_tokens\": 23,\n",
            "    \"output_tokens\": 11,\n",
            "    \"total_tokens\": 34\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\"\n",
        "\n",
        "const prompt = ChatPromptTemplate.fromMessages(\n",
        "    [\n",
        "        [\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ],\n",
        "        [\"human\", \"{input}\"],\n",
        "    ]\n",
        ")\n",
        "\n",
        "const chain = prompt.pipe(llm);\n",
        "await chain.invoke(\n",
        "    {\n",
        "        input_language: \"English\",\n",
        "        output_language: \"German\",\n",
        "        input: \"I love programming.\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dac39db",
      "metadata": {},
      "source": [
        "## 内容块\n",
        "\n",
        "Anthropic 模型与大多数其他模型之间的一个关键区别在于，单个 Anthropic AI 消息的内容可以是一个单独的字符串，也可以是 **内容块列表**。例如，当 Anthropic 模型[调用工具](/docs/how_to/tool_calling)时，该工具调用既是消息内容的一部分（同时也体现在标准化的 `AIMessage.tool_calls` 字段中）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f5994de0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"msg_01DZGs9DyuashaYxJ4WWpWUP\",\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"type\": \"text\",\n",
            "      \"text\": \"Here is the calculation for 2 + 2:\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"tool_use\",\n",
            "      \"id\": \"toolu_01SQXBamkBr6K6NdHE7GWwF8\",\n",
            "      \"name\": \"calculator\",\n",
            "      \"input\": {\n",
            "        \"number1\": 2,\n",
            "        \"number2\": 2,\n",
            "        \"operation\": \"add\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"additional_kwargs\": {\n",
            "    \"id\": \"msg_01DZGs9DyuashaYxJ4WWpWUP\",\n",
            "    \"type\": \"message\",\n",
            "    \"role\": \"assistant\",\n",
            "    \"model\": \"claude-3-haiku-20240307\",\n",
            "    \"stop_reason\": \"tool_use\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"input_tokens\": 449,\n",
            "      \"output_tokens\": 100\n",
            "    }\n",
            "  },\n",
            "  \"response_metadata\": {\n",
            "    \"id\": \"msg_01DZGs9DyuashaYxJ4WWpWUP\",\n",
            "    \"model\": \"claude-3-haiku-20240307\",\n",
            "    \"stop_reason\": \"tool_use\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"input_tokens\": 449,\n",
            "      \"output_tokens\": 100\n",
            "    },\n",
            "    \"type\": \"message\",\n",
            "    \"role\": \"assistant\"\n",
            "  },\n",
            "  \"tool_calls\": [\n",
            "    {\n",
            "      \"name\": \"calculator\",\n",
            "      \"args\": {\n",
            "        \"number1\": 2,\n",
            "        \"number2\": 2,\n",
            "        \"operation\": \"add\"\n",
            "      },\n",
            "      \"id\": \"toolu_01SQXBamkBr6K6NdHE7GWwF8\",\n",
            "      \"type\": \"tool_call\"\n",
            "    }\n",
            "  ],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"input_tokens\": 449,\n",
            "    \"output_tokens\": 100,\n",
            "    \"total_tokens\": 549\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "import { z } from \"zod\";\n",
        "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
        "\n",
        "const calculatorSchema = z.object({\n",
        "  operation: z\n",
        "    .enum([\"add\", \"subtract\", \"multiply\", \"divide\"])\n",
        "    .describe(\"The type of operation to execute.\"),\n",
        "  number1: z.number().describe(\"The first number to operate on.\"),\n",
        "  number2: z.number().describe(\"The second number to operate on.\"),\n",
        "});\n",
        "\n",
        "const calculatorTool = {\n",
        "  name: \"calculator\",\n",
        "  description: \"A simple calculator tool\",\n",
        "  input_schema: zodToJsonSchema(calculatorSchema),\n",
        "};\n",
        "\n",
        "const toolCallingLlm = new ChatAnthropic({\n",
        "  model: \"claude-3-haiku-20240307\",\n",
        "}).bindTools([calculatorTool]);\n",
        "\n",
        "const toolPrompt = ChatPromptTemplate.fromMessages([\n",
        "  [\n",
        "    \"system\",\n",
        "    \"You are a helpful assistant who always needs to use a calculator.\",\n",
        "  ],\n",
        "  [\"human\", \"{input}\"],\n",
        "]);\n",
        "\n",
        "// Chain your prompt and model together\n",
        "const toolCallChain = toolPrompt.pipe(toolCallingLlm);\n",
        "\n",
        "await toolCallChain.invoke({\n",
        "  input: \"What is 2 + 2?\",\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d452d4b6",
      "metadata": {},
      "source": [
        "## 自定义请求头\n",
        "\n",
        "你可以通过如下方式在请求中传递自定义请求头："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "41943f0a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"msg_019z4nWpShzsrbSHTWXWQh6z\",\n",
            "  \"content\": \"The sky appears blue due to a phenomenon called Rayleigh scattering. Here's a brief explanation:\\n\\n1) Sunlight is made up of different wavelengths of visible light, including all the colors of the rainbow.\\n\\n2) As sunlight passes through the atmosphere, the gases (mostly nitrogen and oxygen) cause the shorter wavelengths of light, such as violet and blue, to be scattered more easily than the longer wavelengths like red and orange.\\n\\n3) This scattering of the shorter blue wavelengths occurs in all directions by the gas molecules in the atmosphere.\\n\\n4) Our eyes are more sensitive to the scattered blue light than the scattered violet light, so we perceive the sky as having a blue color.\\n\\n5) The scattering is more pronounced for light traveling over longer distances through the atmosphere. This is why the sky appears even darker blue when looking towards the horizon.\\n\\nSo in essence, the selective scattering of the shorter blue wavelengths of sunlight by the gases in the atmosphere is what causes the sky to appear blue to our eyes during the daytime.\",\n",
            "  \"additional_kwargs\": {\n",
            "    \"id\": \"msg_019z4nWpShzsrbSHTWXWQh6z\",\n",
            "    \"type\": \"message\",\n",
            "    \"role\": \"assistant\",\n",
            "    \"model\": \"claude-3-sonnet-20240229\",\n",
            "    \"stop_reason\": \"end_turn\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"input_tokens\": 13,\n",
            "      \"output_tokens\": 236\n",
            "    }\n",
            "  },\n",
            "  \"response_metadata\": {\n",
            "    \"id\": \"msg_019z4nWpShzsrbSHTWXWQh6z\",\n",
            "    \"model\": \"claude-3-sonnet-20240229\",\n",
            "    \"stop_reason\": \"end_turn\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"input_tokens\": 13,\n",
            "      \"output_tokens\": 236\n",
            "    },\n",
            "    \"type\": \"message\",\n",
            "    \"role\": \"assistant\"\n",
            "  },\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"input_tokens\": 13,\n",
            "    \"output_tokens\": 236,\n",
            "    \"total_tokens\": 249\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "\n",
        "const llmWithCustomHeaders = new ChatAnthropic({\n",
        "  model: \"claude-3-sonnet-20240229\",\n",
        "  maxTokens: 1024,\n",
        "  clientOptions: {\n",
        "    defaultHeaders: {\n",
        "      \"X-Api-Key\": process.env.ANTHROPIC_API_KEY,\n",
        "    },\n",
        "  },\n",
        "});\n",
        "\n",
        "await llmWithCustomHeaders.invoke(\"Why is the sky blue?\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5e6d7a",
      "metadata": {},
      "source": [
        "## 提示缓存\n",
        "\n",
        "```{=mdx}\n",
        "\n",
        ":::caution 兼容性\n",
        "此功能目前处于测试阶段。\n",
        ":::\n",
        "\n",
        "```\n",
        "\n",
        "Anthropic 支持[缓存提示的部分内容](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)，以降低需要长上下文场景的费用。您可以缓存工具，以及完整的消息和单独的区块。\n",
        "\n",
        "在初始请求中包含一个或多个带有 `\"cache_control\": { \"type\": \"ephemeral\" }` 字段的区块或工具定义，将自动缓存提示的该部分。此初始缓存步骤将产生额外费用，但后续请求将以较低费率计费。缓存的生命周期为5分钟，但每次命中缓存时都会刷新此时间。\n",
        "\n",
        "目前还存在可缓存提示的最小长度限制，具体取决于模型。您可以[此处](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#structuring-your-prompt)查看此信息。\n",
        "\n",
        "当前您需要使用测试版头部信息初始化模型。以下是一个缓存系统消息部分内容的示例，该消息包含 LangChain [概念文档](/docs/concepts/)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5e02b056",
      "metadata": {},
      "outputs": [],
      "source": [
        "let CACHED_TEXT = \"...\";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bba739ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "// @lc-docs-hide-cell\n",
        "\n",
        "CACHED_TEXT = `## Components\n",
        "\n",
        "LangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.\n",
        "Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\n",
        "\n",
        "### Chat models\n",
        "\n",
        "<span data-heading-keywords=\"chat model,chat models\"></span>\n",
        "\n",
        "Language models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).\n",
        "These are generally newer models (older models are generally \\`LLMs\\`, see below).\n",
        "Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.\n",
        "\n",
        "Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input.\n",
        "This gives them the same interface as LLMs (and simpler to use).\n",
        "When a string is passed in as input, it will be converted to a \\`HumanMessage\\` under the hood before being passed to the underlying model.\n",
        "\n",
        "LangChain does not host any Chat Models, rather we rely on third party integrations.\n",
        "\n",
        "We have some standardized parameters when constructing ChatModels:\n",
        "\n",
        "- \\`model\\`: the name of the model\n",
        "\n",
        "Chat Models also accept other parameters that are specific to that integration.\n",
        "\n",
        ":::important\n",
        "Some chat models have been fine-tuned for **tool calling** and provide a dedicated API for it.\n",
        "Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.\n",
        "Please see the [tool calling section](/docs/concepts/tool_calling) for more information.\n",
        ":::\n",
        "\n",
        "For specifics on how to use chat models, see the [relevant how-to guides here](/docs/how_to/#chat-models).\n",
        "\n",
        "#### Multimodality\n",
        "\n",
        "Some chat models are multimodal, accepting images, audio and even video as inputs.\n",
        "These are still less common, meaning model providers haven't standardized on the \"best\" way to define the API.\n",
        "Multimodal outputs are even less common. As such, we've kept our multimodal abstractions fairly light weight\n",
        "and plan to further solidify the multimodal APIs and interaction patterns as the field matures.\n",
        "\n",
        "In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format.\n",
        "So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.\n",
        "\n",
        "For specifics on how to use multimodal models, see the [relevant how-to guides here](/docs/how_to/#multimodal).\n",
        "\n",
        "### LLMs\n",
        "\n",
        "<span data-heading-keywords=\"llm,llms\"></span>\n",
        "\n",
        ":::caution\n",
        "Pure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as [chat completion models](/docs/concepts/chat_models),\n",
        "even for non-chat use cases.\n",
        "\n",
        "You are probably looking for [the section above instead](/docs/concepts/chat_models).\n",
        ":::\n",
        "\n",
        "Language models that takes a string as input and returns a string.\n",
        "These are traditionally older models (newer models generally are [Chat Models](/docs/concepts/chat_models), see above).\n",
        "\n",
        "Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.\n",
        "This gives them the same interface as [Chat Models](/docs/concepts/chat_models).\n",
        "When messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.\n",
        "\n",
        "LangChain does not host any LLMs, rather we rely on third party integrations.\n",
        "\n",
        "For specifics on how to use LLMs, see the [relevant how-to guides here](/docs/how_to/#llms).\n",
        "\n",
        "### Message types\n",
        "\n",
        "Some language models take an array of messages as input and return a message.\n",
        "There are a few different types of messages.\n",
        "All messages have a \\`role\\`, \\`content\\`, and \\`response_metadata\\` property.\n",
        "\n",
        "The \\`role\\` describes WHO is saying the message.\n",
        "LangChain has different message classes for different roles.\n",
        "\n",
        "The \\`content\\` property describes the content of the message.\n",
        "This can be a few different things:\n",
        "\n",
        "- A string (most models deal this type of content)\n",
        "- A List of objects (this is used for multi-modal input, where the object contains information about that input type and that input location)\n",
        "\n",
        "#### HumanMessage\n",
        "\n",
        "This represents a message from the user.\n",
        "\n",
        "#### AIMessage\n",
        "\n",
        "This represents a message from the model. In addition to the \\`content\\` property, these messages also have:\n",
        "\n",
        "**\\`response_metadata\\`**\n",
        "\n",
        "The \\`response_metadata\\` property contains additional metadata about the response. The data here is often specific to each model provider.\n",
        "This is where information like log-probs and token usage may be stored.\n",
        "\n",
        "**\\`tool_calls\\`**\n",
        "\n",
        "These represent a decision from an language model to call a tool. They are included as part of an \\`AIMessage\\` output.\n",
        "They can be accessed from there with the \\`.tool_calls\\` property.\n",
        "\n",
        "This property returns a list of \\`ToolCall\\`s. A \\`ToolCall\\` is an object with the following arguments:\n",
        "\n",
        "- \\`name\\`: The name of the tool that should be called.\n",
        "- \\`args\\`: The arguments to that tool.\n",
        "- \\`id\\`: The id of that tool call.\n",
        "\n",
        "#### SystemMessage\n",
        "\n",
        "This represents a system message, which tells the model how to behave. Not every model provider supports this.\n",
        "\n",
        "#### ToolMessage\n",
        "\n",
        "This represents the result of a tool call. In addition to \\`role\\` and \\`content\\`, this message has:\n",
        "\n",
        "- a \\`tool_call_id\\` field which conveys the id of the call to the tool that was called to produce this result.\n",
        "- an \\`artifact\\` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\n",
        "\n",
        "#### (Legacy) FunctionMessage\n",
        "\n",
        "This is a legacy message type, corresponding to OpenAI's legacy function-calling API. \\`ToolMessage\\` should be used instead to correspond to the updated tool-calling API.\n",
        "\n",
        "This represents the result of a function call. In addition to \\`role\\` and \\`content\\`, this message has a \\`name\\` parameter which conveys the name of the function that was called to produce this result.\n",
        "\n",
        "### Prompt templates\n",
        "\n",
        "<span data-heading-keywords=\"prompt,prompttemplate,chatprompttemplate\"></span>\n",
        "\n",
        "Prompt templates help to translate user input and parameters into instructions for a language model.\n",
        "This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
        "\n",
        "Prompt Templates take as input an object, where each key represents a variable in the prompt template to fill in.\n",
        "\n",
        "Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or an array of messages.\n",
        "The reason this PromptValue exists is to make it easy to switch between strings and messages.\n",
        "\n",
        "There are a few different types of prompt templates:\n",
        "\n",
        "#### String PromptTemplates\n",
        "\n",
        "These prompt templates are used to format a single string, and generally are used for simpler inputs.\n",
        "For example, a common way to construct and use a PromptTemplate is as follows:\n",
        "\n",
        "\\`\\`\\`typescript\n",
        "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "const promptTemplate = PromptTemplate.fromTemplate(\n",
        "  \"Tell me a joke about {topic}\"\n",
        ");\n",
        "\n",
        "await promptTemplate.invoke({ topic: \"cats\" });\n",
        "\\`\\`\\`\n",
        "\n",
        "#### ChatPromptTemplates\n",
        "\n",
        "These prompt templates are used to format an array of messages. These \"templates\" consist of an array of templates themselves.\n",
        "For example, a common way to construct and use a ChatPromptTemplate is as follows:\n",
        "\n",
        "\\`\\`\\`typescript\n",
        "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
        "\n",
        "const promptTemplate = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", \"You are a helpful assistant\"],\n",
        "  [\"user\", \"Tell me a joke about {topic}\"],\n",
        "]);\n",
        "\n",
        "await promptTemplate.invoke({ topic: \"cats\" });\n",
        "\\`\\`\\`\n",
        "\n",
        "In the above example, this ChatPromptTemplate will construct two messages when called.\n",
        "The first is a system message, that has no variables to format.\n",
        "The second is a HumanMessage, and will be formatted by the \\`topic\\` variable the user passes in.\n",
        "\n",
        "#### MessagesPlaceholder\n",
        "\n",
        "<span data-heading-keywords=\"messagesplaceholder\"></span>\n",
        "\n",
        "This prompt template is responsible for adding an array of messages in a particular place.\n",
        "In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\n",
        "But what if we wanted the user to pass in an array of messages that we would slot into a particular spot?\n",
        "This is how you use MessagesPlaceholder.\n",
        "\n",
        "\\`\\`\\`typescript\n",
        "import {\n",
        "  ChatPromptTemplate,\n",
        "  MessagesPlaceholder,\n",
        "} from \"@langchain/core/prompts\";\n",
        "import { HumanMessage } from \"@langchain/core/messages\";\n",
        "\n",
        "const promptTemplate = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", \"You are a helpful assistant\"],\n",
        "  new MessagesPlaceholder(\"msgs\"),\n",
        "]);\n",
        "\n",
        "promptTemplate.invoke({ msgs: [new HumanMessage({ content: \"hi!\" })] });\n",
        "\\`\\`\\`\n",
        "\n",
        "This will produce an array of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\n",
        "If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\n",
        "This is useful for letting an array of messages be slotted into a particular spot.\n",
        "\n",
        "An alternative way to accomplish the same thing without using the \\`MessagesPlaceholder\\` class explicitly is:\n",
        "\n",
        "\\`\\`\\`typescript\n",
        "const promptTemplate = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", \"You are a helpful assistant\"],\n",
        "  [\"placeholder\", \"{msgs}\"], // <-- This is the changed part\n",
        "]);\n",
        "\\`\\`\\`\n",
        "\n",
        "For specifics on how to use prompt templates, see the [relevant how-to guides here](/docs/how_to/#prompt-templates).\n",
        "\n",
        "### Example Selectors\n",
        "\n",
        "One common prompting technique for achieving better performance is to include examples as part of the prompt.\n",
        "This gives the language model concrete examples of how it should behave.\n",
        "Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\n",
        "Example Selectors are classes responsible for selecting and then formatting examples into prompts.\n",
        "\n",
        "For specifics on how to use example selectors, see the [relevant how-to guides here](/docs/how_to/#example-selectors).\n",
        "\n",
        "### Output parsers\n",
        "\n",
        "<span data-heading-keywords=\"output parser\"></span>\n",
        "\n",
        ":::note\n",
        "\n",
        "The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\n",
        "More and more models are supporting function (or tool) calling, which handles this automatically.\n",
        "It is recommended to use function/tool calling rather than output parsing.\n",
        "See documentation for that [here](/docs/concepts/tool_calling).\n",
        "\n",
        ":::\n",
        "\n",
        "Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\n",
        "Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\n",
        "\n",
        "There are two main methods an output parser must implement:\n",
        "\n",
        "- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
        "- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
        "\n",
        "And then one optional one:\n",
        "\n",
        "- \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n",
        "\n",
        "Output parsers accept a string or \\`BaseMessage\\` as input and can return an arbitrary type.\n",
        "\n",
        "LangChain has many different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:\n",
        "\n",
        "**Name**: The name of the output parser\n",
        "\n",
        "**Supports Streaming**: Whether the output parser supports streaming.\n",
        "\n",
        "**Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific arguments.\n",
        "\n",
        "**Output Type**: The output type of the object returned by the parser.\n",
        "\n",
        "**Description**: Our commentary on this output parser and when to use it.\n",
        "\n",
        "The current date is ${new Date().toISOString()}`;\n",
        "\n",
        "// Noop statement to hide output\n",
        "void 0;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6e47de9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USAGE: {\n",
            "  input_tokens: 19,\n",
            "  cache_creation_input_tokens: 2921,\n",
            "  cache_read_input_tokens: 0,\n",
            "  output_tokens: 355\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "\n",
        "const modelWithCaching = new ChatAnthropic({\n",
        "  model: \"claude-3-haiku-20240307\",\n",
        "  clientOptions: {\n",
        "    defaultHeaders: {\n",
        "      \"anthropic-beta\": \"prompt-caching-2024-07-31\",\n",
        "    },\n",
        "  },\n",
        "});\n",
        "\n",
        "const LONG_TEXT = `You are a pirate. Always respond in pirate dialect.\n",
        "\n",
        "Use the following as context when answering questions:\n",
        "\n",
        "${CACHED_TEXT}`;\n",
        "\n",
        "const messages = [\n",
        "  {\n",
        "    role: \"system\",\n",
        "    content: [\n",
        "      {\n",
        "        type: \"text\",\n",
        "        text: LONG_TEXT,\n",
        "        // Tell Anthropic to cache this block\n",
        "        cache_control: { type: \"ephemeral\" },\n",
        "      },\n",
        "    ],\n",
        "  },\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: \"What types of messages are supported in LangChain?\",\n",
        "  },\n",
        "];\n",
        "\n",
        "const res = await modelWithCaching.invoke(messages);\n",
        "\n",
        "console.log(\"USAGE:\", res.response_metadata.usage);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "826d95b6",
      "metadata": {},
      "source": [
        "我们可以看到，Anthropic返回的原始使用情况字段中有一个名为`cache_creation_input_tokens`的新字段。\n",
        "\n",
        "如果我们再次使用相同的消息，可以看到长文本的输入代币是从缓存中读取的："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5d264f8b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USAGE: {\n",
            "  input_tokens: 19,\n",
            "  cache_creation_input_tokens: 0,\n",
            "  cache_read_input_tokens: 2921,\n",
            "  output_tokens: 357\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "const res2 = await modelWithCaching.invoke(messages);\n",
        "\n",
        "console.log(\"USAGE:\", res2.response_metadata.usage);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc6bba1b",
      "metadata": {},
      "source": [
        "### 工具缓存\n",
        "\n",
        "您还可以通过在工具定义中设置相同的 `\"cache_control\": { \"type\": \"ephemeral\" }` 来缓存工具。目前，您需要使用[Anthropic的原始工具格式](https://docs.anthropic.com/en/docs/build-with-claude/tool-use)绑定工具。以下是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7c5eaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "const SOME_LONG_DESCRIPTION = \"...\";\n",
        "\n",
        "// Tool in Anthropic format\n",
        "const anthropicTools = [{\n",
        "  name: \"get_weather\",\n",
        "  description: SOME_LONG_DESCRIPTION,\n",
        "  input_schema: {\n",
        "    type: \"object\",\n",
        "    properties: {\n",
        "      location: {\n",
        "        type: \"string\",\n",
        "        description: \"Location to get the weather for\",\n",
        "      },\n",
        "      unit: {\n",
        "        type: \"string\",\n",
        "        description: \"Temperature unit to return\",\n",
        "      },\n",
        "    },\n",
        "    required: [\"location\"],\n",
        "  },\n",
        "  // Tell Anthropic to cache this tool\n",
        "  cache_control: { type: \"ephemeral\" },\n",
        "}]\n",
        "\n",
        "const modelWithCachedTools = modelWithCaching.bindTools(anthropicTools);\n",
        "\n",
        "await modelWithCachedTools.invoke(\"what is the weather in SF?\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d000dd9",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "关于提示缓存的工作原理详情，请参阅 [Anthropic的文档](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#how-prompt-caching-works)。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8dece4e",
      "metadata": {},
      "source": [
        "## 自定义客户端\n",
        "\n",
        "Anthropic 模型 [可以托管在诸如 Google Vertex 等云服务上](https://docs.anthropic.com/en/api/claude-on-vertex-ai)，这些服务依赖于与主 Anthropic 客户端具有相同接口的不同底层客户端。你可以通过提供一个 `createClient` 方法来访问这些服务，该方法返回一个已初始化的 Anthropic 客户端实例。以下是一个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ec6d41",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { AnthropicVertex } from \"@anthropic-ai/vertex-sdk\";\n",
        "\n",
        "const customClient = new AnthropicVertex();\n",
        "\n",
        "const modelWithCustomClient = new ChatAnthropic({\n",
        "  modelName: \"claude-3-sonnet@20240229\",\n",
        "  maxRetries: 0,\n",
        "  createClient: () => customClient,\n",
        "});\n",
        "\n",
        "await modelWithCustomClient.invoke([{ role: \"user\", content: \"Hello!\" }]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a85a61",
      "metadata": {},
      "source": [
        "## 引用\n",
        "\n",
        "Anthropic 支持 [引用](https://docs.anthropic.com/en/docs/build-with-claude/citations) 功能，该功能允许 Claude 根据用户提供来源材料为其回答附加上下文信息。来源材料可以通过 [文档内容块](https://docs.anthropic.com/en/docs/build-with-claude/citations#document-types) 的形式提供，用以描述完整文档，或者通过 [搜索结果](https://docs.anthropic.com/en/docs/build-with-claude/search-results) 的形式提供，用以描述检索系统返回的相关段落或片段。当在查询中包含 `\"citations\": { \"enabled\": true }` 时，Claude 可能在其响应中生成对所提供材料的直接引用。\n",
        "\n",
        "### 文档示例\n",
        "\n",
        "在此示例中，我们提供了一个 [纯文本文档](https://docs.anthropic.com/en/docs/build-with-claude/citations#plain-text-documents)。在后台，Claude 会 [自动将](https://docs.anthropic.com/en/docs/build-with-claude/citations#plain-text-documents) 输入文本拆分为句子，并在生成引用时使用这些句子。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d3f1c754",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"Based on the document, I can tell you that:\\n\\n- \"\n",
            "  },\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"The grass is green\",\n",
            "    \"citations\": [\n",
            "      {\n",
            "        \"type\": \"char_location\",\n",
            "        \"cited_text\": \"The grass is green. \",\n",
            "        \"document_index\": 0,\n",
            "        \"document_title\": \"My Document\",\n",
            "        \"start_char_index\": 0,\n",
            "        \"end_char_index\": 20\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"\\n- \"\n",
            "  },\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"The sky is blue\",\n",
            "    \"citations\": [\n",
            "      {\n",
            "        \"type\": \"char_location\",\n",
            "        \"cited_text\": \"The sky is blue.\",\n",
            "        \"document_index\": 0,\n",
            "        \"document_title\": \"My Document\",\n",
            "        \"start_char_index\": 20,\n",
            "        \"end_char_index\": 36\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "\n",
        "const citationsModel = new ChatAnthropic({\n",
        "  model: \"claude-3-5-haiku-latest\",\n",
        "});\n",
        "\n",
        "const messagesWithCitations = [\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: [\n",
        "      {\n",
        "        type: \"document\",\n",
        "        source: {\n",
        "          type: \"text\",\n",
        "          media_type: \"text/plain\",\n",
        "          data: \"The grass is green. The sky is blue.\",\n",
        "        },\n",
        "        title: \"My Document\",\n",
        "        context: \"This is a trustworthy document.\",\n",
        "        citations: {\n",
        "          enabled: true,\n",
        "        },\n",
        "      },\n",
        "      {\n",
        "        type: \"text\",\n",
        "        text: \"What color is the grass and sky?\",\n",
        "      },\n",
        "    ],\n",
        "  }\n",
        "];\n",
        "\n",
        "const responseWithCitations = await citationsModel.invoke(messagesWithCitations);\n",
        "\n",
        "console.log(JSON.stringify(responseWithCitations.content, null, 2));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d19d17",
      "metadata": {},
      "source": [
        "### 搜索结果示例\n",
        "\n",
        "在此示例中，我们将[搜索结果](https://docs.anthropic.com/en/docs/build-with-claude/search-results)作为消息内容的一部分传入。这允许Claude在其响应中引用您自己的检索系统中的特定段落或片段。\n",
        "\n",
        "当您希望Claude引用来自特定知识集的信息，但又希望直接引入自己预先获取/缓存的内容，而不是让模型自动搜索或检索时，这种方法非常有用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd441c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "\n",
        "const citationsModel = new ChatAnthropic({\n",
        "  model: \"claude-3-5-haiku-latest\",\n",
        "  clientOptions: {\n",
        "    defaultHeaders: {\n",
        "      \"anthropic-beta\": \"search-results-2025-06-09\",\n",
        "    },\n",
        "  },\n",
        "});\n",
        "\n",
        "const messagesWithCitations = [\n",
        "  {\n",
        "    type: \"user\",\n",
        "    content: [\n",
        "      {\n",
        "        type: \"search_result\",\n",
        "        title: \"History of France\",\n",
        "        source: \"https://some-uri.com\",\n",
        "        citations: { enabled: true },\n",
        "        content: [\n",
        "          {\n",
        "            type: \"text\",\n",
        "            text: \"The capital of France is Paris.\",\n",
        "          },\n",
        "          {\n",
        "            type: \"text\",\n",
        "            text: \"The old capital of France was Lyon.\",\n",
        "          },\n",
        "        ],\n",
        "      },\n",
        "      {\n",
        "        type: \"text\",\n",
        "        text: \"What is the capital of France?\",\n",
        "      },\n",
        "    ],\n",
        "  },\n",
        "];\n",
        "\n",
        "const responseWithCitations = await citationsModel.invoke(messagesWithCitations);\n",
        "\n",
        "console.log(JSON.stringify(responseWithCitations.content, null, 2));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae00bcb",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### 来自工具的搜索结果\n",
        "\n",
        "你还可以使用工具提供搜索结果，模型可以在其回答中引用这些结果。这对于RAG（或[检索增强生成](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)）工作流程非常适用，因为Claude可以决定何时以及从何处检索信息。当以[搜索结果](https://docs.anthropic.com/en/docs/build-with-claude/search-results)的形式返回这些信息时，可以让Claude能够从工具返回的材料中创建引用。\n",
        "\n",
        "以下是如何创建一个以Anthropic引用API所期望的格式返回搜索结果的工具：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e83f7598",
      "metadata": {},
      "outputs": [],
      "source": [
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "\n",
        "// Create a tool that returns search results\n",
        "const ragTool = tool(\n",
        "  () => [\n",
        "    {\n",
        "      type: \"search_result\",\n",
        "      title: \"History of France\",\n",
        "      source: \"https://some-uri.com\",\n",
        "      citations: { enabled: true },\n",
        "      content: [\n",
        "        {\n",
        "          type: \"text\",\n",
        "          text: \"The capital of France is Paris.\",\n",
        "        },\n",
        "        {\n",
        "          type: \"text\",\n",
        "          text: \"The old capital of France was Lyon.\",\n",
        "        },\n",
        "      ],\n",
        "    },\n",
        "    {\n",
        "      type: \"search_result\", \n",
        "      title: \"Geography of France\",\n",
        "      source: \"https://some-uri.com\",\n",
        "      citations: { enabled: true },\n",
        "      content: [\n",
        "        {\n",
        "          type: \"text\",\n",
        "          text: \"France is a country in Europe.\",\n",
        "        },\n",
        "        {\n",
        "          type: \"text\",\n",
        "          text: \"The capital of France is Paris.\",\n",
        "        },\n",
        "      ],\n",
        "    },\n",
        "  ],\n",
        "  {\n",
        "    name: \"my_rag_tool\",\n",
        "    description: \"Retrieval system that accesses my knowledge base.\",\n",
        "    schema: z.object({\n",
        "      query: z.string().describe(\"query to search in the knowledge base\"),\n",
        "    }),\n",
        "  }\n",
        ");\n",
        "\n",
        "// Create model with search results beta header\n",
        "const model = new ChatAnthropic({\n",
        "  model: \"claude-3-5-haiku-latest\",\n",
        "  clientOptions: {\n",
        "    defaultHeaders: {\n",
        "      \"anthropic-beta\": \"search-results-2025-06-09\",\n",
        "    },\n",
        "  },\n",
        "}).bindTools([ragTool]);\n",
        "\n",
        "const result = await model.invoke([\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: \"What is the capital of France?\",\n",
        "  },\n",
        "]);\n",
        "\n",
        "console.log(JSON.stringify(result.content, null, 2));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d33db17",
      "metadata": {},
      "source": [
        "[在此处](https://js.langchain.com/docs/concepts/rag/)了解有关RAG在LangChain中如何工作的更多信息\n",
        "\n",
        "[在此处](https://js.langchain.com/docs/how_to/tool_calling/)了解有关工具调用的更多信息"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14269f15",
      "metadata": {},
      "source": [
        "### 与文本拆分器一起使用\n",
        "\n",
        "Anthropic 还允许您使用[自定义文档](https://docs.anthropic.com/en/docs/build-with-claude/citations#custom-content-documents)类型指定自己的拆分方式。LangChain 的[文本拆分器](/docs/concepts/text_splitters/)可用于为此生成有意义的拆分。请参见以下示例，我们将 LangChain.js 的 README（一个 Markdown 文档）进行拆分，并将其作为上下文传递给 Claude："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e9f3213",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"Based on the documentation, I can provide you with a link to LangChain's tutorials:\\n\\n\"\n",
            "  },\n",
            "  {\n",
            "    \"type\": \"text\",\n",
            "    \"text\": \"The tutorials can be found at: https://js.langchain.com/docs/tutorials/\",\n",
            "    \"citations\": [\n",
            "      {\n",
            "        \"type\": \"content_block_location\",\n",
            "        \"cited_text\": \"[Tutorial](https://js.langchain.com/docs/tutorials/)walkthroughs\",\n",
            "        \"document_index\": 0,\n",
            "        \"document_title\": null,\n",
            "        \"start_block_index\": 191,\n",
            "        \"end_block_index\": 194\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
        "import { MarkdownTextSplitter } from \"langchain/text_splitter\";\n",
        "\n",
        "function formatToAnthropicDocuments(documents: string[]) {\n",
        "  return {\n",
        "    type: \"document\",\n",
        "    source: {\n",
        "      type: \"content\",\n",
        "      content: documents.map((document) => ({ type: \"text\", text: document })),\n",
        "    },\n",
        "    citations: { enabled: true },\n",
        "  };\n",
        "}\n",
        "\n",
        "// Pull readme\n",
        "const readmeResponse = await fetch(\n",
        "  \"https://raw.githubusercontent.com/langchain-ai/langchainjs/master/README.md\"\n",
        ");\n",
        "\n",
        "const readme = await readmeResponse.text();\n",
        "\n",
        "// Split into chunks\n",
        "const splitter = new MarkdownTextSplitter({\n",
        "  chunkOverlap: 0,\n",
        "  chunkSize: 50,\n",
        "});\n",
        "const documents = await splitter.splitText(readme);\n",
        "\n",
        "// Construct message\n",
        "const messageWithSplitDocuments = {\n",
        "  role: \"user\",\n",
        "  content: [\n",
        "    formatToAnthropicDocuments(documents),\n",
        "    { type: \"text\", text: \"Give me a link to LangChain's tutorials. Cite your sources\" },\n",
        "  ],\n",
        "};\n",
        "\n",
        "// Query LLM\n",
        "const citationsModelWithSplits = new ChatAnthropic({\n",
        "  model: \"claude-3-5-sonnet-latest\",\n",
        "});\n",
        "const resWithSplits = await citationsModelWithSplits.invoke([messageWithSplitDocuments]);\n",
        "\n",
        "console.log(JSON.stringify(resWithSplits.content, null, 2));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3",
      "metadata": {},
      "source": [
        "## API 参考文档\n",
        "\n",
        "如需了解 ChatAnthropic 所有功能和配置的详细文档，请访问 API 参考页面: https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}