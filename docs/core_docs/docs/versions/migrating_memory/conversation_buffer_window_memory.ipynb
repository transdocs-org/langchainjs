{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ce8457ed-c0b1-4a74-abbd-9d3d2211270f",
      "metadata": {},
      "source": [
        "# 从ConversationTokenBufferMemory迁移\n",
        "\n",
        "如果您正在尝试弃用以下列出的旧内存类之一，请遵循本指南：\n",
        "\n",
        "\n",
        "| 内存类型                      | 描述                                                                                                                                                       |\n",
        "|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| `ConversationTokenBufferMemory`  | 在对话中保留最近的消息，前提是对话中的总token数不超过一定限制。 |\n",
        "\n",
        "`ConversationTokenBufferMemory` 在原始对话历史记录的基础上应用额外的处理，以将对话历史记录修剪为适合聊天模型上下文窗口大小的尺寸。\n",
        "\n",
        "可以使用LangChain内置的[trimMessages](https://api.js.langchain.com/functions/_langchain_core.messages.trimMessages.html)函数来完成此处理功能。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79935247-acc7-4a05-a387-5d72c9c8c8cb",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        ":::important\n",
        "\n",
        "我们将从探索一个简单的方法开始，该方法包括对整个对话历史应用处理逻辑。\n",
        "\n",
        "虽然这种方法易于实现，但它有一个缺点：随着对话的增长，延迟也会增加，因为每次对话回合都会对之前的所有对话内容重新应用相同的处理逻辑。\n",
        "\n",
        "更高级的策略则侧重于增量更新对话历史，以避免重复处理。\n",
        "\n",
        "例如，LangGraph的[总结处理指南](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/)展示了\n",
        "如何维护对话的持续摘要，同时丢弃较旧的消息，确保在后续对话中不会重新处理这些旧消息。\n",
        "\n",
        ":::\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d07f9459-9fb6-4942-99c9-64558aedd7d4",
      "metadata": {},
      "source": [
        "## 环境设置\n",
        "\n",
        "### 依赖项\n",
        "\n",
        "```{=mdx}\n",
        "import Npm2Yarn from \"@theme/Npm2Yarn\"\n",
        "\n",
        "<Npm2Yarn>\n",
        "  @langchain/openai @langchain/core zod\n",
        "</Npm2Yarn>\n",
        "```\n",
        "\n",
        "### 环境变量\n",
        "\n",
        "```typescript\n",
        "process.env.OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\";\n",
        "```\n",
        "\n",
        "```{=mdx}\n",
        "<details open>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce2d951",
      "metadata": {},
      "source": [
        "## 重新实现ConversationTokenBufferMemory逻辑\n",
        "\n",
        "在这里，我们将使用`trimMessages`来保留系统消息和最近的对话消息，确保对话中的总token数量不超过特定限制。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e1550bee",
      "metadata": {},
      "outputs": [],
      "source": [
        "import {\n",
        "  AIMessage,\n",
        "  HumanMessage,\n",
        "  SystemMessage,\n",
        "} from \"@langchain/core/messages\";\n",
        "\n",
        "const messages = [\n",
        "  new SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
        "  new HumanMessage(\"i wonder why it's called langchain\"),\n",
        "  new AIMessage(\n",
        "    'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
        "  ),\n",
        "  new HumanMessage(\"and who is harrison chasing anyways\"),\n",
        "  new AIMessage(\n",
        "      \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
        "  ),\n",
        "  new HumanMessage(\"why is 42 always the answer?\"),\n",
        "  new AIMessage(\n",
        "      \"Because it's the only number that's constantly right, even when it doesn't add up!\"\n",
        "  ),\n",
        "  new HumanMessage(\"What did the cow say?\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6442f74b-2c36-48fd-a3d1-c7c5d18c050f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SystemMessage {\n",
            "  \"content\": \"you're a good assistant, you always respond with a joke.\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {}\n",
            "}\n",
            "HumanMessage {\n",
            "  \"content\": \"and who is harrison chasing anyways\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {}\n",
            "}\n",
            "AIMessage {\n",
            "  \"content\": \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {},\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": []\n",
            "}\n",
            "HumanMessage {\n",
            "  \"content\": \"why is 42 always the answer?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {}\n",
            "}\n",
            "AIMessage {\n",
            "  \"content\": \"Because it's the only number that's constantly right, even when it doesn't add up!\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {},\n",
            "  \"tool_calls\": [],\n",
            "  \"invalid_tool_calls\": []\n",
            "}\n",
            "HumanMessage {\n",
            "  \"content\": \"What did the cow say?\",\n",
            "  \"additional_kwargs\": {},\n",
            "  \"response_metadata\": {}\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { trimMessages } from \"@langchain/core/messages\";\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "\n",
        "const selectedMessages = await trimMessages(\n",
        "  messages,\n",
        "  {\n",
        "    // Please see API reference for trimMessages for other ways to specify a token counter.\n",
        "    tokenCounter: new ChatOpenAI({ model: \"gpt-4o\" }),\n",
        "    maxTokens: 80,  // <-- token limit\n",
        "    // The startOn is specified\n",
        "    // to make sure we do not generate a sequence where\n",
        "    // a ToolMessage that contains the result of a tool invocation\n",
        "    // appears before the AIMessage that requested a tool invocation\n",
        "    // as this will cause some chat models to raise an error.\n",
        "    startOn: \"human\",\n",
        "    strategy: \"last\",\n",
        "    includeSystem: true,  // <-- Keep the system message\n",
        "  }\n",
        ")\n",
        "\n",
        "for (const msg of selectedMessages) {\n",
        "    console.log(msg);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f05d272-2d22-44b7-9fa6-e617a48584b4",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "</details>\n",
        "```\n",
        "\n",
        "## 使用 LangGraph 的现代用法\n",
        "\n",
        "下面的示例展示了如何使用 LangGraph 添加简单的对话预处理逻辑。\n",
        "\n",
        "```{=mdx}\n",
        ":::note\n",
        "\n",
        "如果你希望避免每次都在整个对话历史记录上执行计算，可以参考\n",
        "[摘要指南](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/)，该指南展示了\n",
        "如何丢弃较旧的消息，确保它们在后续对话轮次中不会被重新处理。\n",
        "\n",
        ":::\n",
        "```\n",
        "\n",
        "```{=mdx}\n",
        "<details open>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "05d360e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi! I'm bob\n",
            "Hello, Bob! How can I assist you today?\n",
            "what was my name?\n",
            "You mentioned that your name is Bob. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "import { v4 as uuidv4 } from 'uuid';\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "import { StateGraph, MessagesAnnotation, END, START, MemorySaver } from \"@langchain/langgraph\";\n",
        "import { trimMessages } from \"@langchain/core/messages\";\n",
        "\n",
        "// Define a chat model\n",
        "const model = new ChatOpenAI({ model: \"gpt-4o\" });\n",
        "\n",
        "// Define the function that calls the model\n",
        "const callModel = async (state: typeof MessagesAnnotation.State): Promise<Partial<typeof MessagesAnnotation.State>> => {\n",
        "  // highlight-start\n",
        "  const selectedMessages = await trimMessages(\n",
        "    state.messages,\n",
        "    {\n",
        "      tokenCounter: (messages) => messages.length, // Simple message count instead of token count\n",
        "      maxTokens: 5, // Allow up to 5 messages\n",
        "      strategy: \"last\",\n",
        "      startOn: \"human\",\n",
        "      includeSystem: true,\n",
        "      allowPartial: false,\n",
        "    }\n",
        "  );\n",
        "  // highlight-end\n",
        "\n",
        "  const response = await model.invoke(selectedMessages);\n",
        "\n",
        "  // With LangGraph, we're able to return a single message, and LangGraph will concatenate\n",
        "  // it to the existing list\n",
        "  return { messages: [response] };\n",
        "};\n",
        "\n",
        "\n",
        "// Define a new graph\n",
        "const workflow = new StateGraph(MessagesAnnotation)\n",
        "// Define the two nodes we will cycle between\n",
        "  .addNode(\"model\", callModel)\n",
        "  .addEdge(START, \"model\")\n",
        "  .addEdge(\"model\", END)\n",
        "\n",
        "const app = workflow.compile({\n",
        "  // Adding memory is straightforward in LangGraph!\n",
        "  // Just pass a checkpointer to the compile method.\n",
        "  checkpointer: new MemorySaver()\n",
        "});\n",
        "\n",
        "// The thread id is a unique key that identifies this particular conversation\n",
        "// ---\n",
        "// NOTE: this must be `thread_id` and not `threadId` as the LangGraph internals expect `thread_id`\n",
        "// ---\n",
        "const thread_id = uuidv4();\n",
        "const config = { configurable: { thread_id }, streamMode: \"values\" as const };\n",
        "\n",
        "const inputMessage = {\n",
        "  role: \"user\",\n",
        "  content: \"hi! I'm bob\",\n",
        "}\n",
        "for await (const event of await app.stream({ messages: [inputMessage] }, config)) {\n",
        "  const lastMessage = event.messages[event.messages.length - 1];\n",
        "  console.log(lastMessage.content);\n",
        "}\n",
        "\n",
        "// Here, let's confirm that the AI remembers our name!\n",
        "const followUpMessage = {\n",
        "  role: \"user\",\n",
        "  content: \"what was my name?\",\n",
        "}\n",
        "\n",
        "// ---\n",
        "// NOTE: You must pass the same thread id to continue the conversation\n",
        "// we do that here by passing the same `config` object to the `.stream` call.\n",
        "// ---\n",
        "for await (const event of await app.stream({ messages: [followUpMessage] }, config)) {\n",
        "  const lastMessage = event.messages[event.messages.length - 1];\n",
        "  console.log(lastMessage.content);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84229e2e-a578-4b21-840a-814223406402",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "</details>\n",
        "```\n",
        "\n",
        "## 使用预构建的 langgraph agent\n",
        "\n",
        "本示例展示了如何使用 Agent Executor 与通过 [createReactAgent](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html) 函数构建的预构建 agent。\n",
        "\n",
        "如果您正在使用 [旧版 LangChain 预构建 agent](https://js.langchain.com/v0.1/docs/modules/agents/agent_types/)，\n",
        "则可以用新的 [LangGraph 预构建 agent](https://langchain-ai.github.io/langgraphjs/how-tos/create-react-agent/) 替换原有代码，\n",
        "它利用了聊天模型的原生工具调用能力，开箱即用，效果可能更佳。\n",
        "\n",
        "```{=mdx}\n",
        "<details open>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9e54ccdc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi! I'm bob. What is my age?\n",
            "\n",
            "42 years old\n",
            "Hi Bob! You are 42 years old.\n",
            "do you remember my name?\n",
            "Yes, your name is Bob! If there's anything else you'd like to know or discuss, feel free to ask.\n"
          ]
        }
      ],
      "source": [
        "import { z } from \"zod\";\n",
        "import { v4 as uuidv4 } from 'uuid';\n",
        "import { BaseMessage, trimMessages } from \"@langchain/core/messages\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "import { MemorySaver } from \"@langchain/langgraph\";\n",
        "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
        "\n",
        "const getUserAge = tool(\n",
        "  (name: string): string => {\n",
        "    // This is a placeholder for the actual implementation\n",
        "    if (name.toLowerCase().includes(\"bob\")) {\n",
        "      return \"42 years old\";\n",
        "    }\n",
        "    return \"41 years old\";\n",
        "  },\n",
        "  {\n",
        "    name: \"get_user_age\",\n",
        "    description: \"Use this tool to find the user's age.\",\n",
        "    schema: z.string().describe(\"the name of the user\"),\n",
        "  }\n",
        ");\n",
        "\n",
        "const memory = new MemorySaver();\n",
        "const model2 = new ChatOpenAI({ model: \"gpt-4o\" });\n",
        "\n",
        "// highlight-start\n",
        "const stateModifier = async (messages: BaseMessage[]): Promise<BaseMessage[]> => {\n",
        "  // We're using the message processor defined above.\n",
        "  return trimMessages(\n",
        "    messages,\n",
        "    {\n",
        "      tokenCounter: (msgs) => msgs.length, // <-- .length will simply count the number of messages rather than tokens\n",
        "      maxTokens: 5, // <-- allow up to 5 messages.\n",
        "      strategy: \"last\",\n",
        "      // The startOn is specified\n",
        "      // to make sure we do not generate a sequence where\n",
        "      // a ToolMessage that contains the result of a tool invocation\n",
        "      // appears before the AIMessage that requested a tool invocation\n",
        "      // as this will cause some chat models to raise an error.\n",
        "      startOn: \"human\",\n",
        "      includeSystem: true, // <-- Keep the system message\n",
        "      allowPartial: false,\n",
        "    }\n",
        "  );\n",
        "};\n",
        "// highlight-end\n",
        "\n",
        "const app2 = createReactAgent({\n",
        "  llm: model2,\n",
        "  tools: [getUserAge],\n",
        "  checkpointSaver: memory,\n",
        "  // highlight-next-line\n",
        "  messageModifier: stateModifier,\n",
        "});\n",
        "\n",
        "// The thread id is a unique key that identifies\n",
        "// this particular conversation.\n",
        "// We'll just generate a random uuid here.\n",
        "const threadId2 = uuidv4();\n",
        "const config2 = { configurable: { thread_id: threadId2 }, streamMode: \"values\" as const };\n",
        "\n",
        "// Tell the AI that our name is Bob, and ask it to use a tool to confirm\n",
        "// that it's capable of working like an agent.\n",
        "const inputMessage2 = {\n",
        "  role: \"user\",\n",
        "  content: \"hi! I'm bob. What is my age?\",\n",
        "}\n",
        "\n",
        "for await (const event of await app2.stream({ messages: [inputMessage2] }, config2)) {\n",
        "  const lastMessage = event.messages[event.messages.length - 1];\n",
        "  console.log(lastMessage.content);\n",
        "}\n",
        "\n",
        "// Confirm that the chat bot has access to previous conversation\n",
        "// and can respond to the user saying that the user's name is Bob.\n",
        "const followUpMessage2 = {\n",
        "  role: \"user\",\n",
        "  content: \"do you remember my name?\",\n",
        "};\n",
        "\n",
        "for await (const event of await app2.stream({ messages: [followUpMessage2] }, config2)) {\n",
        "  const lastMessage = event.messages[event.messages.length - 1];\n",
        "  console.log(lastMessage.content);\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f4d16e09-1d90-4153-8576-6d3996cb5a6c",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "</details>\n",
        "```\n",
        "\n",
        "## LCEL：添加预处理步骤\n",
        "\n",
        "添加复杂对话管理的最简单方法是在聊天模型前面引入一个预处理步骤，并将完整的对话历史传递给该预处理步骤。\n",
        "\n",
        "这种方法在概念上比较简单，并且在很多情况下都适用。例如，如果您使用的是 [RunnableWithMessageHistory](/docs/how_to/message_history/)，而不是将聊天模型进行包装，而是使用预处理器来包装聊天模型。\n",
        "\n",
        "这种方法的明显缺点是，由于以下两个原因，随着对话历史的增长，延迟会开始增加：\n",
        "\n",
        "1. 随着对话变长，可能需要从您用于存储对话历史的存储（如果不是在内存中存储的话）中获取更多数据。\n",
        "2. 预处理逻辑最终会进行大量冗余计算，重复对话之前步骤中的计算。\n",
        "\n",
        "```{=mdx}\n",
        ":::caution\n",
        "\n",
        "如果您想使用聊天模型的工具调用功能，请记住在向其添加历史预处理步骤之前，将工具绑定到模型上！\n",
        "\n",
        ":::\n",
        "```\n",
        "\n",
        "```{=mdx}\n",
        "<details open>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a1c8adf2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIMessage {\n",
            "  \"id\": \"chatcmpl-AB6uzWscxviYlbADFeDlnwIH82Fzt\",\n",
            "  \"content\": \"\",\n",
            "  \"additional_kwargs\": {\n",
            "    \"tool_calls\": [\n",
            "      {\n",
            "        \"id\": \"call_TghBL9dzqXFMCt0zj0VYMjfp\",\n",
            "        \"type\": \"function\",\n",
            "        \"function\": \"[Object]\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"response_metadata\": {\n",
            "    \"tokenUsage\": {\n",
            "      \"completionTokens\": 16,\n",
            "      \"promptTokens\": 95,\n",
            "      \"totalTokens\": 111\n",
            "    },\n",
            "    \"finish_reason\": \"tool_calls\",\n",
            "    \"system_fingerprint\": \"fp_a5d11b2ef2\"\n",
            "  },\n",
            "  \"tool_calls\": [\n",
            "    {\n",
            "      \"name\": \"what_did_the_cow_say\",\n",
            "      \"args\": {},\n",
            "      \"type\": \"tool_call\",\n",
            "      \"id\": \"call_TghBL9dzqXFMCt0zj0VYMjfp\"\n",
            "    }\n",
            "  ],\n",
            "  \"invalid_tool_calls\": [],\n",
            "  \"usage_metadata\": {\n",
            "    \"input_tokens\": 95,\n",
            "    \"output_tokens\": 16,\n",
            "    \"total_tokens\": 111\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import { ChatOpenAI } from \"@langchain/openai\";\n",
        "import { AIMessage, HumanMessage, SystemMessage, BaseMessage, trimMessages } from \"@langchain/core/messages\";\n",
        "import { tool } from \"@langchain/core/tools\";\n",
        "import { z } from \"zod\";\n",
        "\n",
        "const model3 = new ChatOpenAI({ model: \"gpt-4o\" });\n",
        "\n",
        "const whatDidTheCowSay = tool(\n",
        "  (): string => {\n",
        "    return \"foo\";\n",
        "  },\n",
        "  {\n",
        "    name: \"what_did_the_cow_say\",\n",
        "    description: \"Check to see what the cow said.\",\n",
        "    schema: z.object({}),\n",
        "  }\n",
        ");\n",
        "\n",
        "// highlight-start\n",
        "const messageProcessor = trimMessages(\n",
        "  {\n",
        "    tokenCounter: (msgs) => msgs.length, // <-- .length will simply count the number of messages rather than tokens\n",
        "    maxTokens: 5, // <-- allow up to 5 messages.\n",
        "    strategy: \"last\",\n",
        "    // The startOn is specified\n",
        "    // to make sure we do not generate a sequence where\n",
        "    // a ToolMessage that contains the result of a tool invocation\n",
        "    // appears before the AIMessage that requested a tool invocation\n",
        "    // as this will cause some chat models to raise an error.\n",
        "    startOn: \"human\",\n",
        "    includeSystem: true, // <-- Keep the system message\n",
        "    allowPartial: false,\n",
        "  }\n",
        ");\n",
        "// highlight-end\n",
        "\n",
        "// Note that we bind tools to the model first!\n",
        "const modelWithTools = model3.bindTools([whatDidTheCowSay]);\n",
        "\n",
        "// highlight-next-line\n",
        "const modelWithPreprocessor = messageProcessor.pipe(modelWithTools);\n",
        "\n",
        "const fullHistory = [\n",
        "  new SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
        "  new HumanMessage(\"i wonder why it's called langchain\"),\n",
        "  new AIMessage('Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'),\n",
        "  new HumanMessage(\"and who is harrison chasing anyways\"),\n",
        "  new AIMessage(\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"),\n",
        "  new HumanMessage(\"why is 42 always the answer?\"),\n",
        "  new AIMessage(\"Because it's the only number that's constantly right, even when it doesn't add up!\"),\n",
        "  new HumanMessage(\"What did the cow say?\"),\n",
        "];\n",
        "\n",
        "// We pass it explicitly to the modelWithPreprocessor for illustrative purposes.\n",
        "// If you're using `RunnableWithMessageHistory` the history will be automatically\n",
        "// read from the source that you configure.\n",
        "const result = await modelWithPreprocessor.invoke(fullHistory);\n",
        "console.log(result);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5da7225a-5e94-4f53-bb0d-86b6b528d150",
      "metadata": {},
      "source": [
        "```{=mdx}\n",
        "</details>\n",
        "```\n",
        "\n",
        "如果你需要实现更高效的逻辑，并且希望目前使用 `RunnableWithMessageHistory`\n",
        "实现此目标的方法是继承 [BaseChatMessageHistory](https://api.js.langchain.com/classes/_langchain_core.chat_history.BaseChatMessageHistory.html)\n",
        "并为 `addMessages` 定义适当的逻辑（不要简单地追加历史记录，而是重写它）。\n",
        "\n",
        "除非你有充分的理由实现此解决方案，否则应使用 LangGraph。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2717810",
      "metadata": {},
      "source": [
        "## 下一步\n",
        "\n",
        "探索使用 LangGraph 的持久化功能：\n",
        "\n",
        "* [LangGraph 快速入门教程](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)\n",
        "* [如何为你的图添加持久化功能（\"记忆\"）](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/)\n",
        "* [如何管理对话历史记录](https://langchain-ai.github.io/langgraphjs/how-tos/manage-conversation-history/)\n",
        "* [如何添加对话历史记录的摘要](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/)\n",
        "\n",
        "使用简单 LCEL 添加持久化功能（对于更复杂的使用场景，请优先使用 LangGraph）：\n",
        "\n",
        "* [如何添加消息历史记录](/docs/how_to/message_history/)\n",
        "\n",
        "处理消息历史记录：\n",
        "\n",
        "* [如何裁剪消息](/docs/how_to/trim_messages)\n",
        "* [如何过滤消息](/docs/how_to/filter_messages/)\n",
        "* [如何合并消息运行](/docs/how_to/merge_message_runs/)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TypeScript",
      "language": "typescript",
      "name": "tslab"
    },
    "language_info": {
      "codemirror_mode": {
        "mode": "typescript",
        "name": "javascript",
        "typescript": true
      },
      "file_extension": ".ts",
      "mimetype": "text/typescript",
      "name": "typescript",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}